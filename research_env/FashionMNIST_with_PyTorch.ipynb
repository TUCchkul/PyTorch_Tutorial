{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "385cac41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f25b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e75a6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_DIR=\"FashionMNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0b2dbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nToTensor()->basically scalling down from 0 to 1\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=torchvision.datasets.FashionMNIST(\n",
    "root=DATA_ROOT_DIR,\n",
    "train=True,\n",
    "download=True,\n",
    "transform=torchvision.transforms.ToTensor())\n",
    "test_data=torchvision.datasets.FashionMNIST(\n",
    "root=DATA_ROOT_DIR,\n",
    "train=False,\n",
    "download=True,\n",
    "transform=torchvision.transforms.ToTensor())\n",
    "\"\"\"\n",
    "ToTensor()->basically scalling down from 0 to 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b68642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d9336d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "\"\"\"\n",
    "train_data_loader-> its a methodand its actually create an python iterator. Its load our data and its take care of all our parallel processing like how you load the data and how you shuffling.\n",
    "\n",
    "\"\"\"\n",
    "train_data_loader=torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "test_data_loader=torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b676e54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64]) torch.int64 tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_data_loader:\n",
    "    print(X.shape)\n",
    "    #For all of the images  we have respective classe label. we can check for that torch.unique(y)\n",
    "    print(y.shape, y.dtype, torch.unique(y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69bab06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba36dffd",
   "metadata": {},
   "source": [
    "# Lets create a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49d4a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module inherites the basic nn module\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # NeuralNetwork is a child class for nn Module\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.flatten=nn.Flatten()\n",
    "        \n",
    "        self.linear_relu_stack=nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, X):#Forward propagation  X->input or train data\n",
    "        X=self.flatten(X)\n",
    "        logits=self.linear_relu_stack(X)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a14bd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a979ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0923f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2f6e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size=len(dataloader.dataset)\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X,y=X.to(device), y.to(device)\n",
    "        #calculate error \n",
    "        #First of all we need to forward proagate the value like whatever the data we have to pass. We will pass from the model\n",
    "        #Right now model is not trained yet\n",
    "        # First data point pass through the model\n",
    "        pred=model(X)\n",
    "        #Calculate the loss and loss we need prediction and the actual label that will give us actual value\n",
    "        loss=loss_fn(pred, y)\n",
    "        # BackPropagation\n",
    "        # we need zero gradient because when we calculate gradient in pytorch, it accumulate the previous gradients, we do not want in every steps\n",
    "        #For example we have created gradient first steps and we do not want to use same gradient in the next steps. So we have to initialize it from zero so that we can get the exact gradient for that particular steps thats why we need to make zero gradend\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()#\n",
    "        optimizer.step()# with the help of optimizer updates the weight\n",
    "        \n",
    "        if batch %100==0:\n",
    "            loss, current=loss.item(), batch*len(X)# len(X)->current data points\n",
    "            print(f\"loss: {loss} [{current}/{size}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c871fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    size=len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct=0,0\n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            X,y=X.to(device), y.to(device)\n",
    "            pred=model(X)\n",
    "            test_loss=test_loss+ loss_fn(pred,y).item()\n",
    "            correct=correct + (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    test_loss=test_loss/size\n",
    "    correct=correct/size\n",
    "    print(f\"Test_error: {100*correct}, average_loss:{test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef84a880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "-----------\n",
      "loss: 2.3030874729156494 [0/60000]\n",
      "loss: 2.285720109939575 [6400/60000]\n",
      "loss: 2.2723944187164307 [12800/60000]\n",
      "loss: 2.2647290229797363 [19200/60000]\n",
      "loss: 2.2470555305480957 [25600/60000]\n",
      "loss: 2.2496793270111084 [32000/60000]\n",
      "loss: 2.227506637573242 [38400/60000]\n",
      "loss: 2.2214245796203613 [44800/60000]\n",
      "loss: 2.217020034790039 [51200/60000]\n",
      "loss: 2.2040748596191406 [57600/60000]\n",
      "Test_error: 46.79, average_loss:0.034445042872428895\n",
      "EPOCH: 2\n",
      "-----------\n",
      "loss: 2.206491470336914 [0/60000]\n",
      "loss: 2.17490291595459 [6400/60000]\n",
      "loss: 2.1507813930511475 [12800/60000]\n",
      "loss: 2.158328056335449 [19200/60000]\n",
      "loss: 2.095644235610962 [25600/60000]\n",
      "loss: 2.1326990127563477 [32000/60000]\n",
      "loss: 2.084961175918579 [38400/60000]\n",
      "loss: 2.0738303661346436 [44800/60000]\n",
      "loss: 2.078347682952881 [51200/60000]\n",
      "loss: 2.0603840351104736 [57600/60000]\n",
      "Test_error: 49.18, average_loss:0.03196538891792297\n",
      "EPOCH: 3\n",
      "-----------\n",
      "loss: 2.0645084381103516 [0/60000]\n",
      "loss: 1.9991745948791504 [6400/60000]\n",
      "loss: 1.960445761680603 [12800/60000]\n",
      "loss: 1.984790563583374 [19200/60000]\n",
      "loss: 1.8651347160339355 [25600/60000]\n",
      "loss: 1.9615535736083984 [32000/60000]\n",
      "loss: 1.874112844467163 [38400/60000]\n",
      "loss: 1.8682732582092285 [44800/60000]\n",
      "loss: 1.8905003070831299 [51200/60000]\n",
      "loss: 1.8666359186172485 [57600/60000]\n",
      "Test_error: 49.53, average_loss:0.0287748309135437\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=3\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"EPOCH: {epoch+1}\\n-----------\")\n",
    "    train(train_data_loader, model, loss_fn, optimizer)\n",
    "    test(test_data_loader, model)\n",
    "print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba391c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "555a53b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankel boot Ankel boot\n"
     ]
    }
   ],
   "source": [
    "classes=[\"T_shirt/top\",\n",
    "        \"Trouser\",\n",
    "        \"Pullover\",\n",
    "        \"Dress\",\n",
    "         \"Coat\",\n",
    "        \"Sandal\",\n",
    "        \"Shirt\",\n",
    "        \"Sneaker\",\n",
    "        \"Bag\",\n",
    "        \"Ankel boot\"]\n",
    "model.eval()\n",
    "X,y=test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    X=X.to(device)\n",
    "    #y=y.to(device)\n",
    "    pred=model(X)\n",
    "    predicted,actual=classes[pred[0].argmax(0)], classes[y]\n",
    "    print(predicted, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24565ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
