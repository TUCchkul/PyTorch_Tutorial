{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=np.array([[32,55,77],\n",
    "                    [31,75,57],\n",
    "                    [52,55,77],\n",
    "                    [22,100,87],\n",
    "                    [62,80,77]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 32.,  55.,  77.],\n",
       "       [ 31.,  75.,  57.],\n",
       "       [ 52.,  55.,  77.],\n",
       "       [ 22., 100.,  87.],\n",
       "       [ 62.,  80.,  77.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.array([[32,66],\n",
    "                [52,76],\n",
    "                [102,155],\n",
    "                [32,26],\n",
    "                [99,100]],dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 32.,  55.,  77.],\n",
      "        [ 31.,  75.,  57.],\n",
      "        [ 52.,  55.,  77.],\n",
      "        [ 22., 100.,  87.],\n",
      "        [ 62.,  80.,  77.]])\n",
      "tensor([[ 32.,  66.],\n",
      "        [ 52.,  76.],\n",
      "        [102., 155.],\n",
      "        [ 32.,  26.],\n",
      "        [ 99., 100.]])\n"
     ]
    }
   ],
   "source": [
    "#Convert numpy tp tensor\n",
    "inputs=torch.from_numpy(input_data)\n",
    "targets=torch.from_numpy(target)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1390,  0.8235,  0.7681],\n",
      "        [-0.1936,  0.3932, -0.9366]], requires_grad=True)\n",
      "tensor([-0.4080, -0.4927], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights and bias\n",
    "w=torch.randn(2,3, requires_grad=True)\n",
    "b=torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 99.5853, -57.1817],\n",
      "        [100.8325, -30.3909],\n",
      "        [ 96.8051, -61.0528],\n",
      "        [145.7165, -46.9181],\n",
      "        [116.0036, -53.1581]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pred=model(inputs)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 32.,  66.],\n",
       "        [ 52.,  76.],\n",
       "        [102., 155.],\n",
       "        [ 32.,  26.],\n",
       "        [ 99., 100.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss\n",
    "def MSE(t1,t2):\n",
    "    diff=t1-t2\n",
    "    return torch.sum(diff*diff)/diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12214.5957, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss=MSE(pred, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradient\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1390,  0.8235,  0.7681],\n",
      "        [-0.1936,  0.3932, -0.9366]], requires_grad=True)\n",
      "tensor([[  1392.4779,   3965.1702,   3758.0256],\n",
      "        [ -5914.9355,  -9236.3340, -10064.4766]])\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w -=w.grad * 1e-5\n",
    "    b -=b.grad * 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1529,  0.7839,  0.7306],\n",
      "        [-0.1344,  0.4856, -0.8360]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4085, -0.4914], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9841.2314, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss=MSE(pred, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0/5000----Loss: 333.4637756347656\n",
      "Epochs: 1/5000----Loss: 333.4261474609375\n",
      "Epochs: 2/5000----Loss: 333.38861083984375\n",
      "Epochs: 3/5000----Loss: 333.3511047363281\n",
      "Epochs: 4/5000----Loss: 333.3138122558594\n",
      "Epochs: 5/5000----Loss: 333.2766418457031\n",
      "Epochs: 6/5000----Loss: 333.239501953125\n",
      "Epochs: 7/5000----Loss: 333.20257568359375\n",
      "Epochs: 8/5000----Loss: 333.16583251953125\n",
      "Epochs: 9/5000----Loss: 333.12921142578125\n",
      "Epochs: 10/5000----Loss: 333.092529296875\n",
      "Epochs: 11/5000----Loss: 333.0560607910156\n",
      "Epochs: 12/5000----Loss: 333.01971435546875\n",
      "Epochs: 13/5000----Loss: 332.9834899902344\n",
      "Epochs: 14/5000----Loss: 332.9473571777344\n",
      "Epochs: 15/5000----Loss: 332.9113464355469\n",
      "Epochs: 16/5000----Loss: 332.87554931640625\n",
      "Epochs: 17/5000----Loss: 332.8396911621094\n",
      "Epochs: 18/5000----Loss: 332.8040466308594\n",
      "Epochs: 19/5000----Loss: 332.7685546875\n",
      "Epochs: 20/5000----Loss: 332.73309326171875\n",
      "Epochs: 21/5000----Loss: 332.6977844238281\n",
      "Epochs: 22/5000----Loss: 332.6626281738281\n",
      "Epochs: 23/5000----Loss: 332.627685546875\n",
      "Epochs: 24/5000----Loss: 332.59283447265625\n",
      "Epochs: 25/5000----Loss: 332.5578918457031\n",
      "Epochs: 26/5000----Loss: 332.52313232421875\n",
      "Epochs: 27/5000----Loss: 332.4884948730469\n",
      "Epochs: 28/5000----Loss: 332.45404052734375\n",
      "Epochs: 29/5000----Loss: 332.4196472167969\n",
      "Epochs: 30/5000----Loss: 332.3852844238281\n",
      "Epochs: 31/5000----Loss: 332.35113525390625\n",
      "Epochs: 32/5000----Loss: 332.3169860839844\n",
      "Epochs: 33/5000----Loss: 332.2830810546875\n",
      "Epochs: 34/5000----Loss: 332.24920654296875\n",
      "Epochs: 35/5000----Loss: 332.2154541015625\n",
      "Epochs: 36/5000----Loss: 332.1819152832031\n",
      "Epochs: 37/5000----Loss: 332.1483154296875\n",
      "Epochs: 38/5000----Loss: 332.114990234375\n",
      "Epochs: 39/5000----Loss: 332.0816955566406\n",
      "Epochs: 40/5000----Loss: 332.04852294921875\n",
      "Epochs: 41/5000----Loss: 332.0154724121094\n",
      "Epochs: 42/5000----Loss: 331.982421875\n",
      "Epochs: 43/5000----Loss: 331.94940185546875\n",
      "Epochs: 44/5000----Loss: 331.9165954589844\n",
      "Epochs: 45/5000----Loss: 331.8839111328125\n",
      "Epochs: 46/5000----Loss: 331.851318359375\n",
      "Epochs: 47/5000----Loss: 331.8188781738281\n",
      "Epochs: 48/5000----Loss: 331.78643798828125\n",
      "Epochs: 49/5000----Loss: 331.7541809082031\n",
      "Epochs: 50/5000----Loss: 331.72198486328125\n",
      "Epochs: 51/5000----Loss: 331.6898498535156\n",
      "Epochs: 52/5000----Loss: 331.65789794921875\n",
      "Epochs: 53/5000----Loss: 331.6259460449219\n",
      "Epochs: 54/5000----Loss: 331.59429931640625\n",
      "Epochs: 55/5000----Loss: 331.5626220703125\n",
      "Epochs: 56/5000----Loss: 331.53106689453125\n",
      "Epochs: 57/5000----Loss: 331.4996032714844\n",
      "Epochs: 58/5000----Loss: 331.46820068359375\n",
      "Epochs: 59/5000----Loss: 331.43682861328125\n",
      "Epochs: 60/5000----Loss: 331.4057312011719\n",
      "Epochs: 61/5000----Loss: 331.3745422363281\n",
      "Epochs: 62/5000----Loss: 331.34356689453125\n",
      "Epochs: 63/5000----Loss: 331.31268310546875\n",
      "Epochs: 64/5000----Loss: 331.2818603515625\n",
      "Epochs: 65/5000----Loss: 331.2513122558594\n",
      "Epochs: 66/5000----Loss: 331.22052001953125\n",
      "Epochs: 67/5000----Loss: 331.1900634765625\n",
      "Epochs: 68/5000----Loss: 331.1595458984375\n",
      "Epochs: 69/5000----Loss: 331.1291198730469\n",
      "Epochs: 70/5000----Loss: 331.09893798828125\n",
      "Epochs: 71/5000----Loss: 331.0687561035156\n",
      "Epochs: 72/5000----Loss: 331.03875732421875\n",
      "Epochs: 73/5000----Loss: 331.00885009765625\n",
      "Epochs: 74/5000----Loss: 330.9788513183594\n",
      "Epochs: 75/5000----Loss: 330.94915771484375\n",
      "Epochs: 76/5000----Loss: 330.9194641113281\n",
      "Epochs: 77/5000----Loss: 330.8897705078125\n",
      "Epochs: 78/5000----Loss: 330.8603515625\n",
      "Epochs: 79/5000----Loss: 330.83087158203125\n",
      "Epochs: 80/5000----Loss: 330.801513671875\n",
      "Epochs: 81/5000----Loss: 330.7723693847656\n",
      "Epochs: 82/5000----Loss: 330.7431335449219\n",
      "Epochs: 83/5000----Loss: 330.71405029296875\n",
      "Epochs: 84/5000----Loss: 330.6850280761719\n",
      "Epochs: 85/5000----Loss: 330.65606689453125\n",
      "Epochs: 86/5000----Loss: 330.6273193359375\n",
      "Epochs: 87/5000----Loss: 330.59857177734375\n",
      "Epochs: 88/5000----Loss: 330.5698547363281\n",
      "Epochs: 89/5000----Loss: 330.54132080078125\n",
      "Epochs: 90/5000----Loss: 330.51300048828125\n",
      "Epochs: 91/5000----Loss: 330.4846496582031\n",
      "Epochs: 92/5000----Loss: 330.45623779296875\n",
      "Epochs: 93/5000----Loss: 330.42803955078125\n",
      "Epochs: 94/5000----Loss: 330.39990234375\n",
      "Epochs: 95/5000----Loss: 330.37188720703125\n",
      "Epochs: 96/5000----Loss: 330.34381103515625\n",
      "Epochs: 97/5000----Loss: 330.3159484863281\n",
      "Epochs: 98/5000----Loss: 330.2882080078125\n",
      "Epochs: 99/5000----Loss: 330.2604675292969\n",
      "Epochs: 100/5000----Loss: 330.2327880859375\n",
      "Epochs: 101/5000----Loss: 330.20526123046875\n",
      "Epochs: 102/5000----Loss: 330.177734375\n",
      "Epochs: 103/5000----Loss: 330.15032958984375\n",
      "Epochs: 104/5000----Loss: 330.1230163574219\n",
      "Epochs: 105/5000----Loss: 330.0958557128906\n",
      "Epochs: 106/5000----Loss: 330.06866455078125\n",
      "Epochs: 107/5000----Loss: 330.0415954589844\n",
      "Epochs: 108/5000----Loss: 330.0145568847656\n",
      "Epochs: 109/5000----Loss: 329.98785400390625\n",
      "Epochs: 110/5000----Loss: 329.9609069824219\n",
      "Epochs: 111/5000----Loss: 329.9342956542969\n",
      "Epochs: 112/5000----Loss: 329.907470703125\n",
      "Epochs: 113/5000----Loss: 329.8809814453125\n",
      "Epochs: 114/5000----Loss: 329.8544006347656\n",
      "Epochs: 115/5000----Loss: 329.8277587890625\n",
      "Epochs: 116/5000----Loss: 329.8015441894531\n",
      "Epochs: 117/5000----Loss: 329.77520751953125\n",
      "Epochs: 118/5000----Loss: 329.7490234375\n",
      "Epochs: 119/5000----Loss: 329.7227783203125\n",
      "Epochs: 120/5000----Loss: 329.69683837890625\n",
      "Epochs: 121/5000----Loss: 329.67071533203125\n",
      "Epochs: 122/5000----Loss: 329.6448669433594\n",
      "Epochs: 123/5000----Loss: 329.61895751953125\n",
      "Epochs: 124/5000----Loss: 329.5931701660156\n",
      "Epochs: 125/5000----Loss: 329.5674743652344\n",
      "Epochs: 126/5000----Loss: 329.54180908203125\n",
      "Epochs: 127/5000----Loss: 329.5163269042969\n",
      "Epochs: 128/5000----Loss: 329.490966796875\n",
      "Epochs: 129/5000----Loss: 329.4655456542969\n",
      "Epochs: 130/5000----Loss: 329.44024658203125\n",
      "Epochs: 131/5000----Loss: 329.4149169921875\n",
      "Epochs: 132/5000----Loss: 329.38970947265625\n",
      "Epochs: 133/5000----Loss: 329.3646240234375\n",
      "Epochs: 134/5000----Loss: 329.33953857421875\n",
      "Epochs: 135/5000----Loss: 329.31463623046875\n",
      "Epochs: 136/5000----Loss: 329.2896728515625\n",
      "Epochs: 137/5000----Loss: 329.26483154296875\n",
      "Epochs: 138/5000----Loss: 329.2402038574219\n",
      "Epochs: 139/5000----Loss: 329.2154235839844\n",
      "Epochs: 140/5000----Loss: 329.19085693359375\n",
      "Epochs: 141/5000----Loss: 329.16632080078125\n",
      "Epochs: 142/5000----Loss: 329.14178466796875\n",
      "Epochs: 143/5000----Loss: 329.1173400878906\n",
      "Epochs: 144/5000----Loss: 329.0929870605469\n",
      "Epochs: 145/5000----Loss: 329.0686950683594\n",
      "Epochs: 146/5000----Loss: 329.0444641113281\n",
      "Epochs: 147/5000----Loss: 329.0203552246094\n",
      "Epochs: 148/5000----Loss: 328.99627685546875\n",
      "Epochs: 149/5000----Loss: 328.9723815917969\n",
      "Epochs: 150/5000----Loss: 328.948486328125\n",
      "Epochs: 151/5000----Loss: 328.92462158203125\n",
      "Epochs: 152/5000----Loss: 328.9007568359375\n",
      "Epochs: 153/5000----Loss: 328.87701416015625\n",
      "Epochs: 154/5000----Loss: 328.8533630371094\n",
      "Epochs: 155/5000----Loss: 328.82977294921875\n",
      "Epochs: 156/5000----Loss: 328.80621337890625\n",
      "Epochs: 157/5000----Loss: 328.78277587890625\n",
      "Epochs: 158/5000----Loss: 328.7592468261719\n",
      "Epochs: 159/5000----Loss: 328.7359619140625\n",
      "Epochs: 160/5000----Loss: 328.7126159667969\n",
      "Epochs: 161/5000----Loss: 328.6894226074219\n",
      "Epochs: 162/5000----Loss: 328.6662902832031\n",
      "Epochs: 163/5000----Loss: 328.6432189941406\n",
      "Epochs: 164/5000----Loss: 328.62017822265625\n",
      "Epochs: 165/5000----Loss: 328.5972595214844\n",
      "Epochs: 166/5000----Loss: 328.57440185546875\n",
      "Epochs: 167/5000----Loss: 328.55145263671875\n",
      "Epochs: 168/5000----Loss: 328.5287780761719\n",
      "Epochs: 169/5000----Loss: 328.5060729980469\n",
      "Epochs: 170/5000----Loss: 328.48345947265625\n",
      "Epochs: 171/5000----Loss: 328.46087646484375\n",
      "Epochs: 172/5000----Loss: 328.43841552734375\n",
      "Epochs: 173/5000----Loss: 328.416015625\n",
      "Epochs: 174/5000----Loss: 328.3935241699219\n",
      "Epochs: 175/5000----Loss: 328.3712158203125\n",
      "Epochs: 176/5000----Loss: 328.34893798828125\n",
      "Epochs: 177/5000----Loss: 328.32672119140625\n",
      "Epochs: 178/5000----Loss: 328.3045349121094\n",
      "Epochs: 179/5000----Loss: 328.2825012207031\n",
      "Epochs: 180/5000----Loss: 328.2603454589844\n",
      "Epochs: 181/5000----Loss: 328.2384338378906\n",
      "Epochs: 182/5000----Loss: 328.216552734375\n",
      "Epochs: 183/5000----Loss: 328.1947326660156\n",
      "Epochs: 184/5000----Loss: 328.1728515625\n",
      "Epochs: 185/5000----Loss: 328.1510925292969\n",
      "Epochs: 186/5000----Loss: 328.12945556640625\n",
      "Epochs: 187/5000----Loss: 328.1079406738281\n",
      "Epochs: 188/5000----Loss: 328.08624267578125\n",
      "Epochs: 189/5000----Loss: 328.064697265625\n",
      "Epochs: 190/5000----Loss: 328.04327392578125\n",
      "Epochs: 191/5000----Loss: 328.02191162109375\n",
      "Epochs: 192/5000----Loss: 328.0005187988281\n",
      "Epochs: 193/5000----Loss: 327.97930908203125\n",
      "Epochs: 194/5000----Loss: 327.9580993652344\n",
      "Epochs: 195/5000----Loss: 327.9369812011719\n",
      "Epochs: 196/5000----Loss: 327.9158630371094\n",
      "Epochs: 197/5000----Loss: 327.8949279785156\n",
      "Epochs: 198/5000----Loss: 327.8738098144531\n",
      "Epochs: 199/5000----Loss: 327.8529357910156\n",
      "Epochs: 200/5000----Loss: 327.83209228515625\n",
      "Epochs: 201/5000----Loss: 327.8111877441406\n",
      "Epochs: 202/5000----Loss: 327.7904052734375\n",
      "Epochs: 203/5000----Loss: 327.769775390625\n",
      "Epochs: 204/5000----Loss: 327.74896240234375\n",
      "Epochs: 205/5000----Loss: 327.7283630371094\n",
      "Epochs: 206/5000----Loss: 327.70794677734375\n",
      "Epochs: 207/5000----Loss: 327.68743896484375\n",
      "Epochs: 208/5000----Loss: 327.6669616699219\n",
      "Epochs: 209/5000----Loss: 327.6466369628906\n",
      "Epochs: 210/5000----Loss: 327.626220703125\n",
      "Epochs: 211/5000----Loss: 327.6059875488281\n",
      "Epochs: 212/5000----Loss: 327.585693359375\n",
      "Epochs: 213/5000----Loss: 327.5655517578125\n",
      "Epochs: 214/5000----Loss: 327.54534912109375\n",
      "Epochs: 215/5000----Loss: 327.52532958984375\n",
      "Epochs: 216/5000----Loss: 327.50531005859375\n",
      "Epochs: 217/5000----Loss: 327.48541259765625\n",
      "Epochs: 218/5000----Loss: 327.4654846191406\n",
      "Epochs: 219/5000----Loss: 327.4455871582031\n",
      "Epochs: 220/5000----Loss: 327.42584228515625\n",
      "Epochs: 221/5000----Loss: 327.40606689453125\n",
      "Epochs: 222/5000----Loss: 327.3863830566406\n",
      "Epochs: 223/5000----Loss: 327.3667907714844\n",
      "Epochs: 224/5000----Loss: 327.3470458984375\n",
      "Epochs: 225/5000----Loss: 327.32763671875\n",
      "Epochs: 226/5000----Loss: 327.30810546875\n",
      "Epochs: 227/5000----Loss: 327.2886657714844\n",
      "Epochs: 228/5000----Loss: 327.2692565917969\n",
      "Epochs: 229/5000----Loss: 327.2498779296875\n",
      "Epochs: 230/5000----Loss: 327.2305908203125\n",
      "Epochs: 231/5000----Loss: 327.21136474609375\n",
      "Epochs: 232/5000----Loss: 327.19219970703125\n",
      "Epochs: 233/5000----Loss: 327.17291259765625\n",
      "Epochs: 234/5000----Loss: 327.1539001464844\n",
      "Epochs: 235/5000----Loss: 327.1348571777344\n",
      "Epochs: 236/5000----Loss: 327.11590576171875\n",
      "Epochs: 237/5000----Loss: 327.09698486328125\n",
      "Epochs: 238/5000----Loss: 327.07794189453125\n",
      "Epochs: 239/5000----Loss: 327.05902099609375\n",
      "Epochs: 240/5000----Loss: 327.0402526855469\n",
      "Epochs: 241/5000----Loss: 327.02154541015625\n",
      "Epochs: 242/5000----Loss: 327.0028381347656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 243/5000----Loss: 326.9841613769531\n",
      "Epochs: 244/5000----Loss: 326.9655456542969\n",
      "Epochs: 245/5000----Loss: 326.94696044921875\n",
      "Epochs: 246/5000----Loss: 326.9283752441406\n",
      "Epochs: 247/5000----Loss: 326.91009521484375\n",
      "Epochs: 248/5000----Loss: 326.8916015625\n",
      "Epochs: 249/5000----Loss: 326.8731994628906\n",
      "Epochs: 250/5000----Loss: 326.85491943359375\n",
      "Epochs: 251/5000----Loss: 326.83660888671875\n",
      "Epochs: 252/5000----Loss: 326.8184509277344\n",
      "Epochs: 253/5000----Loss: 326.8002014160156\n",
      "Epochs: 254/5000----Loss: 326.78204345703125\n",
      "Epochs: 255/5000----Loss: 326.7638854980469\n",
      "Epochs: 256/5000----Loss: 326.7458801269531\n",
      "Epochs: 257/5000----Loss: 326.7279357910156\n",
      "Epochs: 258/5000----Loss: 326.7098693847656\n",
      "Epochs: 259/5000----Loss: 326.6919250488281\n",
      "Epochs: 260/5000----Loss: 326.6741943359375\n",
      "Epochs: 261/5000----Loss: 326.65631103515625\n",
      "Epochs: 262/5000----Loss: 326.63848876953125\n",
      "Epochs: 263/5000----Loss: 326.62078857421875\n",
      "Epochs: 264/5000----Loss: 326.6030578613281\n",
      "Epochs: 265/5000----Loss: 326.58538818359375\n",
      "Epochs: 266/5000----Loss: 326.56781005859375\n",
      "Epochs: 267/5000----Loss: 326.55023193359375\n",
      "Epochs: 268/5000----Loss: 326.5326843261719\n",
      "Epochs: 269/5000----Loss: 326.51519775390625\n",
      "Epochs: 270/5000----Loss: 326.4978332519531\n",
      "Epochs: 271/5000----Loss: 326.48040771484375\n",
      "Epochs: 272/5000----Loss: 326.463134765625\n",
      "Epochs: 273/5000----Loss: 326.44573974609375\n",
      "Epochs: 274/5000----Loss: 326.42852783203125\n",
      "Epochs: 275/5000----Loss: 326.4114074707031\n",
      "Epochs: 276/5000----Loss: 326.3943176269531\n",
      "Epochs: 277/5000----Loss: 326.3771667480469\n",
      "Epochs: 278/5000----Loss: 326.36004638671875\n",
      "Epochs: 279/5000----Loss: 326.34295654296875\n",
      "Epochs: 280/5000----Loss: 326.3260498046875\n",
      "Epochs: 281/5000----Loss: 326.30902099609375\n",
      "Epochs: 282/5000----Loss: 326.29205322265625\n",
      "Epochs: 283/5000----Loss: 326.2751770019531\n",
      "Epochs: 284/5000----Loss: 326.2582702636719\n",
      "Epochs: 285/5000----Loss: 326.24163818359375\n",
      "Epochs: 286/5000----Loss: 326.224853515625\n",
      "Epochs: 287/5000----Loss: 326.2081298828125\n",
      "Epochs: 288/5000----Loss: 326.19146728515625\n",
      "Epochs: 289/5000----Loss: 326.1748046875\n",
      "Epochs: 290/5000----Loss: 326.1582946777344\n",
      "Epochs: 291/5000----Loss: 326.14166259765625\n",
      "Epochs: 292/5000----Loss: 326.125244140625\n",
      "Epochs: 293/5000----Loss: 326.1087951660156\n",
      "Epochs: 294/5000----Loss: 326.0923156738281\n",
      "Epochs: 295/5000----Loss: 326.0760192871094\n",
      "Epochs: 296/5000----Loss: 326.0596923828125\n",
      "Epochs: 297/5000----Loss: 326.0432434082031\n",
      "Epochs: 298/5000----Loss: 326.0269775390625\n",
      "Epochs: 299/5000----Loss: 326.0108947753906\n",
      "Epochs: 300/5000----Loss: 325.9946594238281\n",
      "Epochs: 301/5000----Loss: 325.97845458984375\n",
      "Epochs: 302/5000----Loss: 325.9622802734375\n",
      "Epochs: 303/5000----Loss: 325.94622802734375\n",
      "Epochs: 304/5000----Loss: 325.93023681640625\n",
      "Epochs: 305/5000----Loss: 325.914306640625\n",
      "Epochs: 306/5000----Loss: 325.89825439453125\n",
      "Epochs: 307/5000----Loss: 325.8825378417969\n",
      "Epochs: 308/5000----Loss: 325.8665771484375\n",
      "Epochs: 309/5000----Loss: 325.8508605957031\n",
      "Epochs: 310/5000----Loss: 325.8349609375\n",
      "Epochs: 311/5000----Loss: 325.8191833496094\n",
      "Epochs: 312/5000----Loss: 325.8035583496094\n",
      "Epochs: 313/5000----Loss: 325.78778076171875\n",
      "Epochs: 314/5000----Loss: 325.7721862792969\n",
      "Epochs: 315/5000----Loss: 325.75653076171875\n",
      "Epochs: 316/5000----Loss: 325.74090576171875\n",
      "Epochs: 317/5000----Loss: 325.72552490234375\n",
      "Epochs: 318/5000----Loss: 325.7099609375\n",
      "Epochs: 319/5000----Loss: 325.6945495605469\n",
      "Epochs: 320/5000----Loss: 325.6790466308594\n",
      "Epochs: 321/5000----Loss: 325.66375732421875\n",
      "Epochs: 322/5000----Loss: 325.64837646484375\n",
      "Epochs: 323/5000----Loss: 325.6332092285156\n",
      "Epochs: 324/5000----Loss: 325.6178283691406\n",
      "Epochs: 325/5000----Loss: 325.6025695800781\n",
      "Epochs: 326/5000----Loss: 325.58740234375\n",
      "Epochs: 327/5000----Loss: 325.57208251953125\n",
      "Epochs: 328/5000----Loss: 325.5570068359375\n",
      "Epochs: 329/5000----Loss: 325.54205322265625\n",
      "Epochs: 330/5000----Loss: 325.5270080566406\n",
      "Epochs: 331/5000----Loss: 325.5119934082031\n",
      "Epochs: 332/5000----Loss: 325.49700927734375\n",
      "Epochs: 333/5000----Loss: 325.48193359375\n",
      "Epochs: 334/5000----Loss: 325.4670104980469\n",
      "Epochs: 335/5000----Loss: 325.45208740234375\n",
      "Epochs: 336/5000----Loss: 325.43731689453125\n",
      "Epochs: 337/5000----Loss: 325.4225158691406\n",
      "Epochs: 338/5000----Loss: 325.40777587890625\n",
      "Epochs: 339/5000----Loss: 325.3930969238281\n",
      "Epochs: 340/5000----Loss: 325.3783874511719\n",
      "Epochs: 341/5000----Loss: 325.36370849609375\n",
      "Epochs: 342/5000----Loss: 325.34918212890625\n",
      "Epochs: 343/5000----Loss: 325.3345642089844\n",
      "Epochs: 344/5000----Loss: 325.32000732421875\n",
      "Epochs: 345/5000----Loss: 325.305419921875\n",
      "Epochs: 346/5000----Loss: 325.291015625\n",
      "Epochs: 347/5000----Loss: 325.2764892578125\n",
      "Epochs: 348/5000----Loss: 325.26220703125\n",
      "Epochs: 349/5000----Loss: 325.2476501464844\n",
      "Epochs: 350/5000----Loss: 325.2333679199219\n",
      "Epochs: 351/5000----Loss: 325.218994140625\n",
      "Epochs: 352/5000----Loss: 325.204833984375\n",
      "Epochs: 353/5000----Loss: 325.19049072265625\n",
      "Epochs: 354/5000----Loss: 325.1761474609375\n",
      "Epochs: 355/5000----Loss: 325.16204833984375\n",
      "Epochs: 356/5000----Loss: 325.14788818359375\n",
      "Epochs: 357/5000----Loss: 325.1337585449219\n",
      "Epochs: 358/5000----Loss: 325.11968994140625\n",
      "Epochs: 359/5000----Loss: 325.10565185546875\n",
      "Epochs: 360/5000----Loss: 325.0916748046875\n",
      "Epochs: 361/5000----Loss: 325.0775451660156\n",
      "Epochs: 362/5000----Loss: 325.06378173828125\n",
      "Epochs: 363/5000----Loss: 325.0498046875\n",
      "Epochs: 364/5000----Loss: 325.035888671875\n",
      "Epochs: 365/5000----Loss: 325.02203369140625\n",
      "Epochs: 366/5000----Loss: 325.0080871582031\n",
      "Epochs: 367/5000----Loss: 324.994384765625\n",
      "Epochs: 368/5000----Loss: 324.98065185546875\n",
      "Epochs: 369/5000----Loss: 324.9667663574219\n",
      "Epochs: 370/5000----Loss: 324.9530944824219\n",
      "Epochs: 371/5000----Loss: 324.93951416015625\n",
      "Epochs: 372/5000----Loss: 324.9258728027344\n",
      "Epochs: 373/5000----Loss: 324.912353515625\n",
      "Epochs: 374/5000----Loss: 324.8987121582031\n",
      "Epochs: 375/5000----Loss: 324.8851623535156\n",
      "Epochs: 376/5000----Loss: 324.87176513671875\n",
      "Epochs: 377/5000----Loss: 324.8581848144531\n",
      "Epochs: 378/5000----Loss: 324.8447265625\n",
      "Epochs: 379/5000----Loss: 324.83135986328125\n",
      "Epochs: 380/5000----Loss: 324.8180236816406\n",
      "Epochs: 381/5000----Loss: 324.8046875\n",
      "Epochs: 382/5000----Loss: 324.7914123535156\n",
      "Epochs: 383/5000----Loss: 324.778076171875\n",
      "Epochs: 384/5000----Loss: 324.76483154296875\n",
      "Epochs: 385/5000----Loss: 324.7515563964844\n",
      "Epochs: 386/5000----Loss: 324.7383117675781\n",
      "Epochs: 387/5000----Loss: 324.7252502441406\n",
      "Epochs: 388/5000----Loss: 324.71197509765625\n",
      "Epochs: 389/5000----Loss: 324.69891357421875\n",
      "Epochs: 390/5000----Loss: 324.6858825683594\n",
      "Epochs: 391/5000----Loss: 324.6727600097656\n",
      "Epochs: 392/5000----Loss: 324.6598205566406\n",
      "Epochs: 393/5000----Loss: 324.6468811035156\n",
      "Epochs: 394/5000----Loss: 324.6337890625\n",
      "Epochs: 395/5000----Loss: 324.620849609375\n",
      "Epochs: 396/5000----Loss: 324.60784912109375\n",
      "Epochs: 397/5000----Loss: 324.5950622558594\n",
      "Epochs: 398/5000----Loss: 324.5824279785156\n",
      "Epochs: 399/5000----Loss: 324.56939697265625\n",
      "Epochs: 400/5000----Loss: 324.5567321777344\n",
      "Epochs: 401/5000----Loss: 324.5438537597656\n",
      "Epochs: 402/5000----Loss: 324.5312194824219\n",
      "Epochs: 403/5000----Loss: 324.5185241699219\n",
      "Epochs: 404/5000----Loss: 324.5058288574219\n",
      "Epochs: 405/5000----Loss: 324.49322509765625\n",
      "Epochs: 406/5000----Loss: 324.4805603027344\n",
      "Epochs: 407/5000----Loss: 324.468017578125\n",
      "Epochs: 408/5000----Loss: 324.4554138183594\n",
      "Epochs: 409/5000----Loss: 324.44293212890625\n",
      "Epochs: 410/5000----Loss: 324.43048095703125\n",
      "Epochs: 411/5000----Loss: 324.41796875\n",
      "Epochs: 412/5000----Loss: 324.405517578125\n",
      "Epochs: 413/5000----Loss: 324.3930358886719\n",
      "Epochs: 414/5000----Loss: 324.3806457519531\n",
      "Epochs: 415/5000----Loss: 324.36834716796875\n",
      "Epochs: 416/5000----Loss: 324.35601806640625\n",
      "Epochs: 417/5000----Loss: 324.3437194824219\n",
      "Epochs: 418/5000----Loss: 324.3314514160156\n",
      "Epochs: 419/5000----Loss: 324.31915283203125\n",
      "Epochs: 420/5000----Loss: 324.3070373535156\n",
      "Epochs: 421/5000----Loss: 324.29486083984375\n",
      "Epochs: 422/5000----Loss: 324.28265380859375\n",
      "Epochs: 423/5000----Loss: 324.2705383300781\n",
      "Epochs: 424/5000----Loss: 324.2584533691406\n",
      "Epochs: 425/5000----Loss: 324.2464599609375\n",
      "Epochs: 426/5000----Loss: 324.2343444824219\n",
      "Epochs: 427/5000----Loss: 324.2222595214844\n",
      "Epochs: 428/5000----Loss: 324.2104187011719\n",
      "Epochs: 429/5000----Loss: 324.1983947753906\n",
      "Epochs: 430/5000----Loss: 324.1864013671875\n",
      "Epochs: 431/5000----Loss: 324.1744689941406\n",
      "Epochs: 432/5000----Loss: 324.1626892089844\n",
      "Epochs: 433/5000----Loss: 324.15069580078125\n",
      "Epochs: 434/5000----Loss: 324.1388854980469\n",
      "Epochs: 435/5000----Loss: 324.12701416015625\n",
      "Epochs: 436/5000----Loss: 324.115234375\n",
      "Epochs: 437/5000----Loss: 324.10357666015625\n",
      "Epochs: 438/5000----Loss: 324.0917663574219\n",
      "Epochs: 439/5000----Loss: 324.080078125\n",
      "Epochs: 440/5000----Loss: 324.06842041015625\n",
      "Epochs: 441/5000----Loss: 324.05670166015625\n",
      "Epochs: 442/5000----Loss: 324.0451354980469\n",
      "Epochs: 443/5000----Loss: 324.03350830078125\n",
      "Epochs: 444/5000----Loss: 324.0220031738281\n",
      "Epochs: 445/5000----Loss: 324.01043701171875\n",
      "Epochs: 446/5000----Loss: 323.9988708496094\n",
      "Epochs: 447/5000----Loss: 323.98736572265625\n",
      "Epochs: 448/5000----Loss: 323.9759216308594\n",
      "Epochs: 449/5000----Loss: 323.96441650390625\n",
      "Epochs: 450/5000----Loss: 323.9530944824219\n",
      "Epochs: 451/5000----Loss: 323.9416198730469\n",
      "Epochs: 452/5000----Loss: 323.9302673339844\n",
      "Epochs: 453/5000----Loss: 323.9189453125\n",
      "Epochs: 454/5000----Loss: 323.90753173828125\n",
      "Epochs: 455/5000----Loss: 323.8962707519531\n",
      "Epochs: 456/5000----Loss: 323.8849792480469\n",
      "Epochs: 457/5000----Loss: 323.8738708496094\n",
      "Epochs: 458/5000----Loss: 323.862548828125\n",
      "Epochs: 459/5000----Loss: 323.8512878417969\n",
      "Epochs: 460/5000----Loss: 323.84014892578125\n",
      "Epochs: 461/5000----Loss: 323.8289794921875\n",
      "Epochs: 462/5000----Loss: 323.8179931640625\n",
      "Epochs: 463/5000----Loss: 323.80682373046875\n",
      "Epochs: 464/5000----Loss: 323.79571533203125\n",
      "Epochs: 465/5000----Loss: 323.7846984863281\n",
      "Epochs: 466/5000----Loss: 323.77374267578125\n",
      "Epochs: 467/5000----Loss: 323.7626953125\n",
      "Epochs: 468/5000----Loss: 323.75164794921875\n",
      "Epochs: 469/5000----Loss: 323.74066162109375\n",
      "Epochs: 470/5000----Loss: 323.7298278808594\n",
      "Epochs: 471/5000----Loss: 323.7188415527344\n",
      "Epochs: 472/5000----Loss: 323.70806884765625\n",
      "Epochs: 473/5000----Loss: 323.6972351074219\n",
      "Epochs: 474/5000----Loss: 323.68634033203125\n",
      "Epochs: 475/5000----Loss: 323.6756591796875\n",
      "Epochs: 476/5000----Loss: 323.66485595703125\n",
      "Epochs: 477/5000----Loss: 323.654052734375\n",
      "Epochs: 478/5000----Loss: 323.64337158203125\n",
      "Epochs: 479/5000----Loss: 323.6325988769531\n",
      "Epochs: 480/5000----Loss: 323.6219482421875\n",
      "Epochs: 481/5000----Loss: 323.61114501953125\n",
      "Epochs: 482/5000----Loss: 323.6007080078125\n",
      "Epochs: 483/5000----Loss: 323.5899658203125\n",
      "Epochs: 484/5000----Loss: 323.5794372558594\n",
      "Epochs: 485/5000----Loss: 323.56884765625\n",
      "Epochs: 486/5000----Loss: 323.5581970214844\n",
      "Epochs: 487/5000----Loss: 323.5477294921875\n",
      "Epochs: 488/5000----Loss: 323.5372619628906\n",
      "Epochs: 489/5000----Loss: 323.52667236328125\n",
      "Epochs: 490/5000----Loss: 323.51629638671875\n",
      "Epochs: 491/5000----Loss: 323.505859375\n",
      "Epochs: 492/5000----Loss: 323.4953308105469\n",
      "Epochs: 493/5000----Loss: 323.48492431640625\n",
      "Epochs: 494/5000----Loss: 323.4746398925781\n",
      "Epochs: 495/5000----Loss: 323.4642639160156\n",
      "Epochs: 496/5000----Loss: 323.45391845703125\n",
      "Epochs: 497/5000----Loss: 323.443603515625\n",
      "Epochs: 498/5000----Loss: 323.4333801269531\n",
      "Epochs: 499/5000----Loss: 323.4230651855469\n",
      "Epochs: 500/5000----Loss: 323.4128723144531\n",
      "Epochs: 501/5000----Loss: 323.4026794433594\n",
      "Epochs: 502/5000----Loss: 323.3924560546875\n",
      "Epochs: 503/5000----Loss: 323.3822937011719\n",
      "Epochs: 504/5000----Loss: 323.3722229003906\n",
      "Epochs: 505/5000----Loss: 323.3620300292969\n",
      "Epochs: 506/5000----Loss: 323.3518371582031\n",
      "Epochs: 507/5000----Loss: 323.3419494628906\n",
      "Epochs: 508/5000----Loss: 323.3318176269531\n",
      "Epochs: 509/5000----Loss: 323.3217468261719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 510/5000----Loss: 323.31182861328125\n",
      "Epochs: 511/5000----Loss: 323.3017272949219\n",
      "Epochs: 512/5000----Loss: 323.2917785644531\n",
      "Epochs: 513/5000----Loss: 323.28192138671875\n",
      "Epochs: 514/5000----Loss: 323.27191162109375\n",
      "Epochs: 515/5000----Loss: 323.26202392578125\n",
      "Epochs: 516/5000----Loss: 323.2520751953125\n",
      "Epochs: 517/5000----Loss: 323.2422180175781\n",
      "Epochs: 518/5000----Loss: 323.23248291015625\n",
      "Epochs: 519/5000----Loss: 323.22247314453125\n",
      "Epochs: 520/5000----Loss: 323.2127380371094\n",
      "Epochs: 521/5000----Loss: 323.20294189453125\n",
      "Epochs: 522/5000----Loss: 323.19317626953125\n",
      "Epochs: 523/5000----Loss: 323.1835021972656\n",
      "Epochs: 524/5000----Loss: 323.1736755371094\n",
      "Epochs: 525/5000----Loss: 323.1640625\n",
      "Epochs: 526/5000----Loss: 323.154296875\n",
      "Epochs: 527/5000----Loss: 323.1446228027344\n",
      "Epochs: 528/5000----Loss: 323.13507080078125\n",
      "Epochs: 529/5000----Loss: 323.1255187988281\n",
      "Epochs: 530/5000----Loss: 323.1158142089844\n",
      "Epochs: 531/5000----Loss: 323.10638427734375\n",
      "Epochs: 532/5000----Loss: 323.09686279296875\n",
      "Epochs: 533/5000----Loss: 323.08721923828125\n",
      "Epochs: 534/5000----Loss: 323.0777893066406\n",
      "Epochs: 535/5000----Loss: 323.0683288574219\n",
      "Epochs: 536/5000----Loss: 323.0587463378906\n",
      "Epochs: 537/5000----Loss: 323.04937744140625\n",
      "Epochs: 538/5000----Loss: 323.0399475097656\n",
      "Epochs: 539/5000----Loss: 323.0304870605469\n",
      "Epochs: 540/5000----Loss: 323.0211486816406\n",
      "Epochs: 541/5000----Loss: 323.0117492675781\n",
      "Epochs: 542/5000----Loss: 323.00238037109375\n",
      "Epochs: 543/5000----Loss: 322.9930725097656\n",
      "Epochs: 544/5000----Loss: 322.9837646484375\n",
      "Epochs: 545/5000----Loss: 322.9745178222656\n",
      "Epochs: 546/5000----Loss: 322.96514892578125\n",
      "Epochs: 547/5000----Loss: 322.9559631347656\n",
      "Epochs: 548/5000----Loss: 322.9466552734375\n",
      "Epochs: 549/5000----Loss: 322.9375\n",
      "Epochs: 550/5000----Loss: 322.9281921386719\n",
      "Epochs: 551/5000----Loss: 322.919189453125\n",
      "Epochs: 552/5000----Loss: 322.90997314453125\n",
      "Epochs: 553/5000----Loss: 322.9008483886719\n",
      "Epochs: 554/5000----Loss: 322.8917541503906\n",
      "Epochs: 555/5000----Loss: 322.882568359375\n",
      "Epochs: 556/5000----Loss: 322.8735656738281\n",
      "Epochs: 557/5000----Loss: 322.864501953125\n",
      "Epochs: 558/5000----Loss: 322.8554382324219\n",
      "Epochs: 559/5000----Loss: 322.846435546875\n",
      "Epochs: 560/5000----Loss: 322.8373718261719\n",
      "Epochs: 561/5000----Loss: 322.8283996582031\n",
      "Epochs: 562/5000----Loss: 322.8194885253906\n",
      "Epochs: 563/5000----Loss: 322.8105773925781\n",
      "Epochs: 564/5000----Loss: 322.8016357421875\n",
      "Epochs: 565/5000----Loss: 322.79278564453125\n",
      "Epochs: 566/5000----Loss: 322.7838439941406\n",
      "Epochs: 567/5000----Loss: 322.77496337890625\n",
      "Epochs: 568/5000----Loss: 322.7660827636719\n",
      "Epochs: 569/5000----Loss: 322.75726318359375\n",
      "Epochs: 570/5000----Loss: 322.7484436035156\n",
      "Epochs: 571/5000----Loss: 322.7397155761719\n",
      "Epochs: 572/5000----Loss: 322.73089599609375\n",
      "Epochs: 573/5000----Loss: 322.7221984863281\n",
      "Epochs: 574/5000----Loss: 322.71331787109375\n",
      "Epochs: 575/5000----Loss: 322.7047424316406\n",
      "Epochs: 576/5000----Loss: 322.696044921875\n",
      "Epochs: 577/5000----Loss: 322.6874084472656\n",
      "Epochs: 578/5000----Loss: 322.67864990234375\n",
      "Epochs: 579/5000----Loss: 322.6700439453125\n",
      "Epochs: 580/5000----Loss: 322.66143798828125\n",
      "Epochs: 581/5000----Loss: 322.65277099609375\n",
      "Epochs: 582/5000----Loss: 322.64422607421875\n",
      "Epochs: 583/5000----Loss: 322.6356201171875\n",
      "Epochs: 584/5000----Loss: 322.62701416015625\n",
      "Epochs: 585/5000----Loss: 322.6185302734375\n",
      "Epochs: 586/5000----Loss: 322.6099853515625\n",
      "Epochs: 587/5000----Loss: 322.6014709472656\n",
      "Epochs: 588/5000----Loss: 322.593017578125\n",
      "Epochs: 589/5000----Loss: 322.58453369140625\n",
      "Epochs: 590/5000----Loss: 322.57611083984375\n",
      "Epochs: 591/5000----Loss: 322.5676574707031\n",
      "Epochs: 592/5000----Loss: 322.55926513671875\n",
      "Epochs: 593/5000----Loss: 322.55084228515625\n",
      "Epochs: 594/5000----Loss: 322.5425109863281\n",
      "Epochs: 595/5000----Loss: 322.5340881347656\n",
      "Epochs: 596/5000----Loss: 322.5257263183594\n",
      "Epochs: 597/5000----Loss: 322.5173645019531\n",
      "Epochs: 598/5000----Loss: 322.5090637207031\n",
      "Epochs: 599/5000----Loss: 322.5008544921875\n",
      "Epochs: 600/5000----Loss: 322.49261474609375\n",
      "Epochs: 601/5000----Loss: 322.48431396484375\n",
      "Epochs: 602/5000----Loss: 322.47607421875\n",
      "Epochs: 603/5000----Loss: 322.46783447265625\n",
      "Epochs: 604/5000----Loss: 322.45965576171875\n",
      "Epochs: 605/5000----Loss: 322.45166015625\n",
      "Epochs: 606/5000----Loss: 322.4433898925781\n",
      "Epochs: 607/5000----Loss: 322.435302734375\n",
      "Epochs: 608/5000----Loss: 322.42718505859375\n",
      "Epochs: 609/5000----Loss: 322.4189758300781\n",
      "Epochs: 610/5000----Loss: 322.41094970703125\n",
      "Epochs: 611/5000----Loss: 322.40283203125\n",
      "Epochs: 612/5000----Loss: 322.39471435546875\n",
      "Epochs: 613/5000----Loss: 322.3866882324219\n",
      "Epochs: 614/5000----Loss: 322.3787841796875\n",
      "Epochs: 615/5000----Loss: 322.3707275390625\n",
      "Epochs: 616/5000----Loss: 322.36260986328125\n",
      "Epochs: 617/5000----Loss: 322.3546142578125\n",
      "Epochs: 618/5000----Loss: 322.3466491699219\n",
      "Epochs: 619/5000----Loss: 322.338623046875\n",
      "Epochs: 620/5000----Loss: 322.3307800292969\n",
      "Epochs: 621/5000----Loss: 322.3228759765625\n",
      "Epochs: 622/5000----Loss: 322.3149719238281\n",
      "Epochs: 623/5000----Loss: 322.30706787109375\n",
      "Epochs: 624/5000----Loss: 322.29925537109375\n",
      "Epochs: 625/5000----Loss: 322.2914733886719\n",
      "Epochs: 626/5000----Loss: 322.2835388183594\n",
      "Epochs: 627/5000----Loss: 322.2756652832031\n",
      "Epochs: 628/5000----Loss: 322.26800537109375\n",
      "Epochs: 629/5000----Loss: 322.2601318359375\n",
      "Epochs: 630/5000----Loss: 322.25238037109375\n",
      "Epochs: 631/5000----Loss: 322.24456787109375\n",
      "Epochs: 632/5000----Loss: 322.23681640625\n",
      "Epochs: 633/5000----Loss: 322.22918701171875\n",
      "Epochs: 634/5000----Loss: 322.221435546875\n",
      "Epochs: 635/5000----Loss: 322.2137145996094\n",
      "Epochs: 636/5000----Loss: 322.20611572265625\n",
      "Epochs: 637/5000----Loss: 322.198486328125\n",
      "Epochs: 638/5000----Loss: 322.19073486328125\n",
      "Epochs: 639/5000----Loss: 322.1831359863281\n",
      "Epochs: 640/5000----Loss: 322.1755676269531\n",
      "Epochs: 641/5000----Loss: 322.1678771972656\n",
      "Epochs: 642/5000----Loss: 322.1603698730469\n",
      "Epochs: 643/5000----Loss: 322.15277099609375\n",
      "Epochs: 644/5000----Loss: 322.14520263671875\n",
      "Epochs: 645/5000----Loss: 322.13763427734375\n",
      "Epochs: 646/5000----Loss: 322.1301574707031\n",
      "Epochs: 647/5000----Loss: 322.12261962890625\n",
      "Epochs: 648/5000----Loss: 322.1151123046875\n",
      "Epochs: 649/5000----Loss: 322.1077880859375\n",
      "Epochs: 650/5000----Loss: 322.1002197265625\n",
      "Epochs: 651/5000----Loss: 322.09283447265625\n",
      "Epochs: 652/5000----Loss: 322.0854187011719\n",
      "Epochs: 653/5000----Loss: 322.07794189453125\n",
      "Epochs: 654/5000----Loss: 322.07049560546875\n",
      "Epochs: 655/5000----Loss: 322.06317138671875\n",
      "Epochs: 656/5000----Loss: 322.0558166503906\n",
      "Epochs: 657/5000----Loss: 322.04852294921875\n",
      "Epochs: 658/5000----Loss: 322.04119873046875\n",
      "Epochs: 659/5000----Loss: 322.0339050292969\n",
      "Epochs: 660/5000----Loss: 322.0265808105469\n",
      "Epochs: 661/5000----Loss: 322.0192565917969\n",
      "Epochs: 662/5000----Loss: 322.0119323730469\n",
      "Epochs: 663/5000----Loss: 322.0047607421875\n",
      "Epochs: 664/5000----Loss: 321.9974365234375\n",
      "Epochs: 665/5000----Loss: 321.99029541015625\n",
      "Epochs: 666/5000----Loss: 321.9830017089844\n",
      "Epochs: 667/5000----Loss: 321.975830078125\n",
      "Epochs: 668/5000----Loss: 321.9686584472656\n",
      "Epochs: 669/5000----Loss: 321.9615173339844\n",
      "Epochs: 670/5000----Loss: 321.954345703125\n",
      "Epochs: 671/5000----Loss: 321.94720458984375\n",
      "Epochs: 672/5000----Loss: 321.9400329589844\n",
      "Epochs: 673/5000----Loss: 321.93304443359375\n",
      "Epochs: 674/5000----Loss: 321.92584228515625\n",
      "Epochs: 675/5000----Loss: 321.9187316894531\n",
      "Epochs: 676/5000----Loss: 321.91168212890625\n",
      "Epochs: 677/5000----Loss: 321.90460205078125\n",
      "Epochs: 678/5000----Loss: 321.89764404296875\n",
      "Epochs: 679/5000----Loss: 321.8905944824219\n",
      "Epochs: 680/5000----Loss: 321.88348388671875\n",
      "Epochs: 681/5000----Loss: 321.8766174316406\n",
      "Epochs: 682/5000----Loss: 321.8695983886719\n",
      "Epochs: 683/5000----Loss: 321.8626708984375\n",
      "Epochs: 684/5000----Loss: 321.8556823730469\n",
      "Epochs: 685/5000----Loss: 321.84869384765625\n",
      "Epochs: 686/5000----Loss: 321.8417053222656\n",
      "Epochs: 687/5000----Loss: 321.8349304199219\n",
      "Epochs: 688/5000----Loss: 321.82806396484375\n",
      "Epochs: 689/5000----Loss: 321.82110595703125\n",
      "Epochs: 690/5000----Loss: 321.8141784667969\n",
      "Epochs: 691/5000----Loss: 321.807373046875\n",
      "Epochs: 692/5000----Loss: 321.8006591796875\n",
      "Epochs: 693/5000----Loss: 321.793701171875\n",
      "Epochs: 694/5000----Loss: 321.7868957519531\n",
      "Epochs: 695/5000----Loss: 321.7801513671875\n",
      "Epochs: 696/5000----Loss: 321.77325439453125\n",
      "Epochs: 697/5000----Loss: 321.7666320800781\n",
      "Epochs: 698/5000----Loss: 321.7597351074219\n",
      "Epochs: 699/5000----Loss: 321.7530517578125\n",
      "Epochs: 700/5000----Loss: 321.7463684082031\n",
      "Epochs: 701/5000----Loss: 321.7396240234375\n",
      "Epochs: 702/5000----Loss: 321.73284912109375\n",
      "Epochs: 703/5000----Loss: 321.7262878417969\n",
      "Epochs: 704/5000----Loss: 321.71954345703125\n",
      "Epochs: 705/5000----Loss: 321.7128601074219\n",
      "Epochs: 706/5000----Loss: 321.7062683105469\n",
      "Epochs: 707/5000----Loss: 321.6995849609375\n",
      "Epochs: 708/5000----Loss: 321.69293212890625\n",
      "Epochs: 709/5000----Loss: 321.68646240234375\n",
      "Epochs: 710/5000----Loss: 321.6797790527344\n",
      "Epochs: 711/5000----Loss: 321.67327880859375\n",
      "Epochs: 712/5000----Loss: 321.6667175292969\n",
      "Epochs: 713/5000----Loss: 321.66021728515625\n",
      "Epochs: 714/5000----Loss: 321.6536865234375\n",
      "Epochs: 715/5000----Loss: 321.64715576171875\n",
      "Epochs: 716/5000----Loss: 321.6406555175781\n",
      "Epochs: 717/5000----Loss: 321.6341247558594\n",
      "Epochs: 718/5000----Loss: 321.627685546875\n",
      "Epochs: 719/5000----Loss: 321.6211853027344\n",
      "Epochs: 720/5000----Loss: 321.6146545410156\n",
      "Epochs: 721/5000----Loss: 321.60821533203125\n",
      "Epochs: 722/5000----Loss: 321.60186767578125\n",
      "Epochs: 723/5000----Loss: 321.59539794921875\n",
      "Epochs: 724/5000----Loss: 321.5889587402344\n",
      "Epochs: 725/5000----Loss: 321.58258056640625\n",
      "Epochs: 726/5000----Loss: 321.5762023925781\n",
      "Epochs: 727/5000----Loss: 321.5698547363281\n",
      "Epochs: 728/5000----Loss: 321.5634460449219\n",
      "Epochs: 729/5000----Loss: 321.55712890625\n",
      "Epochs: 730/5000----Loss: 321.55072021484375\n",
      "Epochs: 731/5000----Loss: 321.5444641113281\n",
      "Epochs: 732/5000----Loss: 321.5381774902344\n",
      "Epochs: 733/5000----Loss: 321.5318603515625\n",
      "Epochs: 734/5000----Loss: 321.525634765625\n",
      "Epochs: 735/5000----Loss: 321.5192565917969\n",
      "Epochs: 736/5000----Loss: 321.5131530761719\n",
      "Epochs: 737/5000----Loss: 321.50689697265625\n",
      "Epochs: 738/5000----Loss: 321.5006408691406\n",
      "Epochs: 739/5000----Loss: 321.49444580078125\n",
      "Epochs: 740/5000----Loss: 321.4882507324219\n",
      "Epochs: 741/5000----Loss: 321.4820556640625\n",
      "Epochs: 742/5000----Loss: 321.47589111328125\n",
      "Epochs: 743/5000----Loss: 321.4696960449219\n",
      "Epochs: 744/5000----Loss: 321.4635314941406\n",
      "Epochs: 745/5000----Loss: 321.45733642578125\n",
      "Epochs: 746/5000----Loss: 321.4512939453125\n",
      "Epochs: 747/5000----Loss: 321.4450988769531\n",
      "Epochs: 748/5000----Loss: 321.43902587890625\n",
      "Epochs: 749/5000----Loss: 321.43292236328125\n",
      "Epochs: 750/5000----Loss: 321.4267883300781\n",
      "Epochs: 751/5000----Loss: 321.42071533203125\n",
      "Epochs: 752/5000----Loss: 321.4147644042969\n",
      "Epochs: 753/5000----Loss: 321.4088134765625\n",
      "Epochs: 754/5000----Loss: 321.4027099609375\n",
      "Epochs: 755/5000----Loss: 321.39666748046875\n",
      "Epochs: 756/5000----Loss: 321.3906555175781\n",
      "Epochs: 757/5000----Loss: 321.3846435546875\n",
      "Epochs: 758/5000----Loss: 321.3786926269531\n",
      "Epochs: 759/5000----Loss: 321.37261962890625\n",
      "Epochs: 760/5000----Loss: 321.36663818359375\n",
      "Epochs: 761/5000----Loss: 321.36083984375\n",
      "Epochs: 762/5000----Loss: 321.3548278808594\n",
      "Epochs: 763/5000----Loss: 321.3489685058594\n",
      "Epochs: 764/5000----Loss: 321.3429870605469\n",
      "Epochs: 765/5000----Loss: 321.33709716796875\n",
      "Epochs: 766/5000----Loss: 321.3313293457031\n",
      "Epochs: 767/5000----Loss: 321.3253479003906\n",
      "Epochs: 768/5000----Loss: 321.3194274902344\n",
      "Epochs: 769/5000----Loss: 321.3136291503906\n",
      "Epochs: 770/5000----Loss: 321.3078308105469\n",
      "Epochs: 771/5000----Loss: 321.3020324707031\n",
      "Epochs: 772/5000----Loss: 321.2961730957031\n",
      "Epochs: 773/5000----Loss: 321.2903137207031\n",
      "Epochs: 774/5000----Loss: 321.2845764160156\n",
      "Epochs: 775/5000----Loss: 321.2787780761719\n",
      "Epochs: 776/5000----Loss: 321.2730407714844\n",
      "Epochs: 777/5000----Loss: 321.2672424316406\n",
      "Epochs: 778/5000----Loss: 321.261474609375\n",
      "Epochs: 779/5000----Loss: 321.25579833984375\n",
      "Epochs: 780/5000----Loss: 321.25006103515625\n",
      "Epochs: 781/5000----Loss: 321.2443542480469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 782/5000----Loss: 321.23858642578125\n",
      "Epochs: 783/5000----Loss: 321.2330017089844\n",
      "Epochs: 784/5000----Loss: 321.2272644042969\n",
      "Epochs: 785/5000----Loss: 321.22161865234375\n",
      "Epochs: 786/5000----Loss: 321.21600341796875\n",
      "Epochs: 787/5000----Loss: 321.2102966308594\n",
      "Epochs: 788/5000----Loss: 321.20477294921875\n",
      "Epochs: 789/5000----Loss: 321.1990661621094\n",
      "Epochs: 790/5000----Loss: 321.1934509277344\n",
      "Epochs: 791/5000----Loss: 321.18780517578125\n",
      "Epochs: 792/5000----Loss: 321.1823425292969\n",
      "Epochs: 793/5000----Loss: 321.1766662597656\n",
      "Epochs: 794/5000----Loss: 321.1711730957031\n",
      "Epochs: 795/5000----Loss: 321.16546630859375\n",
      "Epochs: 796/5000----Loss: 321.1600341796875\n",
      "Epochs: 797/5000----Loss: 321.15447998046875\n",
      "Epochs: 798/5000----Loss: 321.1490173339844\n",
      "Epochs: 799/5000----Loss: 321.14349365234375\n",
      "Epochs: 800/5000----Loss: 321.137939453125\n",
      "Epochs: 801/5000----Loss: 321.132568359375\n",
      "Epochs: 802/5000----Loss: 321.12701416015625\n",
      "Epochs: 803/5000----Loss: 321.12152099609375\n",
      "Epochs: 804/5000----Loss: 321.1161193847656\n",
      "Epochs: 805/5000----Loss: 321.11065673828125\n",
      "Epochs: 806/5000----Loss: 321.10528564453125\n",
      "Epochs: 807/5000----Loss: 321.0998840332031\n",
      "Epochs: 808/5000----Loss: 321.0944519042969\n",
      "Epochs: 809/5000----Loss: 321.0890808105469\n",
      "Epochs: 810/5000----Loss: 321.08367919921875\n",
      "Epochs: 811/5000----Loss: 321.0782470703125\n",
      "Epochs: 812/5000----Loss: 321.07293701171875\n",
      "Epochs: 813/5000----Loss: 321.0675048828125\n",
      "Epochs: 814/5000----Loss: 321.062255859375\n",
      "Epochs: 815/5000----Loss: 321.05682373046875\n",
      "Epochs: 816/5000----Loss: 321.05145263671875\n",
      "Epochs: 817/5000----Loss: 321.0462341308594\n",
      "Epochs: 818/5000----Loss: 321.0409240722656\n",
      "Epochs: 819/5000----Loss: 321.03570556640625\n",
      "Epochs: 820/5000----Loss: 321.03033447265625\n",
      "Epochs: 821/5000----Loss: 321.0250549316406\n",
      "Epochs: 822/5000----Loss: 321.0198974609375\n",
      "Epochs: 823/5000----Loss: 321.0146484375\n",
      "Epochs: 824/5000----Loss: 321.00927734375\n",
      "Epochs: 825/5000----Loss: 321.004150390625\n",
      "Epochs: 826/5000----Loss: 320.9988708496094\n",
      "Epochs: 827/5000----Loss: 320.99371337890625\n",
      "Epochs: 828/5000----Loss: 320.9883728027344\n",
      "Epochs: 829/5000----Loss: 320.98333740234375\n",
      "Epochs: 830/5000----Loss: 320.9781494140625\n",
      "Epochs: 831/5000----Loss: 320.9729919433594\n",
      "Epochs: 832/5000----Loss: 320.96771240234375\n",
      "Epochs: 833/5000----Loss: 320.96258544921875\n",
      "Epochs: 834/5000----Loss: 320.9573669433594\n",
      "Epochs: 835/5000----Loss: 320.95245361328125\n",
      "Epochs: 836/5000----Loss: 320.9471740722656\n",
      "Epochs: 837/5000----Loss: 320.94207763671875\n",
      "Epochs: 838/5000----Loss: 320.9371643066406\n",
      "Epochs: 839/5000----Loss: 320.93206787109375\n",
      "Epochs: 840/5000----Loss: 320.9270324707031\n",
      "Epochs: 841/5000----Loss: 320.92181396484375\n",
      "Epochs: 842/5000----Loss: 320.9167785644531\n",
      "Epochs: 843/5000----Loss: 320.9117126464844\n",
      "Epochs: 844/5000----Loss: 320.90667724609375\n",
      "Epochs: 845/5000----Loss: 320.9016418457031\n",
      "Epochs: 846/5000----Loss: 320.8966064453125\n",
      "Epochs: 847/5000----Loss: 320.8917236328125\n",
      "Epochs: 848/5000----Loss: 320.8865966796875\n",
      "Epochs: 849/5000----Loss: 320.8817443847656\n",
      "Epochs: 850/5000----Loss: 320.87664794921875\n",
      "Epochs: 851/5000----Loss: 320.87176513671875\n",
      "Epochs: 852/5000----Loss: 320.86676025390625\n",
      "Epochs: 853/5000----Loss: 320.8619384765625\n",
      "Epochs: 854/5000----Loss: 320.8569030761719\n",
      "Epochs: 855/5000----Loss: 320.8519287109375\n",
      "Epochs: 856/5000----Loss: 320.84716796875\n",
      "Epochs: 857/5000----Loss: 320.8421325683594\n",
      "Epochs: 858/5000----Loss: 320.83721923828125\n",
      "Epochs: 859/5000----Loss: 320.83233642578125\n",
      "Epochs: 860/5000----Loss: 320.8275146484375\n",
      "Epochs: 861/5000----Loss: 320.82257080078125\n",
      "Epochs: 862/5000----Loss: 320.8178405761719\n",
      "Epochs: 863/5000----Loss: 320.81292724609375\n",
      "Epochs: 864/5000----Loss: 320.80816650390625\n",
      "Epochs: 865/5000----Loss: 320.80328369140625\n",
      "Epochs: 866/5000----Loss: 320.79840087890625\n",
      "Epochs: 867/5000----Loss: 320.7936706542969\n",
      "Epochs: 868/5000----Loss: 320.7887878417969\n",
      "Epochs: 869/5000----Loss: 320.7840270996094\n",
      "Epochs: 870/5000----Loss: 320.77923583984375\n",
      "Epochs: 871/5000----Loss: 320.7744445800781\n",
      "Epochs: 872/5000----Loss: 320.76983642578125\n",
      "Epochs: 873/5000----Loss: 320.76495361328125\n",
      "Epochs: 874/5000----Loss: 320.7602233886719\n",
      "Epochs: 875/5000----Loss: 320.7555236816406\n",
      "Epochs: 876/5000----Loss: 320.75079345703125\n",
      "Epochs: 877/5000----Loss: 320.7461242675781\n",
      "Epochs: 878/5000----Loss: 320.7413635253906\n",
      "Epochs: 879/5000----Loss: 320.73681640625\n",
      "Epochs: 880/5000----Loss: 320.7319641113281\n",
      "Epochs: 881/5000----Loss: 320.7272644042969\n",
      "Epochs: 882/5000----Loss: 320.72271728515625\n",
      "Epochs: 883/5000----Loss: 320.7178955078125\n",
      "Epochs: 884/5000----Loss: 320.7134094238281\n",
      "Epochs: 885/5000----Loss: 320.7086486816406\n",
      "Epochs: 886/5000----Loss: 320.7041320800781\n",
      "Epochs: 887/5000----Loss: 320.699462890625\n",
      "Epochs: 888/5000----Loss: 320.6947937011719\n",
      "Epochs: 889/5000----Loss: 320.690185546875\n",
      "Epochs: 890/5000----Loss: 320.6856384277344\n",
      "Epochs: 891/5000----Loss: 320.6810607910156\n",
      "Epochs: 892/5000----Loss: 320.67645263671875\n",
      "Epochs: 893/5000----Loss: 320.67181396484375\n",
      "Epochs: 894/5000----Loss: 320.66729736328125\n",
      "Epochs: 895/5000----Loss: 320.66290283203125\n",
      "Epochs: 896/5000----Loss: 320.6582946777344\n",
      "Epochs: 897/5000----Loss: 320.65374755859375\n",
      "Epochs: 898/5000----Loss: 320.6491394042969\n",
      "Epochs: 899/5000----Loss: 320.64471435546875\n",
      "Epochs: 900/5000----Loss: 320.64019775390625\n",
      "Epochs: 901/5000----Loss: 320.6356201171875\n",
      "Epochs: 902/5000----Loss: 320.6312561035156\n",
      "Epochs: 903/5000----Loss: 320.6267395019531\n",
      "Epochs: 904/5000----Loss: 320.62225341796875\n",
      "Epochs: 905/5000----Loss: 320.61785888671875\n",
      "Epochs: 906/5000----Loss: 320.61328125\n",
      "Epochs: 907/5000----Loss: 320.60894775390625\n",
      "Epochs: 908/5000----Loss: 320.6044616699219\n",
      "Epochs: 909/5000----Loss: 320.6000061035156\n",
      "Epochs: 910/5000----Loss: 320.59564208984375\n",
      "Epochs: 911/5000----Loss: 320.5911865234375\n",
      "Epochs: 912/5000----Loss: 320.58685302734375\n",
      "Epochs: 913/5000----Loss: 320.5823974609375\n",
      "Epochs: 914/5000----Loss: 320.5780944824219\n",
      "Epochs: 915/5000----Loss: 320.5736999511719\n",
      "Epochs: 916/5000----Loss: 320.5693664550781\n",
      "Epochs: 917/5000----Loss: 320.56494140625\n",
      "Epochs: 918/5000----Loss: 320.5606384277344\n",
      "Epochs: 919/5000----Loss: 320.5562438964844\n",
      "Epochs: 920/5000----Loss: 320.5520324707031\n",
      "Epochs: 921/5000----Loss: 320.5476379394531\n",
      "Epochs: 922/5000----Loss: 320.5433044433594\n",
      "Epochs: 923/5000----Loss: 320.53900146484375\n",
      "Epochs: 924/5000----Loss: 320.5347595214844\n",
      "Epochs: 925/5000----Loss: 320.5303955078125\n",
      "Epochs: 926/5000----Loss: 320.52618408203125\n",
      "Epochs: 927/5000----Loss: 320.5218811035156\n",
      "Epochs: 928/5000----Loss: 320.517578125\n",
      "Epochs: 929/5000----Loss: 320.513427734375\n",
      "Epochs: 930/5000----Loss: 320.5091247558594\n",
      "Epochs: 931/5000----Loss: 320.5049133300781\n",
      "Epochs: 932/5000----Loss: 320.5007019042969\n",
      "Epochs: 933/5000----Loss: 320.49639892578125\n",
      "Epochs: 934/5000----Loss: 320.49224853515625\n",
      "Epochs: 935/5000----Loss: 320.488037109375\n",
      "Epochs: 936/5000----Loss: 320.4839172363281\n",
      "Epochs: 937/5000----Loss: 320.4797058105469\n",
      "Epochs: 938/5000----Loss: 320.4754333496094\n",
      "Epochs: 939/5000----Loss: 320.4712829589844\n",
      "Epochs: 940/5000----Loss: 320.46710205078125\n",
      "Epochs: 941/5000----Loss: 320.46295166015625\n",
      "Epochs: 942/5000----Loss: 320.45892333984375\n",
      "Epochs: 943/5000----Loss: 320.45477294921875\n",
      "Epochs: 944/5000----Loss: 320.45062255859375\n",
      "Epochs: 945/5000----Loss: 320.44647216796875\n",
      "Epochs: 946/5000----Loss: 320.44232177734375\n",
      "Epochs: 947/5000----Loss: 320.43817138671875\n",
      "Epochs: 948/5000----Loss: 320.4341735839844\n",
      "Epochs: 949/5000----Loss: 320.4300537109375\n",
      "Epochs: 950/5000----Loss: 320.4259948730469\n",
      "Epochs: 951/5000----Loss: 320.4219055175781\n",
      "Epochs: 952/5000----Loss: 320.41790771484375\n",
      "Epochs: 953/5000----Loss: 320.4137268066406\n",
      "Epochs: 954/5000----Loss: 320.40972900390625\n",
      "Epochs: 955/5000----Loss: 320.40570068359375\n",
      "Epochs: 956/5000----Loss: 320.40167236328125\n",
      "Epochs: 957/5000----Loss: 320.39752197265625\n",
      "Epochs: 958/5000----Loss: 320.3935852050781\n",
      "Epochs: 959/5000----Loss: 320.3896484375\n",
      "Epochs: 960/5000----Loss: 320.38555908203125\n",
      "Epochs: 961/5000----Loss: 320.381591796875\n",
      "Epochs: 962/5000----Loss: 320.377685546875\n",
      "Epochs: 963/5000----Loss: 320.37371826171875\n",
      "Epochs: 964/5000----Loss: 320.3697204589844\n",
      "Epochs: 965/5000----Loss: 320.3658142089844\n",
      "Epochs: 966/5000----Loss: 320.36175537109375\n",
      "Epochs: 967/5000----Loss: 320.3578186035156\n",
      "Epochs: 968/5000----Loss: 320.3538818359375\n",
      "Epochs: 969/5000----Loss: 320.349853515625\n",
      "Epochs: 970/5000----Loss: 320.3460693359375\n",
      "Epochs: 971/5000----Loss: 320.34210205078125\n",
      "Epochs: 972/5000----Loss: 320.33819580078125\n",
      "Epochs: 973/5000----Loss: 320.33428955078125\n",
      "Epochs: 974/5000----Loss: 320.3304443359375\n",
      "Epochs: 975/5000----Loss: 320.3265380859375\n",
      "Epochs: 976/5000----Loss: 320.3226623535156\n",
      "Epochs: 977/5000----Loss: 320.3188781738281\n",
      "Epochs: 978/5000----Loss: 320.3148193359375\n",
      "Epochs: 979/5000----Loss: 320.31109619140625\n",
      "Epochs: 980/5000----Loss: 320.30718994140625\n",
      "Epochs: 981/5000----Loss: 320.303466796875\n",
      "Epochs: 982/5000----Loss: 320.299560546875\n",
      "Epochs: 983/5000----Loss: 320.2958068847656\n",
      "Epochs: 984/5000----Loss: 320.29193115234375\n",
      "Epochs: 985/5000----Loss: 320.28802490234375\n",
      "Epochs: 986/5000----Loss: 320.2843017578125\n",
      "Epochs: 987/5000----Loss: 320.28045654296875\n",
      "Epochs: 988/5000----Loss: 320.2767028808594\n",
      "Epochs: 989/5000----Loss: 320.27288818359375\n",
      "Epochs: 990/5000----Loss: 320.269287109375\n",
      "Epochs: 991/5000----Loss: 320.2654113769531\n",
      "Epochs: 992/5000----Loss: 320.26171875\n",
      "Epochs: 993/5000----Loss: 320.25787353515625\n",
      "Epochs: 994/5000----Loss: 320.2541809082031\n",
      "Epochs: 995/5000----Loss: 320.2504577636719\n",
      "Epochs: 996/5000----Loss: 320.2467041015625\n",
      "Epochs: 997/5000----Loss: 320.2430725097656\n",
      "Epochs: 998/5000----Loss: 320.23931884765625\n",
      "Epochs: 999/5000----Loss: 320.2355651855469\n",
      "Epochs: 1000/5000----Loss: 320.2319641113281\n",
      "Epochs: 1001/5000----Loss: 320.22821044921875\n",
      "Epochs: 1002/5000----Loss: 320.2245788574219\n",
      "Epochs: 1003/5000----Loss: 320.22088623046875\n",
      "Epochs: 1004/5000----Loss: 320.2171325683594\n",
      "Epochs: 1005/5000----Loss: 320.2135314941406\n",
      "Epochs: 1006/5000----Loss: 320.20989990234375\n",
      "Epochs: 1007/5000----Loss: 320.2060546875\n",
      "Epochs: 1008/5000----Loss: 320.2026672363281\n",
      "Epochs: 1009/5000----Loss: 320.19891357421875\n",
      "Epochs: 1010/5000----Loss: 320.1953125\n",
      "Epochs: 1011/5000----Loss: 320.19171142578125\n",
      "Epochs: 1012/5000----Loss: 320.1880798339844\n",
      "Epochs: 1013/5000----Loss: 320.1844787597656\n",
      "Epochs: 1014/5000----Loss: 320.18084716796875\n",
      "Epochs: 1015/5000----Loss: 320.17724609375\n",
      "Epochs: 1016/5000----Loss: 320.17376708984375\n",
      "Epochs: 1017/5000----Loss: 320.17010498046875\n",
      "Epochs: 1018/5000----Loss: 320.16650390625\n",
      "Epochs: 1019/5000----Loss: 320.16302490234375\n",
      "Epochs: 1020/5000----Loss: 320.159423828125\n",
      "Epochs: 1021/5000----Loss: 320.1559143066406\n",
      "Epochs: 1022/5000----Loss: 320.15240478515625\n",
      "Epochs: 1023/5000----Loss: 320.14886474609375\n",
      "Epochs: 1024/5000----Loss: 320.145263671875\n",
      "Epochs: 1025/5000----Loss: 320.1417541503906\n",
      "Epochs: 1026/5000----Loss: 320.13836669921875\n",
      "Epochs: 1027/5000----Loss: 320.1347351074219\n",
      "Epochs: 1028/5000----Loss: 320.1312561035156\n",
      "Epochs: 1029/5000----Loss: 320.12774658203125\n",
      "Epochs: 1030/5000----Loss: 320.12432861328125\n",
      "Epochs: 1031/5000----Loss: 320.12078857421875\n",
      "Epochs: 1032/5000----Loss: 320.11737060546875\n",
      "Epochs: 1033/5000----Loss: 320.11383056640625\n",
      "Epochs: 1034/5000----Loss: 320.11041259765625\n",
      "Epochs: 1035/5000----Loss: 320.10693359375\n",
      "Epochs: 1036/5000----Loss: 320.1034240722656\n",
      "Epochs: 1037/5000----Loss: 320.10003662109375\n",
      "Epochs: 1038/5000----Loss: 320.0967102050781\n",
      "Epochs: 1039/5000----Loss: 320.09326171875\n",
      "Epochs: 1040/5000----Loss: 320.0897521972656\n",
      "Epochs: 1041/5000----Loss: 320.0863952636719\n",
      "Epochs: 1042/5000----Loss: 320.08294677734375\n",
      "Epochs: 1043/5000----Loss: 320.0794677734375\n",
      "Epochs: 1044/5000----Loss: 320.0761413574219\n",
      "Epochs: 1045/5000----Loss: 320.0728454589844\n",
      "Epochs: 1046/5000----Loss: 320.0694885253906\n",
      "Epochs: 1047/5000----Loss: 320.0660705566406\n",
      "Epochs: 1048/5000----Loss: 320.0626220703125\n",
      "Epochs: 1049/5000----Loss: 320.05938720703125\n",
      "Epochs: 1050/5000----Loss: 320.0559387207031\n",
      "Epochs: 1051/5000----Loss: 320.0526428222656\n",
      "Epochs: 1052/5000----Loss: 320.0492248535156\n",
      "Epochs: 1053/5000----Loss: 320.04595947265625\n",
      "Epochs: 1054/5000----Loss: 320.0426330566406\n",
      "Epochs: 1055/5000----Loss: 320.0392761230469\n",
      "Epochs: 1056/5000----Loss: 320.03594970703125\n",
      "Epochs: 1057/5000----Loss: 320.0325927734375\n",
      "Epochs: 1058/5000----Loss: 320.02935791015625\n",
      "Epochs: 1059/5000----Loss: 320.026123046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1060/5000----Loss: 320.0228271484375\n",
      "Epochs: 1061/5000----Loss: 320.0195007324219\n",
      "Epochs: 1062/5000----Loss: 320.0162048339844\n",
      "Epochs: 1063/5000----Loss: 320.01300048828125\n",
      "Epochs: 1064/5000----Loss: 320.00970458984375\n",
      "Epochs: 1065/5000----Loss: 320.00640869140625\n",
      "Epochs: 1066/5000----Loss: 320.0031433105469\n",
      "Epochs: 1067/5000----Loss: 319.99993896484375\n",
      "Epochs: 1068/5000----Loss: 319.9967041015625\n",
      "Epochs: 1069/5000----Loss: 319.9934387207031\n",
      "Epochs: 1070/5000----Loss: 319.99029541015625\n",
      "Epochs: 1071/5000----Loss: 319.9870910644531\n",
      "Epochs: 1072/5000----Loss: 319.9838562011719\n",
      "Epochs: 1073/5000----Loss: 319.98065185546875\n",
      "Epochs: 1074/5000----Loss: 319.9773864746094\n",
      "Epochs: 1075/5000----Loss: 319.9742736816406\n",
      "Epochs: 1076/5000----Loss: 319.9711608886719\n",
      "Epochs: 1077/5000----Loss: 319.96783447265625\n",
      "Epochs: 1078/5000----Loss: 319.9647216796875\n",
      "Epochs: 1079/5000----Loss: 319.96160888671875\n",
      "Epochs: 1080/5000----Loss: 319.9583435058594\n",
      "Epochs: 1081/5000----Loss: 319.95526123046875\n",
      "Epochs: 1082/5000----Loss: 319.9520263671875\n",
      "Epochs: 1083/5000----Loss: 319.94891357421875\n",
      "Epochs: 1084/5000----Loss: 319.9458312988281\n",
      "Epochs: 1085/5000----Loss: 319.94268798828125\n",
      "Epochs: 1086/5000----Loss: 319.9395751953125\n",
      "Epochs: 1087/5000----Loss: 319.9364318847656\n",
      "Epochs: 1088/5000----Loss: 319.93328857421875\n",
      "Epochs: 1089/5000----Loss: 319.93023681640625\n",
      "Epochs: 1090/5000----Loss: 319.92706298828125\n",
      "Epochs: 1091/5000----Loss: 319.9239196777344\n",
      "Epochs: 1092/5000----Loss: 319.92095947265625\n",
      "Epochs: 1093/5000----Loss: 319.9178771972656\n",
      "Epochs: 1094/5000----Loss: 319.9147644042969\n",
      "Epochs: 1095/5000----Loss: 319.9115905761719\n",
      "Epochs: 1096/5000----Loss: 319.9085998535156\n",
      "Epochs: 1097/5000----Loss: 319.90557861328125\n",
      "Epochs: 1098/5000----Loss: 319.9024353027344\n",
      "Epochs: 1099/5000----Loss: 319.89947509765625\n",
      "Epochs: 1100/5000----Loss: 319.8965148925781\n",
      "Epochs: 1101/5000----Loss: 319.8934020996094\n",
      "Epochs: 1102/5000----Loss: 319.8904113769531\n",
      "Epochs: 1103/5000----Loss: 319.8873291015625\n",
      "Epochs: 1104/5000----Loss: 319.8842468261719\n",
      "Epochs: 1105/5000----Loss: 319.88134765625\n",
      "Epochs: 1106/5000----Loss: 319.8781433105469\n",
      "Epochs: 1107/5000----Loss: 319.8752746582031\n",
      "Epochs: 1108/5000----Loss: 319.8722839355469\n",
      "Epochs: 1109/5000----Loss: 319.86920166015625\n",
      "Epochs: 1110/5000----Loss: 319.8663330078125\n",
      "Epochs: 1111/5000----Loss: 319.8634033203125\n",
      "Epochs: 1112/5000----Loss: 319.86029052734375\n",
      "Epochs: 1113/5000----Loss: 319.85736083984375\n",
      "Epochs: 1114/5000----Loss: 319.85443115234375\n",
      "Epochs: 1115/5000----Loss: 319.8514099121094\n",
      "Epochs: 1116/5000----Loss: 319.8485412597656\n",
      "Epochs: 1117/5000----Loss: 319.8456115722656\n",
      "Epochs: 1118/5000----Loss: 319.8426513671875\n",
      "Epochs: 1119/5000----Loss: 319.83978271484375\n",
      "Epochs: 1120/5000----Loss: 319.8368225097656\n",
      "Epochs: 1121/5000----Loss: 319.8338928222656\n",
      "Epochs: 1122/5000----Loss: 319.83099365234375\n",
      "Epochs: 1123/5000----Loss: 319.8280334472656\n",
      "Epochs: 1124/5000----Loss: 319.8252258300781\n",
      "Epochs: 1125/5000----Loss: 319.822265625\n",
      "Epochs: 1126/5000----Loss: 319.8193359375\n",
      "Epochs: 1127/5000----Loss: 319.81658935546875\n",
      "Epochs: 1128/5000----Loss: 319.81365966796875\n",
      "Epochs: 1129/5000----Loss: 319.8107604980469\n",
      "Epochs: 1130/5000----Loss: 319.80780029296875\n",
      "Epochs: 1131/5000----Loss: 319.80511474609375\n",
      "Epochs: 1132/5000----Loss: 319.80230712890625\n",
      "Epochs: 1133/5000----Loss: 319.7992858886719\n",
      "Epochs: 1134/5000----Loss: 319.79656982421875\n",
      "Epochs: 1135/5000----Loss: 319.79376220703125\n",
      "Epochs: 1136/5000----Loss: 319.7907409667969\n",
      "Epochs: 1137/5000----Loss: 319.78802490234375\n",
      "Epochs: 1138/5000----Loss: 319.7850646972656\n",
      "Epochs: 1139/5000----Loss: 319.7823791503906\n",
      "Epochs: 1140/5000----Loss: 319.7795104980469\n",
      "Epochs: 1141/5000----Loss: 319.7766418457031\n",
      "Epochs: 1142/5000----Loss: 319.77392578125\n",
      "Epochs: 1143/5000----Loss: 319.7711181640625\n",
      "Epochs: 1144/5000----Loss: 319.76837158203125\n",
      "Epochs: 1145/5000----Loss: 319.76556396484375\n",
      "Epochs: 1146/5000----Loss: 319.76287841796875\n",
      "Epochs: 1147/5000----Loss: 319.75994873046875\n",
      "Epochs: 1148/5000----Loss: 319.75726318359375\n",
      "Epochs: 1149/5000----Loss: 319.75439453125\n",
      "Epochs: 1150/5000----Loss: 319.7518005371094\n",
      "Epochs: 1151/5000----Loss: 319.7489013671875\n",
      "Epochs: 1152/5000----Loss: 319.74627685546875\n",
      "Epochs: 1153/5000----Loss: 319.743408203125\n",
      "Epochs: 1154/5000----Loss: 319.7408142089844\n",
      "Epochs: 1155/5000----Loss: 319.7380065917969\n",
      "Epochs: 1156/5000----Loss: 319.73529052734375\n",
      "Epochs: 1157/5000----Loss: 319.73260498046875\n",
      "Epochs: 1158/5000----Loss: 319.72991943359375\n",
      "Epochs: 1159/5000----Loss: 319.7270812988281\n",
      "Epochs: 1160/5000----Loss: 319.7244567871094\n",
      "Epochs: 1161/5000----Loss: 319.7217102050781\n",
      "Epochs: 1162/5000----Loss: 319.71905517578125\n",
      "Epochs: 1163/5000----Loss: 319.7164001464844\n",
      "Epochs: 1164/5000----Loss: 319.7136535644531\n",
      "Epochs: 1165/5000----Loss: 319.71099853515625\n",
      "Epochs: 1166/5000----Loss: 319.7083740234375\n",
      "Epochs: 1167/5000----Loss: 319.7056884765625\n",
      "Epochs: 1168/5000----Loss: 319.7030029296875\n",
      "Epochs: 1169/5000----Loss: 319.70037841796875\n",
      "Epochs: 1170/5000----Loss: 319.69769287109375\n",
      "Epochs: 1171/5000----Loss: 319.6950378417969\n",
      "Epochs: 1172/5000----Loss: 319.6923828125\n",
      "Epochs: 1173/5000----Loss: 319.6898193359375\n",
      "Epochs: 1174/5000----Loss: 319.6871032714844\n",
      "Epochs: 1175/5000----Loss: 319.68450927734375\n",
      "Epochs: 1176/5000----Loss: 319.6818542480469\n",
      "Epochs: 1177/5000----Loss: 319.67926025390625\n",
      "Epochs: 1178/5000----Loss: 319.6766052246094\n",
      "Epochs: 1179/5000----Loss: 319.6740417480469\n",
      "Epochs: 1180/5000----Loss: 319.6713562011719\n",
      "Epochs: 1181/5000----Loss: 319.66888427734375\n",
      "Epochs: 1182/5000----Loss: 319.666259765625\n",
      "Epochs: 1183/5000----Loss: 319.66363525390625\n",
      "Epochs: 1184/5000----Loss: 319.66107177734375\n",
      "Epochs: 1185/5000----Loss: 319.6585693359375\n",
      "Epochs: 1186/5000----Loss: 319.6558837890625\n",
      "Epochs: 1187/5000----Loss: 319.65338134765625\n",
      "Epochs: 1188/5000----Loss: 319.6508483886719\n",
      "Epochs: 1189/5000----Loss: 319.6482238769531\n",
      "Epochs: 1190/5000----Loss: 319.64569091796875\n",
      "Epochs: 1191/5000----Loss: 319.6430969238281\n",
      "Epochs: 1192/5000----Loss: 319.64068603515625\n",
      "Epochs: 1193/5000----Loss: 319.63818359375\n",
      "Epochs: 1194/5000----Loss: 319.6355285644531\n",
      "Epochs: 1195/5000----Loss: 319.63299560546875\n",
      "Epochs: 1196/5000----Loss: 319.63043212890625\n",
      "Epochs: 1197/5000----Loss: 319.62799072265625\n",
      "Epochs: 1198/5000----Loss: 319.6253967285156\n",
      "Epochs: 1199/5000----Loss: 319.6229248046875\n",
      "Epochs: 1200/5000----Loss: 319.62042236328125\n",
      "Epochs: 1201/5000----Loss: 319.6180114746094\n",
      "Epochs: 1202/5000----Loss: 319.61541748046875\n",
      "Epochs: 1203/5000----Loss: 319.61297607421875\n",
      "Epochs: 1204/5000----Loss: 319.61053466796875\n",
      "Epochs: 1205/5000----Loss: 319.6080322265625\n",
      "Epochs: 1206/5000----Loss: 319.6053771972656\n",
      "Epochs: 1207/5000----Loss: 319.6029968261719\n",
      "Epochs: 1208/5000----Loss: 319.6005554199219\n",
      "Epochs: 1209/5000----Loss: 319.59808349609375\n",
      "Epochs: 1210/5000----Loss: 319.59552001953125\n",
      "Epochs: 1211/5000----Loss: 319.59307861328125\n",
      "Epochs: 1212/5000----Loss: 319.5906982421875\n",
      "Epochs: 1213/5000----Loss: 319.5882568359375\n",
      "Epochs: 1214/5000----Loss: 319.5858154296875\n",
      "Epochs: 1215/5000----Loss: 319.5834045410156\n",
      "Epochs: 1216/5000----Loss: 319.58099365234375\n",
      "Epochs: 1217/5000----Loss: 319.57843017578125\n",
      "Epochs: 1218/5000----Loss: 319.57611083984375\n",
      "Epochs: 1219/5000----Loss: 319.57366943359375\n",
      "Epochs: 1220/5000----Loss: 319.5713195800781\n",
      "Epochs: 1221/5000----Loss: 319.5689392089844\n",
      "Epochs: 1222/5000----Loss: 319.56646728515625\n",
      "Epochs: 1223/5000----Loss: 319.56414794921875\n",
      "Epochs: 1224/5000----Loss: 319.5618591308594\n",
      "Epochs: 1225/5000----Loss: 319.55938720703125\n",
      "Epochs: 1226/5000----Loss: 319.55706787109375\n",
      "Epochs: 1227/5000----Loss: 319.5545959472656\n",
      "Epochs: 1228/5000----Loss: 319.55218505859375\n",
      "Epochs: 1229/5000----Loss: 319.5498352050781\n",
      "Epochs: 1230/5000----Loss: 319.54754638671875\n",
      "Epochs: 1231/5000----Loss: 319.5451965332031\n",
      "Epochs: 1232/5000----Loss: 319.5428161621094\n",
      "Epochs: 1233/5000----Loss: 319.5404968261719\n",
      "Epochs: 1234/5000----Loss: 319.5380554199219\n",
      "Epochs: 1235/5000----Loss: 319.53582763671875\n",
      "Epochs: 1236/5000----Loss: 319.533447265625\n",
      "Epochs: 1237/5000----Loss: 319.53118896484375\n",
      "Epochs: 1238/5000----Loss: 319.5287170410156\n",
      "Epochs: 1239/5000----Loss: 319.5264892578125\n",
      "Epochs: 1240/5000----Loss: 319.52410888671875\n",
      "Epochs: 1241/5000----Loss: 319.5218200683594\n",
      "Epochs: 1242/5000----Loss: 319.5195617675781\n",
      "Epochs: 1243/5000----Loss: 319.51715087890625\n",
      "Epochs: 1244/5000----Loss: 319.5149841308594\n",
      "Epochs: 1245/5000----Loss: 319.51263427734375\n",
      "Epochs: 1246/5000----Loss: 319.5103454589844\n",
      "Epochs: 1247/5000----Loss: 319.5080871582031\n",
      "Epochs: 1248/5000----Loss: 319.5058288574219\n",
      "Epochs: 1249/5000----Loss: 319.50360107421875\n",
      "Epochs: 1250/5000----Loss: 319.501220703125\n",
      "Epochs: 1251/5000----Loss: 319.4990539550781\n",
      "Epochs: 1252/5000----Loss: 319.4967041015625\n",
      "Epochs: 1253/5000----Loss: 319.4943542480469\n",
      "Epochs: 1254/5000----Loss: 319.4922180175781\n",
      "Epochs: 1255/5000----Loss: 319.48992919921875\n",
      "Epochs: 1256/5000----Loss: 319.4877014160156\n",
      "Epochs: 1257/5000----Loss: 319.4854736328125\n",
      "Epochs: 1258/5000----Loss: 319.483154296875\n",
      "Epochs: 1259/5000----Loss: 319.4808654785156\n",
      "Epochs: 1260/5000----Loss: 319.47882080078125\n",
      "Epochs: 1261/5000----Loss: 319.4765319824219\n",
      "Epochs: 1262/5000----Loss: 319.4742736816406\n",
      "Epochs: 1263/5000----Loss: 319.4721984863281\n",
      "Epochs: 1264/5000----Loss: 319.46990966796875\n",
      "Epochs: 1265/5000----Loss: 319.4678039550781\n",
      "Epochs: 1266/5000----Loss: 319.46551513671875\n",
      "Epochs: 1267/5000----Loss: 319.4632263183594\n",
      "Epochs: 1268/5000----Loss: 319.46112060546875\n",
      "Epochs: 1269/5000----Loss: 319.4589538574219\n",
      "Epochs: 1270/5000----Loss: 319.4568176269531\n",
      "Epochs: 1271/5000----Loss: 319.4545593261719\n",
      "Epochs: 1272/5000----Loss: 319.452392578125\n",
      "Epochs: 1273/5000----Loss: 319.4501647949219\n",
      "Epochs: 1274/5000----Loss: 319.44805908203125\n",
      "Epochs: 1275/5000----Loss: 319.4458312988281\n",
      "Epochs: 1276/5000----Loss: 319.4436950683594\n",
      "Epochs: 1277/5000----Loss: 319.4416198730469\n",
      "Epochs: 1278/5000----Loss: 319.43939208984375\n",
      "Epochs: 1279/5000----Loss: 319.43719482421875\n",
      "Epochs: 1280/5000----Loss: 319.43511962890625\n",
      "Epochs: 1281/5000----Loss: 319.4330139160156\n",
      "Epochs: 1282/5000----Loss: 319.43084716796875\n",
      "Epochs: 1283/5000----Loss: 319.4287414550781\n",
      "Epochs: 1284/5000----Loss: 319.4266052246094\n",
      "Epochs: 1285/5000----Loss: 319.424560546875\n",
      "Epochs: 1286/5000----Loss: 319.4223327636719\n",
      "Epochs: 1287/5000----Loss: 319.42022705078125\n",
      "Epochs: 1288/5000----Loss: 319.4181823730469\n",
      "Epochs: 1289/5000----Loss: 319.4159851074219\n",
      "Epochs: 1290/5000----Loss: 319.41387939453125\n",
      "Epochs: 1291/5000----Loss: 319.41192626953125\n",
      "Epochs: 1292/5000----Loss: 319.4096984863281\n",
      "Epochs: 1293/5000----Loss: 319.40765380859375\n",
      "Epochs: 1294/5000----Loss: 319.4055480957031\n",
      "Epochs: 1295/5000----Loss: 319.4034423828125\n",
      "Epochs: 1296/5000----Loss: 319.4013671875\n",
      "Epochs: 1297/5000----Loss: 319.3992919921875\n",
      "Epochs: 1298/5000----Loss: 319.3973083496094\n",
      "Epochs: 1299/5000----Loss: 319.395263671875\n",
      "Epochs: 1300/5000----Loss: 319.3931579589844\n",
      "Epochs: 1301/5000----Loss: 319.39105224609375\n",
      "Epochs: 1302/5000----Loss: 319.38909912109375\n",
      "Epochs: 1303/5000----Loss: 319.3869934082031\n",
      "Epochs: 1304/5000----Loss: 319.3848876953125\n",
      "Epochs: 1305/5000----Loss: 319.38287353515625\n",
      "Epochs: 1306/5000----Loss: 319.38079833984375\n",
      "Epochs: 1307/5000----Loss: 319.37884521484375\n",
      "Epochs: 1308/5000----Loss: 319.3768310546875\n",
      "Epochs: 1309/5000----Loss: 319.3747863769531\n",
      "Epochs: 1310/5000----Loss: 319.37286376953125\n",
      "Epochs: 1311/5000----Loss: 319.3707580566406\n",
      "Epochs: 1312/5000----Loss: 319.36865234375\n",
      "Epochs: 1313/5000----Loss: 319.3667297363281\n",
      "Epochs: 1314/5000----Loss: 319.3647155761719\n",
      "Epochs: 1315/5000----Loss: 319.36273193359375\n",
      "Epochs: 1316/5000----Loss: 319.36065673828125\n",
      "Epochs: 1317/5000----Loss: 319.358642578125\n",
      "Epochs: 1318/5000----Loss: 319.356689453125\n",
      "Epochs: 1319/5000----Loss: 319.3547058105469\n",
      "Epochs: 1320/5000----Loss: 319.3528137207031\n",
      "Epochs: 1321/5000----Loss: 319.3507995605469\n",
      "Epochs: 1322/5000----Loss: 319.3487854003906\n",
      "Epochs: 1323/5000----Loss: 319.34686279296875\n",
      "Epochs: 1324/5000----Loss: 319.3447570800781\n",
      "Epochs: 1325/5000----Loss: 319.34283447265625\n",
      "Epochs: 1326/5000----Loss: 319.3409118652344\n",
      "Epochs: 1327/5000----Loss: 319.3388977050781\n",
      "Epochs: 1328/5000----Loss: 319.33697509765625\n",
      "Epochs: 1329/5000----Loss: 319.3350524902344\n",
      "Epochs: 1330/5000----Loss: 319.33306884765625\n",
      "Epochs: 1331/5000----Loss: 319.3311767578125\n",
      "Epochs: 1332/5000----Loss: 319.32916259765625\n",
      "Epochs: 1333/5000----Loss: 319.32733154296875\n",
      "Epochs: 1334/5000----Loss: 319.32537841796875\n",
      "Epochs: 1335/5000----Loss: 319.3233642578125\n",
      "Epochs: 1336/5000----Loss: 319.3215637207031\n",
      "Epochs: 1337/5000----Loss: 319.31964111328125\n",
      "Epochs: 1338/5000----Loss: 319.3177490234375\n",
      "Epochs: 1339/5000----Loss: 319.31585693359375\n",
      "Epochs: 1340/5000----Loss: 319.31390380859375\n",
      "Epochs: 1341/5000----Loss: 319.31195068359375\n",
      "Epochs: 1342/5000----Loss: 319.3100891113281\n",
      "Epochs: 1343/5000----Loss: 319.3081359863281\n",
      "Epochs: 1344/5000----Loss: 319.3063049316406\n",
      "Epochs: 1345/5000----Loss: 319.30438232421875\n",
      "Epochs: 1346/5000----Loss: 319.3024597167969\n",
      "Epochs: 1347/5000----Loss: 319.30059814453125\n",
      "Epochs: 1348/5000----Loss: 319.2987060546875\n",
      "Epochs: 1349/5000----Loss: 319.296875\n",
      "Epochs: 1350/5000----Loss: 319.2949523925781\n",
      "Epochs: 1351/5000----Loss: 319.29302978515625\n",
      "Epochs: 1352/5000----Loss: 319.29107666015625\n",
      "Epochs: 1353/5000----Loss: 319.2892761230469\n",
      "Epochs: 1354/5000----Loss: 319.2874450683594\n",
      "Epochs: 1355/5000----Loss: 319.2855224609375\n",
      "Epochs: 1356/5000----Loss: 319.2837219238281\n",
      "Epochs: 1357/5000----Loss: 319.28192138671875\n",
      "Epochs: 1358/5000----Loss: 319.28009033203125\n",
      "Epochs: 1359/5000----Loss: 319.2782287597656\n",
      "Epochs: 1360/5000----Loss: 319.2763671875\n",
      "Epochs: 1361/5000----Loss: 319.2745361328125\n",
      "Epochs: 1362/5000----Loss: 319.27276611328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1363/5000----Loss: 319.27093505859375\n",
      "Epochs: 1364/5000----Loss: 319.2690734863281\n",
      "Epochs: 1365/5000----Loss: 319.2673645019531\n",
      "Epochs: 1366/5000----Loss: 319.26544189453125\n",
      "Epochs: 1367/5000----Loss: 319.26373291015625\n",
      "Epochs: 1368/5000----Loss: 319.26190185546875\n",
      "Epochs: 1369/5000----Loss: 319.2599792480469\n",
      "Epochs: 1370/5000----Loss: 319.25823974609375\n",
      "Epochs: 1371/5000----Loss: 319.2564697265625\n",
      "Epochs: 1372/5000----Loss: 319.254638671875\n",
      "Epochs: 1373/5000----Loss: 319.2529296875\n",
      "Epochs: 1374/5000----Loss: 319.2510986328125\n",
      "Epochs: 1375/5000----Loss: 319.2492370605469\n",
      "Epochs: 1376/5000----Loss: 319.24749755859375\n",
      "Epochs: 1377/5000----Loss: 319.2456970214844\n",
      "Epochs: 1378/5000----Loss: 319.24383544921875\n",
      "Epochs: 1379/5000----Loss: 319.24212646484375\n",
      "Epochs: 1380/5000----Loss: 319.2403564453125\n",
      "Epochs: 1381/5000----Loss: 319.2386474609375\n",
      "Epochs: 1382/5000----Loss: 319.2369689941406\n",
      "Epochs: 1383/5000----Loss: 319.235107421875\n",
      "Epochs: 1384/5000----Loss: 319.2334289550781\n",
      "Epochs: 1385/5000----Loss: 319.23162841796875\n",
      "Epochs: 1386/5000----Loss: 319.22998046875\n",
      "Epochs: 1387/5000----Loss: 319.2281494140625\n",
      "Epochs: 1388/5000----Loss: 319.2264099121094\n",
      "Epochs: 1389/5000----Loss: 319.2247009277344\n",
      "Epochs: 1390/5000----Loss: 319.222900390625\n",
      "Epochs: 1391/5000----Loss: 319.22125244140625\n",
      "Epochs: 1392/5000----Loss: 319.21954345703125\n",
      "Epochs: 1393/5000----Loss: 319.21783447265625\n",
      "Epochs: 1394/5000----Loss: 319.2160949707031\n",
      "Epochs: 1395/5000----Loss: 319.21429443359375\n",
      "Epochs: 1396/5000----Loss: 319.21258544921875\n",
      "Epochs: 1397/5000----Loss: 319.2109680175781\n",
      "Epochs: 1398/5000----Loss: 319.20916748046875\n",
      "Epochs: 1399/5000----Loss: 319.20745849609375\n",
      "Epochs: 1400/5000----Loss: 319.2058410644531\n",
      "Epochs: 1401/5000----Loss: 319.20416259765625\n",
      "Epochs: 1402/5000----Loss: 319.2023620605469\n",
      "Epochs: 1403/5000----Loss: 319.20074462890625\n",
      "Epochs: 1404/5000----Loss: 319.1990051269531\n",
      "Epochs: 1405/5000----Loss: 319.1973571777344\n",
      "Epochs: 1406/5000----Loss: 319.1956787109375\n",
      "Epochs: 1407/5000----Loss: 319.1939697265625\n",
      "Epochs: 1408/5000----Loss: 319.1922607421875\n",
      "Epochs: 1409/5000----Loss: 319.19061279296875\n",
      "Epochs: 1410/5000----Loss: 319.18890380859375\n",
      "Epochs: 1411/5000----Loss: 319.187255859375\n",
      "Epochs: 1412/5000----Loss: 319.18572998046875\n",
      "Epochs: 1413/5000----Loss: 319.1839294433594\n",
      "Epochs: 1414/5000----Loss: 319.182373046875\n",
      "Epochs: 1415/5000----Loss: 319.18072509765625\n",
      "Epochs: 1416/5000----Loss: 319.1790466308594\n",
      "Epochs: 1417/5000----Loss: 319.177490234375\n",
      "Epochs: 1418/5000----Loss: 319.1757507324219\n",
      "Epochs: 1419/5000----Loss: 319.17413330078125\n",
      "Epochs: 1420/5000----Loss: 319.1724548339844\n",
      "Epochs: 1421/5000----Loss: 319.1708679199219\n",
      "Epochs: 1422/5000----Loss: 319.1691589355469\n",
      "Epochs: 1423/5000----Loss: 319.1676025390625\n",
      "Epochs: 1424/5000----Loss: 319.16595458984375\n",
      "Epochs: 1425/5000----Loss: 319.1644287109375\n",
      "Epochs: 1426/5000----Loss: 319.16278076171875\n",
      "Epochs: 1427/5000----Loss: 319.16107177734375\n",
      "Epochs: 1428/5000----Loss: 319.15948486328125\n",
      "Epochs: 1429/5000----Loss: 319.15789794921875\n",
      "Epochs: 1430/5000----Loss: 319.15631103515625\n",
      "Epochs: 1431/5000----Loss: 319.1546630859375\n",
      "Epochs: 1432/5000----Loss: 319.15313720703125\n",
      "Epochs: 1433/5000----Loss: 319.1514587402344\n",
      "Epochs: 1434/5000----Loss: 319.14996337890625\n",
      "Epochs: 1435/5000----Loss: 319.1483154296875\n",
      "Epochs: 1436/5000----Loss: 319.14678955078125\n",
      "Epochs: 1437/5000----Loss: 319.1452331542969\n",
      "Epochs: 1438/5000----Loss: 319.1435852050781\n",
      "Epochs: 1439/5000----Loss: 319.14202880859375\n",
      "Epochs: 1440/5000----Loss: 319.14044189453125\n",
      "Epochs: 1441/5000----Loss: 319.1388244628906\n",
      "Epochs: 1442/5000----Loss: 319.13726806640625\n",
      "Epochs: 1443/5000----Loss: 319.1357727050781\n",
      "Epochs: 1444/5000----Loss: 319.13421630859375\n",
      "Epochs: 1445/5000----Loss: 319.1326599121094\n",
      "Epochs: 1446/5000----Loss: 319.13104248046875\n",
      "Epochs: 1447/5000----Loss: 319.12957763671875\n",
      "Epochs: 1448/5000----Loss: 319.12786865234375\n",
      "Epochs: 1449/5000----Loss: 319.12652587890625\n",
      "Epochs: 1450/5000----Loss: 319.1249694824219\n",
      "Epochs: 1451/5000----Loss: 319.12335205078125\n",
      "Epochs: 1452/5000----Loss: 319.121826171875\n",
      "Epochs: 1453/5000----Loss: 319.1202697753906\n",
      "Epochs: 1454/5000----Loss: 319.1188049316406\n",
      "Epochs: 1455/5000----Loss: 319.1172180175781\n",
      "Epochs: 1456/5000----Loss: 319.11566162109375\n",
      "Epochs: 1457/5000----Loss: 319.11419677734375\n",
      "Epochs: 1458/5000----Loss: 319.1126708984375\n",
      "Epochs: 1459/5000----Loss: 319.1111145019531\n",
      "Epochs: 1460/5000----Loss: 319.1096496582031\n",
      "Epochs: 1461/5000----Loss: 319.108154296875\n",
      "Epochs: 1462/5000----Loss: 319.10662841796875\n",
      "Epochs: 1463/5000----Loss: 319.10504150390625\n",
      "Epochs: 1464/5000----Loss: 319.1035461425781\n",
      "Epochs: 1465/5000----Loss: 319.10198974609375\n",
      "Epochs: 1466/5000----Loss: 319.10040283203125\n",
      "Epochs: 1467/5000----Loss: 319.09893798828125\n",
      "Epochs: 1468/5000----Loss: 319.0975036621094\n",
      "Epochs: 1469/5000----Loss: 319.09613037109375\n",
      "Epochs: 1470/5000----Loss: 319.0946044921875\n",
      "Epochs: 1471/5000----Loss: 319.0931396484375\n",
      "Epochs: 1472/5000----Loss: 319.09161376953125\n",
      "Epochs: 1473/5000----Loss: 319.090087890625\n",
      "Epochs: 1474/5000----Loss: 319.088623046875\n",
      "Epochs: 1475/5000----Loss: 319.08721923828125\n",
      "Epochs: 1476/5000----Loss: 319.0857238769531\n",
      "Epochs: 1477/5000----Loss: 319.0842590332031\n",
      "Epochs: 1478/5000----Loss: 319.0827941894531\n",
      "Epochs: 1479/5000----Loss: 319.0812683105469\n",
      "Epochs: 1480/5000----Loss: 319.07989501953125\n",
      "Epochs: 1481/5000----Loss: 319.0784606933594\n",
      "Epochs: 1482/5000----Loss: 319.076904296875\n",
      "Epochs: 1483/5000----Loss: 319.0754089355469\n",
      "Epochs: 1484/5000----Loss: 319.0741271972656\n",
      "Epochs: 1485/5000----Loss: 319.07257080078125\n",
      "Epochs: 1486/5000----Loss: 319.0711975097656\n",
      "Epochs: 1487/5000----Loss: 319.06976318359375\n",
      "Epochs: 1488/5000----Loss: 319.0683288574219\n",
      "Epochs: 1489/5000----Loss: 319.06689453125\n",
      "Epochs: 1490/5000----Loss: 319.0654296875\n",
      "Epochs: 1491/5000----Loss: 319.0640563964844\n",
      "Epochs: 1492/5000----Loss: 319.0626525878906\n",
      "Epochs: 1493/5000----Loss: 319.0611572265625\n",
      "Epochs: 1494/5000----Loss: 319.05987548828125\n",
      "Epochs: 1495/5000----Loss: 319.05841064453125\n",
      "Epochs: 1496/5000----Loss: 319.05694580078125\n",
      "Epochs: 1497/5000----Loss: 319.0555419921875\n",
      "Epochs: 1498/5000----Loss: 319.05401611328125\n",
      "Epochs: 1499/5000----Loss: 319.05267333984375\n",
      "Epochs: 1500/5000----Loss: 319.05133056640625\n",
      "Epochs: 1501/5000----Loss: 319.04986572265625\n",
      "Epochs: 1502/5000----Loss: 319.04852294921875\n",
      "Epochs: 1503/5000----Loss: 319.047119140625\n",
      "Epochs: 1504/5000----Loss: 319.0456237792969\n",
      "Epochs: 1505/5000----Loss: 319.0443115234375\n",
      "Epochs: 1506/5000----Loss: 319.04296875\n",
      "Epochs: 1507/5000----Loss: 319.04156494140625\n",
      "Epochs: 1508/5000----Loss: 319.0401611328125\n",
      "Epochs: 1509/5000----Loss: 319.0387878417969\n",
      "Epochs: 1510/5000----Loss: 319.0374450683594\n",
      "Epochs: 1511/5000----Loss: 319.0361022949219\n",
      "Epochs: 1512/5000----Loss: 319.03472900390625\n",
      "Epochs: 1513/5000----Loss: 319.0332946777344\n",
      "Epochs: 1514/5000----Loss: 319.0319519042969\n",
      "Epochs: 1515/5000----Loss: 319.0306396484375\n",
      "Epochs: 1516/5000----Loss: 319.02923583984375\n",
      "Epochs: 1517/5000----Loss: 319.0279235839844\n",
      "Epochs: 1518/5000----Loss: 319.0264892578125\n",
      "Epochs: 1519/5000----Loss: 319.0251159667969\n",
      "Epochs: 1520/5000----Loss: 319.0238342285156\n",
      "Epochs: 1521/5000----Loss: 319.0225830078125\n",
      "Epochs: 1522/5000----Loss: 319.0211181640625\n",
      "Epochs: 1523/5000----Loss: 319.01983642578125\n",
      "Epochs: 1524/5000----Loss: 319.01837158203125\n",
      "Epochs: 1525/5000----Loss: 319.01715087890625\n",
      "Epochs: 1526/5000----Loss: 319.0158386230469\n",
      "Epochs: 1527/5000----Loss: 319.0145263671875\n",
      "Epochs: 1528/5000----Loss: 319.01312255859375\n",
      "Epochs: 1529/5000----Loss: 319.01177978515625\n",
      "Epochs: 1530/5000----Loss: 319.01043701171875\n",
      "Epochs: 1531/5000----Loss: 319.0091552734375\n",
      "Epochs: 1532/5000----Loss: 319.00787353515625\n",
      "Epochs: 1533/5000----Loss: 319.0064697265625\n",
      "Epochs: 1534/5000----Loss: 319.00518798828125\n",
      "Epochs: 1535/5000----Loss: 319.00396728515625\n",
      "Epochs: 1536/5000----Loss: 319.0025939941406\n",
      "Epochs: 1537/5000----Loss: 319.0011901855469\n",
      "Epochs: 1538/5000----Loss: 318.99993896484375\n",
      "Epochs: 1539/5000----Loss: 318.99859619140625\n",
      "Epochs: 1540/5000----Loss: 318.9973449707031\n",
      "Epochs: 1541/5000----Loss: 318.99609375\n",
      "Epochs: 1542/5000----Loss: 318.9947509765625\n",
      "Epochs: 1543/5000----Loss: 318.99346923828125\n",
      "Epochs: 1544/5000----Loss: 318.9921875\n",
      "Epochs: 1545/5000----Loss: 318.9909362792969\n",
      "Epochs: 1546/5000----Loss: 318.9896545410156\n",
      "Epochs: 1547/5000----Loss: 318.9883728027344\n",
      "Epochs: 1548/5000----Loss: 318.9870300292969\n",
      "Epochs: 1549/5000----Loss: 318.98577880859375\n",
      "Epochs: 1550/5000----Loss: 318.9845886230469\n",
      "Epochs: 1551/5000----Loss: 318.9832458496094\n",
      "Epochs: 1552/5000----Loss: 318.9820251464844\n",
      "Epochs: 1553/5000----Loss: 318.9808044433594\n",
      "Epochs: 1554/5000----Loss: 318.9794616699219\n",
      "Epochs: 1555/5000----Loss: 318.978271484375\n",
      "Epochs: 1556/5000----Loss: 318.9769592285156\n",
      "Epochs: 1557/5000----Loss: 318.9757995605469\n",
      "Epochs: 1558/5000----Loss: 318.9744567871094\n",
      "Epochs: 1559/5000----Loss: 318.9732360839844\n",
      "Epochs: 1560/5000----Loss: 318.971923828125\n",
      "Epochs: 1561/5000----Loss: 318.9706726074219\n",
      "Epochs: 1562/5000----Loss: 318.96954345703125\n",
      "Epochs: 1563/5000----Loss: 318.96832275390625\n",
      "Epochs: 1564/5000----Loss: 318.9668884277344\n",
      "Epochs: 1565/5000----Loss: 318.9656066894531\n",
      "Epochs: 1566/5000----Loss: 318.9644775390625\n",
      "Epochs: 1567/5000----Loss: 318.963134765625\n",
      "Epochs: 1568/5000----Loss: 318.9620056152344\n",
      "Epochs: 1569/5000----Loss: 318.96075439453125\n",
      "Epochs: 1570/5000----Loss: 318.9594421386719\n",
      "Epochs: 1571/5000----Loss: 318.9582214355469\n",
      "Epochs: 1572/5000----Loss: 318.95709228515625\n",
      "Epochs: 1573/5000----Loss: 318.9557189941406\n",
      "Epochs: 1574/5000----Loss: 318.95458984375\n",
      "Epochs: 1575/5000----Loss: 318.95343017578125\n",
      "Epochs: 1576/5000----Loss: 318.9522399902344\n",
      "Epochs: 1577/5000----Loss: 318.9509582519531\n",
      "Epochs: 1578/5000----Loss: 318.94970703125\n",
      "Epochs: 1579/5000----Loss: 318.94866943359375\n",
      "Epochs: 1580/5000----Loss: 318.94732666015625\n",
      "Epochs: 1581/5000----Loss: 318.9461364746094\n",
      "Epochs: 1582/5000----Loss: 318.9449462890625\n",
      "Epochs: 1583/5000----Loss: 318.9437561035156\n",
      "Epochs: 1584/5000----Loss: 318.94268798828125\n",
      "Epochs: 1585/5000----Loss: 318.9413757324219\n",
      "Epochs: 1586/5000----Loss: 318.94024658203125\n",
      "Epochs: 1587/5000----Loss: 318.9389953613281\n",
      "Epochs: 1588/5000----Loss: 318.9378356933594\n",
      "Epochs: 1589/5000----Loss: 318.9366149902344\n",
      "Epochs: 1590/5000----Loss: 318.93548583984375\n",
      "Epochs: 1591/5000----Loss: 318.9342346191406\n",
      "Epochs: 1592/5000----Loss: 318.93316650390625\n",
      "Epochs: 1593/5000----Loss: 318.9320373535156\n",
      "Epochs: 1594/5000----Loss: 318.9307556152344\n",
      "Epochs: 1595/5000----Loss: 318.92962646484375\n",
      "Epochs: 1596/5000----Loss: 318.928466796875\n",
      "Epochs: 1597/5000----Loss: 318.9272766113281\n",
      "Epochs: 1598/5000----Loss: 318.9261779785156\n",
      "Epochs: 1599/5000----Loss: 318.9250183105469\n",
      "Epochs: 1600/5000----Loss: 318.92388916015625\n",
      "Epochs: 1601/5000----Loss: 318.92266845703125\n",
      "Epochs: 1602/5000----Loss: 318.92156982421875\n",
      "Epochs: 1603/5000----Loss: 318.92034912109375\n",
      "Epochs: 1604/5000----Loss: 318.91925048828125\n",
      "Epochs: 1605/5000----Loss: 318.9179992675781\n",
      "Epochs: 1606/5000----Loss: 318.91680908203125\n",
      "Epochs: 1607/5000----Loss: 318.915771484375\n",
      "Epochs: 1608/5000----Loss: 318.9146728515625\n",
      "Epochs: 1609/5000----Loss: 318.9134521484375\n",
      "Epochs: 1610/5000----Loss: 318.91229248046875\n",
      "Epochs: 1611/5000----Loss: 318.91119384765625\n",
      "Epochs: 1612/5000----Loss: 318.91009521484375\n",
      "Epochs: 1613/5000----Loss: 318.9089660644531\n",
      "Epochs: 1614/5000----Loss: 318.9079284667969\n",
      "Epochs: 1615/5000----Loss: 318.9067687988281\n",
      "Epochs: 1616/5000----Loss: 318.90557861328125\n",
      "Epochs: 1617/5000----Loss: 318.9044189453125\n",
      "Epochs: 1618/5000----Loss: 318.9033203125\n",
      "Epochs: 1619/5000----Loss: 318.9022216796875\n",
      "Epochs: 1620/5000----Loss: 318.901123046875\n",
      "Epochs: 1621/5000----Loss: 318.89996337890625\n",
      "Epochs: 1622/5000----Loss: 318.89898681640625\n",
      "Epochs: 1623/5000----Loss: 318.89776611328125\n",
      "Epochs: 1624/5000----Loss: 318.89678955078125\n",
      "Epochs: 1625/5000----Loss: 318.8956298828125\n",
      "Epochs: 1626/5000----Loss: 318.8945007324219\n",
      "Epochs: 1627/5000----Loss: 318.8933410644531\n",
      "Epochs: 1628/5000----Loss: 318.89227294921875\n",
      "Epochs: 1629/5000----Loss: 318.8912658691406\n",
      "Epochs: 1630/5000----Loss: 318.8901672363281\n",
      "Epochs: 1631/5000----Loss: 318.8890075683594\n",
      "Epochs: 1632/5000----Loss: 318.887939453125\n",
      "Epochs: 1633/5000----Loss: 318.8868408203125\n",
      "Epochs: 1634/5000----Loss: 318.8857116699219\n",
      "Epochs: 1635/5000----Loss: 318.8846740722656\n",
      "Epochs: 1636/5000----Loss: 318.8835754394531\n",
      "Epochs: 1637/5000----Loss: 318.8825988769531\n",
      "Epochs: 1638/5000----Loss: 318.8815002441406\n",
      "Epochs: 1639/5000----Loss: 318.88043212890625\n",
      "Epochs: 1640/5000----Loss: 318.8792724609375\n",
      "Epochs: 1641/5000----Loss: 318.878173828125\n",
      "Epochs: 1642/5000----Loss: 318.87725830078125\n",
      "Epochs: 1643/5000----Loss: 318.8760681152344\n",
      "Epochs: 1644/5000----Loss: 318.87506103515625\n",
      "Epochs: 1645/5000----Loss: 318.8739929199219\n",
      "Epochs: 1646/5000----Loss: 318.873046875\n",
      "Epochs: 1647/5000----Loss: 318.8718566894531\n",
      "Epochs: 1648/5000----Loss: 318.87091064453125\n",
      "Epochs: 1649/5000----Loss: 318.8695983886719\n",
      "Epochs: 1650/5000----Loss: 318.86865234375\n",
      "Epochs: 1651/5000----Loss: 318.86761474609375\n",
      "Epochs: 1652/5000----Loss: 318.866455078125\n",
      "Epochs: 1653/5000----Loss: 318.8655090332031\n",
      "Epochs: 1654/5000----Loss: 318.86431884765625\n",
      "Epochs: 1655/5000----Loss: 318.8634338378906\n",
      "Epochs: 1656/5000----Loss: 318.8622131347656\n",
      "Epochs: 1657/5000----Loss: 318.86126708984375\n",
      "Epochs: 1658/5000----Loss: 318.86016845703125\n",
      "Epochs: 1659/5000----Loss: 318.85906982421875\n",
      "Epochs: 1660/5000----Loss: 318.85821533203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1661/5000----Loss: 318.85711669921875\n",
      "Epochs: 1662/5000----Loss: 318.8560485839844\n",
      "Epochs: 1663/5000----Loss: 318.8551025390625\n",
      "Epochs: 1664/5000----Loss: 318.85406494140625\n",
      "Epochs: 1665/5000----Loss: 318.8530578613281\n",
      "Epochs: 1666/5000----Loss: 318.85198974609375\n",
      "Epochs: 1667/5000----Loss: 318.8509216308594\n",
      "Epochs: 1668/5000----Loss: 318.8500061035156\n",
      "Epochs: 1669/5000----Loss: 318.84906005859375\n",
      "Epochs: 1670/5000----Loss: 318.8479919433594\n",
      "Epochs: 1671/5000----Loss: 318.846923828125\n",
      "Epochs: 1672/5000----Loss: 318.8459777832031\n",
      "Epochs: 1673/5000----Loss: 318.844970703125\n",
      "Epochs: 1674/5000----Loss: 318.8439025878906\n",
      "Epochs: 1675/5000----Loss: 318.8429260253906\n",
      "Epochs: 1676/5000----Loss: 318.8419494628906\n",
      "Epochs: 1677/5000----Loss: 318.8409423828125\n",
      "Epochs: 1678/5000----Loss: 318.8399963378906\n",
      "Epochs: 1679/5000----Loss: 318.8389892578125\n",
      "Epochs: 1680/5000----Loss: 318.83795166015625\n",
      "Epochs: 1681/5000----Loss: 318.8369140625\n",
      "Epochs: 1682/5000----Loss: 318.83599853515625\n",
      "Epochs: 1683/5000----Loss: 318.83502197265625\n",
      "Epochs: 1684/5000----Loss: 318.8341064453125\n",
      "Epochs: 1685/5000----Loss: 318.83294677734375\n",
      "Epochs: 1686/5000----Loss: 318.8321228027344\n",
      "Epochs: 1687/5000----Loss: 318.83111572265625\n",
      "Epochs: 1688/5000----Loss: 318.83013916015625\n",
      "Epochs: 1689/5000----Loss: 318.8291320800781\n",
      "Epochs: 1690/5000----Loss: 318.8280944824219\n",
      "Epochs: 1691/5000----Loss: 318.82720947265625\n",
      "Epochs: 1692/5000----Loss: 318.82623291015625\n",
      "Epochs: 1693/5000----Loss: 318.8251953125\n",
      "Epochs: 1694/5000----Loss: 318.82421875\n",
      "Epochs: 1695/5000----Loss: 318.8232727050781\n",
      "Epochs: 1696/5000----Loss: 318.82232666015625\n",
      "Epochs: 1697/5000----Loss: 318.8213806152344\n",
      "Epochs: 1698/5000----Loss: 318.8204345703125\n",
      "Epochs: 1699/5000----Loss: 318.81939697265625\n",
      "Epochs: 1700/5000----Loss: 318.8185119628906\n",
      "Epochs: 1701/5000----Loss: 318.8175964355469\n",
      "Epochs: 1702/5000----Loss: 318.8166198730469\n",
      "Epochs: 1703/5000----Loss: 318.8156433105469\n",
      "Epochs: 1704/5000----Loss: 318.81463623046875\n",
      "Epochs: 1705/5000----Loss: 318.8137512207031\n",
      "Epochs: 1706/5000----Loss: 318.81280517578125\n",
      "Epochs: 1707/5000----Loss: 318.8117980957031\n",
      "Epochs: 1708/5000----Loss: 318.8110046386719\n",
      "Epochs: 1709/5000----Loss: 318.80999755859375\n",
      "Epochs: 1710/5000----Loss: 318.8091125488281\n",
      "Epochs: 1711/5000----Loss: 318.80804443359375\n",
      "Epochs: 1712/5000----Loss: 318.8072509765625\n",
      "Epochs: 1713/5000----Loss: 318.80621337890625\n",
      "Epochs: 1714/5000----Loss: 318.80523681640625\n",
      "Epochs: 1715/5000----Loss: 318.80438232421875\n",
      "Epochs: 1716/5000----Loss: 318.80340576171875\n",
      "Epochs: 1717/5000----Loss: 318.8025817871094\n",
      "Epochs: 1718/5000----Loss: 318.8016662597656\n",
      "Epochs: 1719/5000----Loss: 318.8006896972656\n",
      "Epochs: 1720/5000----Loss: 318.7997131347656\n",
      "Epochs: 1721/5000----Loss: 318.79888916015625\n",
      "Epochs: 1722/5000----Loss: 318.7978210449219\n",
      "Epochs: 1723/5000----Loss: 318.7969970703125\n",
      "Epochs: 1724/5000----Loss: 318.7961730957031\n",
      "Epochs: 1725/5000----Loss: 318.79522705078125\n",
      "Epochs: 1726/5000----Loss: 318.79425048828125\n",
      "Epochs: 1727/5000----Loss: 318.79339599609375\n",
      "Epochs: 1728/5000----Loss: 318.79241943359375\n",
      "Epochs: 1729/5000----Loss: 318.7915954589844\n",
      "Epochs: 1730/5000----Loss: 318.79058837890625\n",
      "Epochs: 1731/5000----Loss: 318.7897644042969\n",
      "Epochs: 1732/5000----Loss: 318.7887878417969\n",
      "Epochs: 1733/5000----Loss: 318.7879638671875\n",
      "Epochs: 1734/5000----Loss: 318.78692626953125\n",
      "Epochs: 1735/5000----Loss: 318.78607177734375\n",
      "Epochs: 1736/5000----Loss: 318.78521728515625\n",
      "Epochs: 1737/5000----Loss: 318.78424072265625\n",
      "Epochs: 1738/5000----Loss: 318.7834777832031\n",
      "Epochs: 1739/5000----Loss: 318.78265380859375\n",
      "Epochs: 1740/5000----Loss: 318.78167724609375\n",
      "Epochs: 1741/5000----Loss: 318.7807312011719\n",
      "Epochs: 1742/5000----Loss: 318.77984619140625\n",
      "Epochs: 1743/5000----Loss: 318.77899169921875\n",
      "Epochs: 1744/5000----Loss: 318.77813720703125\n",
      "Epochs: 1745/5000----Loss: 318.77728271484375\n",
      "Epochs: 1746/5000----Loss: 318.7763671875\n",
      "Epochs: 1747/5000----Loss: 318.77557373046875\n",
      "Epochs: 1748/5000----Loss: 318.7746887207031\n",
      "Epochs: 1749/5000----Loss: 318.7738037109375\n",
      "Epochs: 1750/5000----Loss: 318.7728271484375\n",
      "Epochs: 1751/5000----Loss: 318.77203369140625\n",
      "Epochs: 1752/5000----Loss: 318.7710876464844\n",
      "Epochs: 1753/5000----Loss: 318.770263671875\n",
      "Epochs: 1754/5000----Loss: 318.76934814453125\n",
      "Epochs: 1755/5000----Loss: 318.76861572265625\n",
      "Epochs: 1756/5000----Loss: 318.76763916015625\n",
      "Epochs: 1757/5000----Loss: 318.76678466796875\n",
      "Epochs: 1758/5000----Loss: 318.7659606933594\n",
      "Epochs: 1759/5000----Loss: 318.7651672363281\n",
      "Epochs: 1760/5000----Loss: 318.7643127441406\n",
      "Epochs: 1761/5000----Loss: 318.76348876953125\n",
      "Epochs: 1762/5000----Loss: 318.76263427734375\n",
      "Epochs: 1763/5000----Loss: 318.7616882324219\n",
      "Epochs: 1764/5000----Loss: 318.76080322265625\n",
      "Epochs: 1765/5000----Loss: 318.7601013183594\n",
      "Epochs: 1766/5000----Loss: 318.7591552734375\n",
      "Epochs: 1767/5000----Loss: 318.7583312988281\n",
      "Epochs: 1768/5000----Loss: 318.7574768066406\n",
      "Epochs: 1769/5000----Loss: 318.7567138671875\n",
      "Epochs: 1770/5000----Loss: 318.75579833984375\n",
      "Epochs: 1771/5000----Loss: 318.7550964355469\n",
      "Epochs: 1772/5000----Loss: 318.7542724609375\n",
      "Epochs: 1773/5000----Loss: 318.7534484863281\n",
      "Epochs: 1774/5000----Loss: 318.75250244140625\n",
      "Epochs: 1775/5000----Loss: 318.7516174316406\n",
      "Epochs: 1776/5000----Loss: 318.75091552734375\n",
      "Epochs: 1777/5000----Loss: 318.74993896484375\n",
      "Epochs: 1778/5000----Loss: 318.74920654296875\n",
      "Epochs: 1779/5000----Loss: 318.7484436035156\n",
      "Epochs: 1780/5000----Loss: 318.74755859375\n",
      "Epochs: 1781/5000----Loss: 318.7467956542969\n",
      "Epochs: 1782/5000----Loss: 318.74591064453125\n",
      "Epochs: 1783/5000----Loss: 318.74517822265625\n",
      "Epochs: 1784/5000----Loss: 318.7443542480469\n",
      "Epochs: 1785/5000----Loss: 318.74346923828125\n",
      "Epochs: 1786/5000----Loss: 318.7426452636719\n",
      "Epochs: 1787/5000----Loss: 318.7418518066406\n",
      "Epochs: 1788/5000----Loss: 318.74102783203125\n",
      "Epochs: 1789/5000----Loss: 318.7402038574219\n",
      "Epochs: 1790/5000----Loss: 318.7394714355469\n",
      "Epochs: 1791/5000----Loss: 318.73870849609375\n",
      "Epochs: 1792/5000----Loss: 318.73785400390625\n",
      "Epochs: 1793/5000----Loss: 318.73699951171875\n",
      "Epochs: 1794/5000----Loss: 318.7362365722656\n",
      "Epochs: 1795/5000----Loss: 318.7354431152344\n",
      "Epochs: 1796/5000----Loss: 318.7347106933594\n",
      "Epochs: 1797/5000----Loss: 318.7338562011719\n",
      "Epochs: 1798/5000----Loss: 318.7330017089844\n",
      "Epochs: 1799/5000----Loss: 318.73236083984375\n",
      "Epochs: 1800/5000----Loss: 318.7315368652344\n",
      "Epochs: 1801/5000----Loss: 318.7307434082031\n",
      "Epochs: 1802/5000----Loss: 318.7298278808594\n",
      "Epochs: 1803/5000----Loss: 318.7291564941406\n",
      "Epochs: 1804/5000----Loss: 318.72833251953125\n",
      "Epochs: 1805/5000----Loss: 318.7275085449219\n",
      "Epochs: 1806/5000----Loss: 318.72674560546875\n",
      "Epochs: 1807/5000----Loss: 318.7259826660156\n",
      "Epochs: 1808/5000----Loss: 318.7250671386719\n",
      "Epochs: 1809/5000----Loss: 318.72442626953125\n",
      "Epochs: 1810/5000----Loss: 318.72357177734375\n",
      "Epochs: 1811/5000----Loss: 318.7227478027344\n",
      "Epochs: 1812/5000----Loss: 318.721923828125\n",
      "Epochs: 1813/5000----Loss: 318.7212829589844\n",
      "Epochs: 1814/5000----Loss: 318.72052001953125\n",
      "Epochs: 1815/5000----Loss: 318.7196960449219\n",
      "Epochs: 1816/5000----Loss: 318.7188415527344\n",
      "Epochs: 1817/5000----Loss: 318.7181701660156\n",
      "Epochs: 1818/5000----Loss: 318.7174072265625\n",
      "Epochs: 1819/5000----Loss: 318.71673583984375\n",
      "Epochs: 1820/5000----Loss: 318.71588134765625\n",
      "Epochs: 1821/5000----Loss: 318.715087890625\n",
      "Epochs: 1822/5000----Loss: 318.7143249511719\n",
      "Epochs: 1823/5000----Loss: 318.7136535644531\n",
      "Epochs: 1824/5000----Loss: 318.712890625\n",
      "Epochs: 1825/5000----Loss: 318.71197509765625\n",
      "Epochs: 1826/5000----Loss: 318.71136474609375\n",
      "Epochs: 1827/5000----Loss: 318.71063232421875\n",
      "Epochs: 1828/5000----Loss: 318.70977783203125\n",
      "Epochs: 1829/5000----Loss: 318.7091369628906\n",
      "Epochs: 1830/5000----Loss: 318.70843505859375\n",
      "Epochs: 1831/5000----Loss: 318.70758056640625\n",
      "Epochs: 1832/5000----Loss: 318.7068176269531\n",
      "Epochs: 1833/5000----Loss: 318.7061462402344\n",
      "Epochs: 1834/5000----Loss: 318.7053527832031\n",
      "Epochs: 1835/5000----Loss: 318.7047119140625\n",
      "Epochs: 1836/5000----Loss: 318.70391845703125\n",
      "Epochs: 1837/5000----Loss: 318.703125\n",
      "Epochs: 1838/5000----Loss: 318.70245361328125\n",
      "Epochs: 1839/5000----Loss: 318.7016906738281\n",
      "Epochs: 1840/5000----Loss: 318.70086669921875\n",
      "Epochs: 1841/5000----Loss: 318.7001953125\n",
      "Epochs: 1842/5000----Loss: 318.69952392578125\n",
      "Epochs: 1843/5000----Loss: 318.6987609863281\n",
      "Epochs: 1844/5000----Loss: 318.69805908203125\n",
      "Epochs: 1845/5000----Loss: 318.69732666015625\n",
      "Epochs: 1846/5000----Loss: 318.696533203125\n",
      "Epochs: 1847/5000----Loss: 318.69586181640625\n",
      "Epochs: 1848/5000----Loss: 318.695068359375\n",
      "Epochs: 1849/5000----Loss: 318.69439697265625\n",
      "Epochs: 1850/5000----Loss: 318.69366455078125\n",
      "Epochs: 1851/5000----Loss: 318.6930236816406\n",
      "Epochs: 1852/5000----Loss: 318.6922607421875\n",
      "Epochs: 1853/5000----Loss: 318.6914978027344\n",
      "Epochs: 1854/5000----Loss: 318.69073486328125\n",
      "Epochs: 1855/5000----Loss: 318.6902160644531\n",
      "Epochs: 1856/5000----Loss: 318.6894226074219\n",
      "Epochs: 1857/5000----Loss: 318.68878173828125\n",
      "Epochs: 1858/5000----Loss: 318.68804931640625\n",
      "Epochs: 1859/5000----Loss: 318.6873474121094\n",
      "Epochs: 1860/5000----Loss: 318.68658447265625\n",
      "Epochs: 1861/5000----Loss: 318.68585205078125\n",
      "Epochs: 1862/5000----Loss: 318.68524169921875\n",
      "Epochs: 1863/5000----Loss: 318.68450927734375\n",
      "Epochs: 1864/5000----Loss: 318.68377685546875\n",
      "Epochs: 1865/5000----Loss: 318.6829833984375\n",
      "Epochs: 1866/5000----Loss: 318.68231201171875\n",
      "Epochs: 1867/5000----Loss: 318.68182373046875\n",
      "Epochs: 1868/5000----Loss: 318.6809997558594\n",
      "Epochs: 1869/5000----Loss: 318.6803283691406\n",
      "Epochs: 1870/5000----Loss: 318.67962646484375\n",
      "Epochs: 1871/5000----Loss: 318.6789245605469\n",
      "Epochs: 1872/5000----Loss: 318.6782531738281\n",
      "Epochs: 1873/5000----Loss: 318.67755126953125\n",
      "Epochs: 1874/5000----Loss: 318.67681884765625\n",
      "Epochs: 1875/5000----Loss: 318.6761779785156\n",
      "Epochs: 1876/5000----Loss: 318.6754455566406\n",
      "Epochs: 1877/5000----Loss: 318.6748962402344\n",
      "Epochs: 1878/5000----Loss: 318.6742248535156\n",
      "Epochs: 1879/5000----Loss: 318.6734619140625\n",
      "Epochs: 1880/5000----Loss: 318.6727600097656\n",
      "Epochs: 1881/5000----Loss: 318.6719970703125\n",
      "Epochs: 1882/5000----Loss: 318.6714172363281\n",
      "Epochs: 1883/5000----Loss: 318.670654296875\n",
      "Epochs: 1884/5000----Loss: 318.66998291015625\n",
      "Epochs: 1885/5000----Loss: 318.6692199707031\n",
      "Epochs: 1886/5000----Loss: 318.6685791015625\n",
      "Epochs: 1887/5000----Loss: 318.6678466796875\n",
      "Epochs: 1888/5000----Loss: 318.6672058105469\n",
      "Epochs: 1889/5000----Loss: 318.6666259765625\n",
      "Epochs: 1890/5000----Loss: 318.6659240722656\n",
      "Epochs: 1891/5000----Loss: 318.665283203125\n",
      "Epochs: 1892/5000----Loss: 318.6647033691406\n",
      "Epochs: 1893/5000----Loss: 318.66400146484375\n",
      "Epochs: 1894/5000----Loss: 318.66339111328125\n",
      "Epochs: 1895/5000----Loss: 318.66265869140625\n",
      "Epochs: 1896/5000----Loss: 318.662109375\n",
      "Epochs: 1897/5000----Loss: 318.6612548828125\n",
      "Epochs: 1898/5000----Loss: 318.6606750488281\n",
      "Epochs: 1899/5000----Loss: 318.6599426269531\n",
      "Epochs: 1900/5000----Loss: 318.65936279296875\n",
      "Epochs: 1901/5000----Loss: 318.6586608886719\n",
      "Epochs: 1902/5000----Loss: 318.6579895019531\n",
      "Epochs: 1903/5000----Loss: 318.65740966796875\n",
      "Epochs: 1904/5000----Loss: 318.65679931640625\n",
      "Epochs: 1905/5000----Loss: 318.6561584472656\n",
      "Epochs: 1906/5000----Loss: 318.65545654296875\n",
      "Epochs: 1907/5000----Loss: 318.65478515625\n",
      "Epochs: 1908/5000----Loss: 318.65411376953125\n",
      "Epochs: 1909/5000----Loss: 318.65362548828125\n",
      "Epochs: 1910/5000----Loss: 318.65289306640625\n",
      "Epochs: 1911/5000----Loss: 318.65228271484375\n",
      "Epochs: 1912/5000----Loss: 318.65167236328125\n",
      "Epochs: 1913/5000----Loss: 318.65093994140625\n",
      "Epochs: 1914/5000----Loss: 318.6502685546875\n",
      "Epochs: 1915/5000----Loss: 318.6496887207031\n",
      "Epochs: 1916/5000----Loss: 318.6490783691406\n",
      "Epochs: 1917/5000----Loss: 318.64837646484375\n",
      "Epochs: 1918/5000----Loss: 318.64776611328125\n",
      "Epochs: 1919/5000----Loss: 318.64715576171875\n",
      "Epochs: 1920/5000----Loss: 318.6466064453125\n",
      "Epochs: 1921/5000----Loss: 318.6458740234375\n",
      "Epochs: 1922/5000----Loss: 318.6451721191406\n",
      "Epochs: 1923/5000----Loss: 318.6446533203125\n",
      "Epochs: 1924/5000----Loss: 318.6440124511719\n",
      "Epochs: 1925/5000----Loss: 318.64337158203125\n",
      "Epochs: 1926/5000----Loss: 318.64276123046875\n",
      "Epochs: 1927/5000----Loss: 318.6421203613281\n",
      "Epochs: 1928/5000----Loss: 318.6415100097656\n",
      "Epochs: 1929/5000----Loss: 318.6408386230469\n",
      "Epochs: 1930/5000----Loss: 318.6402893066406\n",
      "Epochs: 1931/5000----Loss: 318.63958740234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1932/5000----Loss: 318.6390686035156\n",
      "Epochs: 1933/5000----Loss: 318.6384582519531\n",
      "Epochs: 1934/5000----Loss: 318.63787841796875\n",
      "Epochs: 1935/5000----Loss: 318.6371765136719\n",
      "Epochs: 1936/5000----Loss: 318.6366271972656\n",
      "Epochs: 1937/5000----Loss: 318.6360778808594\n",
      "Epochs: 1938/5000----Loss: 318.63531494140625\n",
      "Epochs: 1939/5000----Loss: 318.6348571777344\n",
      "Epochs: 1940/5000----Loss: 318.63421630859375\n",
      "Epochs: 1941/5000----Loss: 318.63360595703125\n",
      "Epochs: 1942/5000----Loss: 318.6329650878906\n",
      "Epochs: 1943/5000----Loss: 318.63238525390625\n",
      "Epochs: 1944/5000----Loss: 318.63177490234375\n",
      "Epochs: 1945/5000----Loss: 318.6311340332031\n",
      "Epochs: 1946/5000----Loss: 318.630615234375\n",
      "Epochs: 1947/5000----Loss: 318.63006591796875\n",
      "Epochs: 1948/5000----Loss: 318.62933349609375\n",
      "Epochs: 1949/5000----Loss: 318.6287841796875\n",
      "Epochs: 1950/5000----Loss: 318.6281433105469\n",
      "Epochs: 1951/5000----Loss: 318.6275939941406\n",
      "Epochs: 1952/5000----Loss: 318.6270446777344\n",
      "Epochs: 1953/5000----Loss: 318.62640380859375\n",
      "Epochs: 1954/5000----Loss: 318.62567138671875\n",
      "Epochs: 1955/5000----Loss: 318.6250915527344\n",
      "Epochs: 1956/5000----Loss: 318.62445068359375\n",
      "Epochs: 1957/5000----Loss: 318.6239013671875\n",
      "Epochs: 1958/5000----Loss: 318.62335205078125\n",
      "Epochs: 1959/5000----Loss: 318.6228332519531\n",
      "Epochs: 1960/5000----Loss: 318.6222229003906\n",
      "Epochs: 1961/5000----Loss: 318.6215515136719\n",
      "Epochs: 1962/5000----Loss: 318.62103271484375\n",
      "Epochs: 1963/5000----Loss: 318.620361328125\n",
      "Epochs: 1964/5000----Loss: 318.6198425292969\n",
      "Epochs: 1965/5000----Loss: 318.61932373046875\n",
      "Epochs: 1966/5000----Loss: 318.6186828613281\n",
      "Epochs: 1967/5000----Loss: 318.6181945800781\n",
      "Epochs: 1968/5000----Loss: 318.6175231933594\n",
      "Epochs: 1969/5000----Loss: 318.61700439453125\n",
      "Epochs: 1970/5000----Loss: 318.6163635253906\n",
      "Epochs: 1971/5000----Loss: 318.61578369140625\n",
      "Epochs: 1972/5000----Loss: 318.61529541015625\n",
      "Epochs: 1973/5000----Loss: 318.61468505859375\n",
      "Epochs: 1974/5000----Loss: 318.61419677734375\n",
      "Epochs: 1975/5000----Loss: 318.61358642578125\n",
      "Epochs: 1976/5000----Loss: 318.6130065917969\n",
      "Epochs: 1977/5000----Loss: 318.61248779296875\n",
      "Epochs: 1978/5000----Loss: 318.6119384765625\n",
      "Epochs: 1979/5000----Loss: 318.6111755371094\n",
      "Epochs: 1980/5000----Loss: 318.6106872558594\n",
      "Epochs: 1981/5000----Loss: 318.61016845703125\n",
      "Epochs: 1982/5000----Loss: 318.609619140625\n",
      "Epochs: 1983/5000----Loss: 318.6091003417969\n",
      "Epochs: 1984/5000----Loss: 318.6084899902344\n",
      "Epochs: 1985/5000----Loss: 318.6080017089844\n",
      "Epochs: 1986/5000----Loss: 318.60736083984375\n",
      "Epochs: 1987/5000----Loss: 318.6069030761719\n",
      "Epochs: 1988/5000----Loss: 318.606201171875\n",
      "Epochs: 1989/5000----Loss: 318.6056823730469\n",
      "Epochs: 1990/5000----Loss: 318.60516357421875\n",
      "Epochs: 1991/5000----Loss: 318.6046447753906\n",
      "Epochs: 1992/5000----Loss: 318.60406494140625\n",
      "Epochs: 1993/5000----Loss: 318.60357666015625\n",
      "Epochs: 1994/5000----Loss: 318.60296630859375\n",
      "Epochs: 1995/5000----Loss: 318.6024475097656\n",
      "Epochs: 1996/5000----Loss: 318.60186767578125\n",
      "Epochs: 1997/5000----Loss: 318.60137939453125\n",
      "Epochs: 1998/5000----Loss: 318.6007995605469\n",
      "Epochs: 1999/5000----Loss: 318.6002197265625\n",
      "Epochs: 2000/5000----Loss: 318.5996398925781\n",
      "Epochs: 2001/5000----Loss: 318.59906005859375\n",
      "Epochs: 2002/5000----Loss: 318.5986328125\n",
      "Epochs: 2003/5000----Loss: 318.59808349609375\n",
      "Epochs: 2004/5000----Loss: 318.5975036621094\n",
      "Epochs: 2005/5000----Loss: 318.5970153808594\n",
      "Epochs: 2006/5000----Loss: 318.596435546875\n",
      "Epochs: 2007/5000----Loss: 318.595947265625\n",
      "Epochs: 2008/5000----Loss: 318.5953674316406\n",
      "Epochs: 2009/5000----Loss: 318.594970703125\n",
      "Epochs: 2010/5000----Loss: 318.5943603515625\n",
      "Epochs: 2011/5000----Loss: 318.5937805175781\n",
      "Epochs: 2012/5000----Loss: 318.5932922363281\n",
      "Epochs: 2013/5000----Loss: 318.59271240234375\n",
      "Epochs: 2014/5000----Loss: 318.5922546386719\n",
      "Epochs: 2015/5000----Loss: 318.5917663574219\n",
      "Epochs: 2016/5000----Loss: 318.5912170410156\n",
      "Epochs: 2017/5000----Loss: 318.5906066894531\n",
      "Epochs: 2018/5000----Loss: 318.5901184082031\n",
      "Epochs: 2019/5000----Loss: 318.5895690917969\n",
      "Epochs: 2020/5000----Loss: 318.58905029296875\n",
      "Epochs: 2021/5000----Loss: 318.5884094238281\n",
      "Epochs: 2022/5000----Loss: 318.58795166015625\n",
      "Epochs: 2023/5000----Loss: 318.58740234375\n",
      "Epochs: 2024/5000----Loss: 318.58685302734375\n",
      "Epochs: 2025/5000----Loss: 318.5863342285156\n",
      "Epochs: 2026/5000----Loss: 318.5859680175781\n",
      "Epochs: 2027/5000----Loss: 318.5853576660156\n",
      "Epochs: 2028/5000----Loss: 318.58489990234375\n",
      "Epochs: 2029/5000----Loss: 318.58428955078125\n",
      "Epochs: 2030/5000----Loss: 318.5837707519531\n",
      "Epochs: 2031/5000----Loss: 318.5834045410156\n",
      "Epochs: 2032/5000----Loss: 318.58282470703125\n",
      "Epochs: 2033/5000----Loss: 318.5822448730469\n",
      "Epochs: 2034/5000----Loss: 318.58172607421875\n",
      "Epochs: 2035/5000----Loss: 318.58123779296875\n",
      "Epochs: 2036/5000----Loss: 318.58074951171875\n",
      "Epochs: 2037/5000----Loss: 318.58026123046875\n",
      "Epochs: 2038/5000----Loss: 318.57977294921875\n",
      "Epochs: 2039/5000----Loss: 318.57928466796875\n",
      "Epochs: 2040/5000----Loss: 318.5787658691406\n",
      "Epochs: 2041/5000----Loss: 318.5782165527344\n",
      "Epochs: 2042/5000----Loss: 318.57769775390625\n",
      "Epochs: 2043/5000----Loss: 318.5772399902344\n",
      "Epochs: 2044/5000----Loss: 318.57666015625\n",
      "Epochs: 2045/5000----Loss: 318.5762939453125\n",
      "Epochs: 2046/5000----Loss: 318.5758361816406\n",
      "Epochs: 2047/5000----Loss: 318.5752868652344\n",
      "Epochs: 2048/5000----Loss: 318.57476806640625\n",
      "Epochs: 2049/5000----Loss: 318.5743103027344\n",
      "Epochs: 2050/5000----Loss: 318.5738220214844\n",
      "Epochs: 2051/5000----Loss: 318.5733337402344\n",
      "Epochs: 2052/5000----Loss: 318.5727844238281\n",
      "Epochs: 2053/5000----Loss: 318.5722351074219\n",
      "Epochs: 2054/5000----Loss: 318.5717468261719\n",
      "Epochs: 2055/5000----Loss: 318.57135009765625\n",
      "Epochs: 2056/5000----Loss: 318.5709228515625\n",
      "Epochs: 2057/5000----Loss: 318.5704040527344\n",
      "Epochs: 2058/5000----Loss: 318.56988525390625\n",
      "Epochs: 2059/5000----Loss: 318.5693359375\n",
      "Epochs: 2060/5000----Loss: 318.5688781738281\n",
      "Epochs: 2061/5000----Loss: 318.568359375\n",
      "Epochs: 2062/5000----Loss: 318.5680236816406\n",
      "Epochs: 2063/5000----Loss: 318.56744384765625\n",
      "Epochs: 2064/5000----Loss: 318.56695556640625\n",
      "Epochs: 2065/5000----Loss: 318.56646728515625\n",
      "Epochs: 2066/5000----Loss: 318.56597900390625\n",
      "Epochs: 2067/5000----Loss: 318.56549072265625\n",
      "Epochs: 2068/5000----Loss: 318.5650329589844\n",
      "Epochs: 2069/5000----Loss: 318.56451416015625\n",
      "Epochs: 2070/5000----Loss: 318.5641174316406\n",
      "Epochs: 2071/5000----Loss: 318.56365966796875\n",
      "Epochs: 2072/5000----Loss: 318.5631408691406\n",
      "Epochs: 2073/5000----Loss: 318.56268310546875\n",
      "Epochs: 2074/5000----Loss: 318.5621643066406\n",
      "Epochs: 2075/5000----Loss: 318.561767578125\n",
      "Epochs: 2076/5000----Loss: 318.561279296875\n",
      "Epochs: 2077/5000----Loss: 318.56085205078125\n",
      "Epochs: 2078/5000----Loss: 318.5602722167969\n",
      "Epochs: 2079/5000----Loss: 318.5599060058594\n",
      "Epochs: 2080/5000----Loss: 318.5594482421875\n",
      "Epochs: 2081/5000----Loss: 318.5589294433594\n",
      "Epochs: 2082/5000----Loss: 318.5584411621094\n",
      "Epochs: 2083/5000----Loss: 318.5579528808594\n",
      "Epochs: 2084/5000----Loss: 318.5576171875\n",
      "Epochs: 2085/5000----Loss: 318.55694580078125\n",
      "Epochs: 2086/5000----Loss: 318.5564880371094\n",
      "Epochs: 2087/5000----Loss: 318.5560302734375\n",
      "Epochs: 2088/5000----Loss: 318.5556335449219\n",
      "Epochs: 2089/5000----Loss: 318.55517578125\n",
      "Epochs: 2090/5000----Loss: 318.55462646484375\n",
      "Epochs: 2091/5000----Loss: 318.5541687011719\n",
      "Epochs: 2092/5000----Loss: 318.5538330078125\n",
      "Epochs: 2093/5000----Loss: 318.55328369140625\n",
      "Epochs: 2094/5000----Loss: 318.5528564453125\n",
      "Epochs: 2095/5000----Loss: 318.5523681640625\n",
      "Epochs: 2096/5000----Loss: 318.55194091796875\n",
      "Epochs: 2097/5000----Loss: 318.55145263671875\n",
      "Epochs: 2098/5000----Loss: 318.5511169433594\n",
      "Epochs: 2099/5000----Loss: 318.55059814453125\n",
      "Epochs: 2100/5000----Loss: 318.5501708984375\n",
      "Epochs: 2101/5000----Loss: 318.54974365234375\n",
      "Epochs: 2102/5000----Loss: 318.5491943359375\n",
      "Epochs: 2103/5000----Loss: 318.54888916015625\n",
      "Epochs: 2104/5000----Loss: 318.5483703613281\n",
      "Epochs: 2105/5000----Loss: 318.5478820800781\n",
      "Epochs: 2106/5000----Loss: 318.54742431640625\n",
      "Epochs: 2107/5000----Loss: 318.54705810546875\n",
      "Epochs: 2108/5000----Loss: 318.5466003417969\n",
      "Epochs: 2109/5000----Loss: 318.54608154296875\n",
      "Epochs: 2110/5000----Loss: 318.5456237792969\n",
      "Epochs: 2111/5000----Loss: 318.54522705078125\n",
      "Epochs: 2112/5000----Loss: 318.5448303222656\n",
      "Epochs: 2113/5000----Loss: 318.5443420410156\n",
      "Epochs: 2114/5000----Loss: 318.5439453125\n",
      "Epochs: 2115/5000----Loss: 318.5435791015625\n",
      "Epochs: 2116/5000----Loss: 318.54302978515625\n",
      "Epochs: 2117/5000----Loss: 318.54254150390625\n",
      "Epochs: 2118/5000----Loss: 318.5421142578125\n",
      "Epochs: 2119/5000----Loss: 318.54180908203125\n",
      "Epochs: 2120/5000----Loss: 318.54144287109375\n",
      "Epochs: 2121/5000----Loss: 318.5408630371094\n",
      "Epochs: 2122/5000----Loss: 318.5404968261719\n",
      "Epochs: 2123/5000----Loss: 318.5401306152344\n",
      "Epochs: 2124/5000----Loss: 318.53955078125\n",
      "Epochs: 2125/5000----Loss: 318.53912353515625\n",
      "Epochs: 2126/5000----Loss: 318.53875732421875\n",
      "Epochs: 2127/5000----Loss: 318.53826904296875\n",
      "Epochs: 2128/5000----Loss: 318.5378723144531\n",
      "Epochs: 2129/5000----Loss: 318.5375061035156\n",
      "Epochs: 2130/5000----Loss: 318.5369567871094\n",
      "Epochs: 2131/5000----Loss: 318.5366516113281\n",
      "Epochs: 2132/5000----Loss: 318.53619384765625\n",
      "Epochs: 2133/5000----Loss: 318.53582763671875\n",
      "Epochs: 2134/5000----Loss: 318.5353088378906\n",
      "Epochs: 2135/5000----Loss: 318.53497314453125\n",
      "Epochs: 2136/5000----Loss: 318.5346374511719\n",
      "Epochs: 2137/5000----Loss: 318.5340881347656\n",
      "Epochs: 2138/5000----Loss: 318.5337219238281\n",
      "Epochs: 2139/5000----Loss: 318.53314208984375\n",
      "Epochs: 2140/5000----Loss: 318.5328063964844\n",
      "Epochs: 2141/5000----Loss: 318.53240966796875\n",
      "Epochs: 2142/5000----Loss: 318.5320129394531\n",
      "Epochs: 2143/5000----Loss: 318.5314636230469\n",
      "Epochs: 2144/5000----Loss: 318.5311584472656\n",
      "Epochs: 2145/5000----Loss: 318.53070068359375\n",
      "Epochs: 2146/5000----Loss: 318.53033447265625\n",
      "Epochs: 2147/5000----Loss: 318.52996826171875\n",
      "Epochs: 2148/5000----Loss: 318.5295104980469\n",
      "Epochs: 2149/5000----Loss: 318.52899169921875\n",
      "Epochs: 2150/5000----Loss: 318.5286560058594\n",
      "Epochs: 2151/5000----Loss: 318.52813720703125\n",
      "Epochs: 2152/5000----Loss: 318.5278015136719\n",
      "Epochs: 2153/5000----Loss: 318.52728271484375\n",
      "Epochs: 2154/5000----Loss: 318.5270080566406\n",
      "Epochs: 2155/5000----Loss: 318.52655029296875\n",
      "Epochs: 2156/5000----Loss: 318.52618408203125\n",
      "Epochs: 2157/5000----Loss: 318.5257568359375\n",
      "Epochs: 2158/5000----Loss: 318.525390625\n",
      "Epochs: 2159/5000----Loss: 318.52490234375\n",
      "Epochs: 2160/5000----Loss: 318.5245361328125\n",
      "Epochs: 2161/5000----Loss: 318.52410888671875\n",
      "Epochs: 2162/5000----Loss: 318.52374267578125\n",
      "Epochs: 2163/5000----Loss: 318.5233459472656\n",
      "Epochs: 2164/5000----Loss: 318.5229187011719\n",
      "Epochs: 2165/5000----Loss: 318.5226135253906\n",
      "Epochs: 2166/5000----Loss: 318.52215576171875\n",
      "Epochs: 2167/5000----Loss: 318.52178955078125\n",
      "Epochs: 2168/5000----Loss: 318.52130126953125\n",
      "Epochs: 2169/5000----Loss: 318.5209655761719\n",
      "Epochs: 2170/5000----Loss: 318.52056884765625\n",
      "Epochs: 2171/5000----Loss: 318.5202331542969\n",
      "Epochs: 2172/5000----Loss: 318.5197448730469\n",
      "Epochs: 2173/5000----Loss: 318.5194091796875\n",
      "Epochs: 2174/5000----Loss: 318.5190124511719\n",
      "Epochs: 2175/5000----Loss: 318.5185546875\n",
      "Epochs: 2176/5000----Loss: 318.5182189941406\n",
      "Epochs: 2177/5000----Loss: 318.51776123046875\n",
      "Epochs: 2178/5000----Loss: 318.51739501953125\n",
      "Epochs: 2179/5000----Loss: 318.51702880859375\n",
      "Epochs: 2180/5000----Loss: 318.51666259765625\n",
      "Epochs: 2181/5000----Loss: 318.5162658691406\n",
      "Epochs: 2182/5000----Loss: 318.515869140625\n",
      "Epochs: 2183/5000----Loss: 318.515380859375\n",
      "Epochs: 2184/5000----Loss: 318.5151062011719\n",
      "Epochs: 2185/5000----Loss: 318.5146789550781\n",
      "Epochs: 2186/5000----Loss: 318.5143127441406\n",
      "Epochs: 2187/5000----Loss: 318.51397705078125\n",
      "Epochs: 2188/5000----Loss: 318.5135803222656\n",
      "Epochs: 2189/5000----Loss: 318.51324462890625\n",
      "Epochs: 2190/5000----Loss: 318.5128479003906\n",
      "Epochs: 2191/5000----Loss: 318.5124816894531\n",
      "Epochs: 2192/5000----Loss: 318.51202392578125\n",
      "Epochs: 2193/5000----Loss: 318.51165771484375\n",
      "Epochs: 2194/5000----Loss: 318.51129150390625\n",
      "Epochs: 2195/5000----Loss: 318.5108947753906\n",
      "Epochs: 2196/5000----Loss: 318.51055908203125\n",
      "Epochs: 2197/5000----Loss: 318.5101013183594\n",
      "Epochs: 2198/5000----Loss: 318.5098876953125\n",
      "Epochs: 2199/5000----Loss: 318.5093994140625\n",
      "Epochs: 2200/5000----Loss: 318.50909423828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2201/5000----Loss: 318.5086975097656\n",
      "Epochs: 2202/5000----Loss: 318.5082092285156\n",
      "Epochs: 2203/5000----Loss: 318.5079345703125\n",
      "Epochs: 2204/5000----Loss: 318.507568359375\n",
      "Epochs: 2205/5000----Loss: 318.5071716308594\n",
      "Epochs: 2206/5000----Loss: 318.5068359375\n",
      "Epochs: 2207/5000----Loss: 318.50653076171875\n",
      "Epochs: 2208/5000----Loss: 318.50604248046875\n",
      "Epochs: 2209/5000----Loss: 318.5057067871094\n",
      "Epochs: 2210/5000----Loss: 318.50531005859375\n",
      "Epochs: 2211/5000----Loss: 318.5050354003906\n",
      "Epochs: 2212/5000----Loss: 318.50457763671875\n",
      "Epochs: 2213/5000----Loss: 318.5041809082031\n",
      "Epochs: 2214/5000----Loss: 318.50384521484375\n",
      "Epochs: 2215/5000----Loss: 318.50341796875\n",
      "Epochs: 2216/5000----Loss: 318.5030822753906\n",
      "Epochs: 2217/5000----Loss: 318.502685546875\n",
      "Epochs: 2218/5000----Loss: 318.5022888183594\n",
      "Epochs: 2219/5000----Loss: 318.501953125\n",
      "Epochs: 2220/5000----Loss: 318.5016784667969\n",
      "Epochs: 2221/5000----Loss: 318.50115966796875\n",
      "Epochs: 2222/5000----Loss: 318.5008850097656\n",
      "Epochs: 2223/5000----Loss: 318.50054931640625\n",
      "Epochs: 2224/5000----Loss: 318.50018310546875\n",
      "Epochs: 2225/5000----Loss: 318.4998474121094\n",
      "Epochs: 2226/5000----Loss: 318.49945068359375\n",
      "Epochs: 2227/5000----Loss: 318.4991455078125\n",
      "Epochs: 2228/5000----Loss: 318.498779296875\n",
      "Epochs: 2229/5000----Loss: 318.4984436035156\n",
      "Epochs: 2230/5000----Loss: 318.4981384277344\n",
      "Epochs: 2231/5000----Loss: 318.49774169921875\n",
      "Epochs: 2232/5000----Loss: 318.49737548828125\n",
      "Epochs: 2233/5000----Loss: 318.49700927734375\n",
      "Epochs: 2234/5000----Loss: 318.49676513671875\n",
      "Epochs: 2235/5000----Loss: 318.49639892578125\n",
      "Epochs: 2236/5000----Loss: 318.49591064453125\n",
      "Epochs: 2237/5000----Loss: 318.49554443359375\n",
      "Epochs: 2238/5000----Loss: 318.49530029296875\n",
      "Epochs: 2239/5000----Loss: 318.4948425292969\n",
      "Epochs: 2240/5000----Loss: 318.4945373535156\n",
      "Epochs: 2241/5000----Loss: 318.4942321777344\n",
      "Epochs: 2242/5000----Loss: 318.49383544921875\n",
      "Epochs: 2243/5000----Loss: 318.4935302734375\n",
      "Epochs: 2244/5000----Loss: 318.4932556152344\n",
      "Epochs: 2245/5000----Loss: 318.4928283691406\n",
      "Epochs: 2246/5000----Loss: 318.492431640625\n",
      "Epochs: 2247/5000----Loss: 318.4920959472656\n",
      "Epochs: 2248/5000----Loss: 318.4917907714844\n",
      "Epochs: 2249/5000----Loss: 318.49139404296875\n",
      "Epochs: 2250/5000----Loss: 318.4910888671875\n",
      "Epochs: 2251/5000----Loss: 318.49078369140625\n",
      "Epochs: 2252/5000----Loss: 318.49041748046875\n",
      "Epochs: 2253/5000----Loss: 318.4900817871094\n",
      "Epochs: 2254/5000----Loss: 318.48974609375\n",
      "Epochs: 2255/5000----Loss: 318.4894714355469\n",
      "Epochs: 2256/5000----Loss: 318.4891052246094\n",
      "Epochs: 2257/5000----Loss: 318.4886779785156\n",
      "Epochs: 2258/5000----Loss: 318.48828125\n",
      "Epochs: 2259/5000----Loss: 318.488037109375\n",
      "Epochs: 2260/5000----Loss: 318.48779296875\n",
      "Epochs: 2261/5000----Loss: 318.48736572265625\n",
      "Epochs: 2262/5000----Loss: 318.48699951171875\n",
      "Epochs: 2263/5000----Loss: 318.48675537109375\n",
      "Epochs: 2264/5000----Loss: 318.48638916015625\n",
      "Epochs: 2265/5000----Loss: 318.486083984375\n",
      "Epochs: 2266/5000----Loss: 318.4857177734375\n",
      "Epochs: 2267/5000----Loss: 318.48541259765625\n",
      "Epochs: 2268/5000----Loss: 318.485107421875\n",
      "Epochs: 2269/5000----Loss: 318.48480224609375\n",
      "Epochs: 2270/5000----Loss: 318.4844055175781\n",
      "Epochs: 2271/5000----Loss: 318.48419189453125\n",
      "Epochs: 2272/5000----Loss: 318.4836730957031\n",
      "Epochs: 2273/5000----Loss: 318.48345947265625\n",
      "Epochs: 2274/5000----Loss: 318.4830627441406\n",
      "Epochs: 2275/5000----Loss: 318.482666015625\n",
      "Epochs: 2276/5000----Loss: 318.48248291015625\n",
      "Epochs: 2277/5000----Loss: 318.4820861816406\n",
      "Epochs: 2278/5000----Loss: 318.481689453125\n",
      "Epochs: 2279/5000----Loss: 318.48138427734375\n",
      "Epochs: 2280/5000----Loss: 318.48114013671875\n",
      "Epochs: 2281/5000----Loss: 318.4807434082031\n",
      "Epochs: 2282/5000----Loss: 318.48046875\n",
      "Epochs: 2283/5000----Loss: 318.48004150390625\n",
      "Epochs: 2284/5000----Loss: 318.4797668457031\n",
      "Epochs: 2285/5000----Loss: 318.47943115234375\n",
      "Epochs: 2286/5000----Loss: 318.47918701171875\n",
      "Epochs: 2287/5000----Loss: 318.4787902832031\n",
      "Epochs: 2288/5000----Loss: 318.478515625\n",
      "Epochs: 2289/5000----Loss: 318.478271484375\n",
      "Epochs: 2290/5000----Loss: 318.4779052734375\n",
      "Epochs: 2291/5000----Loss: 318.4775695800781\n",
      "Epochs: 2292/5000----Loss: 318.4772033691406\n",
      "Epochs: 2293/5000----Loss: 318.47686767578125\n",
      "Epochs: 2294/5000----Loss: 318.4766540527344\n",
      "Epochs: 2295/5000----Loss: 318.4762268066406\n",
      "Epochs: 2296/5000----Loss: 318.4759521484375\n",
      "Epochs: 2297/5000----Loss: 318.47564697265625\n",
      "Epochs: 2298/5000----Loss: 318.4753112792969\n",
      "Epochs: 2299/5000----Loss: 318.47503662109375\n",
      "Epochs: 2300/5000----Loss: 318.47479248046875\n",
      "Epochs: 2301/5000----Loss: 318.47442626953125\n",
      "Epochs: 2302/5000----Loss: 318.47406005859375\n",
      "Epochs: 2303/5000----Loss: 318.4737548828125\n",
      "Epochs: 2304/5000----Loss: 318.4735107421875\n",
      "Epochs: 2305/5000----Loss: 318.47314453125\n",
      "Epochs: 2306/5000----Loss: 318.47296142578125\n",
      "Epochs: 2307/5000----Loss: 318.47259521484375\n",
      "Epochs: 2308/5000----Loss: 318.4722900390625\n",
      "Epochs: 2309/5000----Loss: 318.4720458984375\n",
      "Epochs: 2310/5000----Loss: 318.4716491699219\n",
      "Epochs: 2311/5000----Loss: 318.4713134765625\n",
      "Epochs: 2312/5000----Loss: 318.4710998535156\n",
      "Epochs: 2313/5000----Loss: 318.47076416015625\n",
      "Epochs: 2314/5000----Loss: 318.4704284667969\n",
      "Epochs: 2315/5000----Loss: 318.4700622558594\n",
      "Epochs: 2316/5000----Loss: 318.4697570800781\n",
      "Epochs: 2317/5000----Loss: 318.4695129394531\n",
      "Epochs: 2318/5000----Loss: 318.46923828125\n",
      "Epochs: 2319/5000----Loss: 318.4689025878906\n",
      "Epochs: 2320/5000----Loss: 318.4686584472656\n",
      "Epochs: 2321/5000----Loss: 318.46832275390625\n",
      "Epochs: 2322/5000----Loss: 318.46807861328125\n",
      "Epochs: 2323/5000----Loss: 318.4676818847656\n",
      "Epochs: 2324/5000----Loss: 318.4673767089844\n",
      "Epochs: 2325/5000----Loss: 318.46710205078125\n",
      "Epochs: 2326/5000----Loss: 318.46685791015625\n",
      "Epochs: 2327/5000----Loss: 318.4664611816406\n",
      "Epochs: 2328/5000----Loss: 318.4662780761719\n",
      "Epochs: 2329/5000----Loss: 318.4659729003906\n",
      "Epochs: 2330/5000----Loss: 318.4656982421875\n",
      "Epochs: 2331/5000----Loss: 318.46527099609375\n",
      "Epochs: 2332/5000----Loss: 318.4650573730469\n",
      "Epochs: 2333/5000----Loss: 318.46478271484375\n",
      "Epochs: 2334/5000----Loss: 318.4643859863281\n",
      "Epochs: 2335/5000----Loss: 318.46405029296875\n",
      "Epochs: 2336/5000----Loss: 318.46380615234375\n",
      "Epochs: 2337/5000----Loss: 318.4635314941406\n",
      "Epochs: 2338/5000----Loss: 318.46319580078125\n",
      "Epochs: 2339/5000----Loss: 318.462890625\n",
      "Epochs: 2340/5000----Loss: 318.4626770019531\n",
      "Epochs: 2341/5000----Loss: 318.46240234375\n",
      "Epochs: 2342/5000----Loss: 318.4620056152344\n",
      "Epochs: 2343/5000----Loss: 318.4618225097656\n",
      "Epochs: 2344/5000----Loss: 318.4614562988281\n",
      "Epochs: 2345/5000----Loss: 318.46124267578125\n",
      "Epochs: 2346/5000----Loss: 318.4609069824219\n",
      "Epochs: 2347/5000----Loss: 318.460693359375\n",
      "Epochs: 2348/5000----Loss: 318.4604187011719\n",
      "Epochs: 2349/5000----Loss: 318.4600524902344\n",
      "Epochs: 2350/5000----Loss: 318.4598693847656\n",
      "Epochs: 2351/5000----Loss: 318.45953369140625\n",
      "Epochs: 2352/5000----Loss: 318.45916748046875\n",
      "Epochs: 2353/5000----Loss: 318.458984375\n",
      "Epochs: 2354/5000----Loss: 318.45867919921875\n",
      "Epochs: 2355/5000----Loss: 318.45831298828125\n",
      "Epochs: 2356/5000----Loss: 318.4581298828125\n",
      "Epochs: 2357/5000----Loss: 318.4579162597656\n",
      "Epochs: 2358/5000----Loss: 318.45751953125\n",
      "Epochs: 2359/5000----Loss: 318.45721435546875\n",
      "Epochs: 2360/5000----Loss: 318.45697021484375\n",
      "Epochs: 2361/5000----Loss: 318.4566345214844\n",
      "Epochs: 2362/5000----Loss: 318.45635986328125\n",
      "Epochs: 2363/5000----Loss: 318.4562072753906\n",
      "Epochs: 2364/5000----Loss: 318.4558410644531\n",
      "Epochs: 2365/5000----Loss: 318.45562744140625\n",
      "Epochs: 2366/5000----Loss: 318.4553527832031\n",
      "Epochs: 2367/5000----Loss: 318.45489501953125\n",
      "Epochs: 2368/5000----Loss: 318.45477294921875\n",
      "Epochs: 2369/5000----Loss: 318.45452880859375\n",
      "Epochs: 2370/5000----Loss: 318.4542236328125\n",
      "Epochs: 2371/5000----Loss: 318.45391845703125\n",
      "Epochs: 2372/5000----Loss: 318.45361328125\n",
      "Epochs: 2373/5000----Loss: 318.453369140625\n",
      "Epochs: 2374/5000----Loss: 318.4530944824219\n",
      "Epochs: 2375/5000----Loss: 318.452880859375\n",
      "Epochs: 2376/5000----Loss: 318.4525451660156\n",
      "Epochs: 2377/5000----Loss: 318.45233154296875\n",
      "Epochs: 2378/5000----Loss: 318.45208740234375\n",
      "Epochs: 2379/5000----Loss: 318.4517517089844\n",
      "Epochs: 2380/5000----Loss: 318.451416015625\n",
      "Epochs: 2381/5000----Loss: 318.45123291015625\n",
      "Epochs: 2382/5000----Loss: 318.4509582519531\n",
      "Epochs: 2383/5000----Loss: 318.45074462890625\n",
      "Epochs: 2384/5000----Loss: 318.45050048828125\n",
      "Epochs: 2385/5000----Loss: 318.45013427734375\n",
      "Epochs: 2386/5000----Loss: 318.44989013671875\n",
      "Epochs: 2387/5000----Loss: 318.44952392578125\n",
      "Epochs: 2388/5000----Loss: 318.4492492675781\n",
      "Epochs: 2389/5000----Loss: 318.44903564453125\n",
      "Epochs: 2390/5000----Loss: 318.4488525390625\n",
      "Epochs: 2391/5000----Loss: 318.4485168457031\n",
      "Epochs: 2392/5000----Loss: 318.44818115234375\n",
      "Epochs: 2393/5000----Loss: 318.44793701171875\n",
      "Epochs: 2394/5000----Loss: 318.4476318359375\n",
      "Epochs: 2395/5000----Loss: 318.4473571777344\n",
      "Epochs: 2396/5000----Loss: 318.44708251953125\n",
      "Epochs: 2397/5000----Loss: 318.4469299316406\n",
      "Epochs: 2398/5000----Loss: 318.4466857910156\n",
      "Epochs: 2399/5000----Loss: 318.4464416503906\n",
      "Epochs: 2400/5000----Loss: 318.4461669921875\n",
      "Epochs: 2401/5000----Loss: 318.4458312988281\n",
      "Epochs: 2402/5000----Loss: 318.4455871582031\n",
      "Epochs: 2403/5000----Loss: 318.4453430175781\n",
      "Epochs: 2404/5000----Loss: 318.44500732421875\n",
      "Epochs: 2405/5000----Loss: 318.44482421875\n",
      "Epochs: 2406/5000----Loss: 318.44464111328125\n",
      "Epochs: 2407/5000----Loss: 318.4443359375\n",
      "Epochs: 2408/5000----Loss: 318.44403076171875\n",
      "Epochs: 2409/5000----Loss: 318.44384765625\n",
      "Epochs: 2410/5000----Loss: 318.4435119628906\n",
      "Epochs: 2411/5000----Loss: 318.44329833984375\n",
      "Epochs: 2412/5000----Loss: 318.44293212890625\n",
      "Epochs: 2413/5000----Loss: 318.44281005859375\n",
      "Epochs: 2414/5000----Loss: 318.4425354003906\n",
      "Epochs: 2415/5000----Loss: 318.44232177734375\n",
      "Epochs: 2416/5000----Loss: 318.4420166015625\n",
      "Epochs: 2417/5000----Loss: 318.4418029785156\n",
      "Epochs: 2418/5000----Loss: 318.44146728515625\n",
      "Epochs: 2419/5000----Loss: 318.4412536621094\n",
      "Epochs: 2420/5000----Loss: 318.44097900390625\n",
      "Epochs: 2421/5000----Loss: 318.4407653808594\n",
      "Epochs: 2422/5000----Loss: 318.44049072265625\n",
      "Epochs: 2423/5000----Loss: 318.4403381347656\n",
      "Epochs: 2424/5000----Loss: 318.43988037109375\n",
      "Epochs: 2425/5000----Loss: 318.4397277832031\n",
      "Epochs: 2426/5000----Loss: 318.43951416015625\n",
      "Epochs: 2427/5000----Loss: 318.43927001953125\n",
      "Epochs: 2428/5000----Loss: 318.4390563964844\n",
      "Epochs: 2429/5000----Loss: 318.4388732910156\n",
      "Epochs: 2430/5000----Loss: 318.4385681152344\n",
      "Epochs: 2431/5000----Loss: 318.43829345703125\n",
      "Epochs: 2432/5000----Loss: 318.4380798339844\n",
      "Epochs: 2433/5000----Loss: 318.43780517578125\n",
      "Epochs: 2434/5000----Loss: 318.4375915527344\n",
      "Epochs: 2435/5000----Loss: 318.4373779296875\n",
      "Epochs: 2436/5000----Loss: 318.4371337890625\n",
      "Epochs: 2437/5000----Loss: 318.43682861328125\n",
      "Epochs: 2438/5000----Loss: 318.4364929199219\n",
      "Epochs: 2439/5000----Loss: 318.4364318847656\n",
      "Epochs: 2440/5000----Loss: 318.4361267089844\n",
      "Epochs: 2441/5000----Loss: 318.4358215332031\n",
      "Epochs: 2442/5000----Loss: 318.43560791015625\n",
      "Epochs: 2443/5000----Loss: 318.43548583984375\n",
      "Epochs: 2444/5000----Loss: 318.4352111816406\n",
      "Epochs: 2445/5000----Loss: 318.43487548828125\n",
      "Epochs: 2446/5000----Loss: 318.43463134765625\n",
      "Epochs: 2447/5000----Loss: 318.43438720703125\n",
      "Epochs: 2448/5000----Loss: 318.43408203125\n",
      "Epochs: 2449/5000----Loss: 318.43389892578125\n",
      "Epochs: 2450/5000----Loss: 318.43353271484375\n",
      "Epochs: 2451/5000----Loss: 318.4333801269531\n",
      "Epochs: 2452/5000----Loss: 318.43316650390625\n",
      "Epochs: 2453/5000----Loss: 318.43280029296875\n",
      "Epochs: 2454/5000----Loss: 318.4327087402344\n",
      "Epochs: 2455/5000----Loss: 318.4324035644531\n",
      "Epochs: 2456/5000----Loss: 318.4321594238281\n",
      "Epochs: 2457/5000----Loss: 318.4319152832031\n",
      "Epochs: 2458/5000----Loss: 318.4316101074219\n",
      "Epochs: 2459/5000----Loss: 318.43145751953125\n",
      "Epochs: 2460/5000----Loss: 318.43109130859375\n",
      "Epochs: 2461/5000----Loss: 318.4310302734375\n",
      "Epochs: 2462/5000----Loss: 318.430908203125\n",
      "Epochs: 2463/5000----Loss: 318.43048095703125\n",
      "Epochs: 2464/5000----Loss: 318.4303283691406\n",
      "Epochs: 2465/5000----Loss: 318.4302062988281\n",
      "Epochs: 2466/5000----Loss: 318.4298400878906\n",
      "Epochs: 2467/5000----Loss: 318.42962646484375\n",
      "Epochs: 2468/5000----Loss: 318.429443359375\n",
      "Epochs: 2469/5000----Loss: 318.42919921875\n",
      "Epochs: 2470/5000----Loss: 318.4291076660156\n",
      "Epochs: 2471/5000----Loss: 318.4287414550781\n",
      "Epochs: 2472/5000----Loss: 318.4284362792969\n",
      "Epochs: 2473/5000----Loss: 318.4283142089844\n",
      "Epochs: 2474/5000----Loss: 318.42803955078125\n",
      "Epochs: 2475/5000----Loss: 318.4278259277344\n",
      "Epochs: 2476/5000----Loss: 318.4275817871094\n",
      "Epochs: 2477/5000----Loss: 318.4273376464844\n",
      "Epochs: 2478/5000----Loss: 318.4270324707031\n",
      "Epochs: 2479/5000----Loss: 318.42681884765625\n",
      "Epochs: 2480/5000----Loss: 318.42669677734375\n",
      "Epochs: 2481/5000----Loss: 318.42645263671875\n",
      "Epochs: 2482/5000----Loss: 318.42620849609375\n",
      "Epochs: 2483/5000----Loss: 318.4259033203125\n",
      "Epochs: 2484/5000----Loss: 318.4257507324219\n",
      "Epochs: 2485/5000----Loss: 318.42559814453125\n",
      "Epochs: 2486/5000----Loss: 318.42535400390625\n",
      "Epochs: 2487/5000----Loss: 318.425048828125\n",
      "Epochs: 2488/5000----Loss: 318.42486572265625\n",
      "Epochs: 2489/5000----Loss: 318.42462158203125\n",
      "Epochs: 2490/5000----Loss: 318.42449951171875\n",
      "Epochs: 2491/5000----Loss: 318.4242248535156\n",
      "Epochs: 2492/5000----Loss: 318.4239807128906\n",
      "Epochs: 2493/5000----Loss: 318.42376708984375\n",
      "Epochs: 2494/5000----Loss: 318.423583984375\n",
      "Epochs: 2495/5000----Loss: 318.4233093261719\n",
      "Epochs: 2496/5000----Loss: 318.42315673828125\n",
      "Epochs: 2497/5000----Loss: 318.4229431152344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2498/5000----Loss: 318.4226379394531\n",
      "Epochs: 2499/5000----Loss: 318.42236328125\n",
      "Epochs: 2500/5000----Loss: 318.42230224609375\n",
      "Epochs: 2501/5000----Loss: 318.4221496582031\n",
      "Epochs: 2502/5000----Loss: 318.4217529296875\n",
      "Epochs: 2503/5000----Loss: 318.421630859375\n",
      "Epochs: 2504/5000----Loss: 318.4213562011719\n",
      "Epochs: 2505/5000----Loss: 318.42108154296875\n",
      "Epochs: 2506/5000----Loss: 318.4208984375\n",
      "Epochs: 2507/5000----Loss: 318.420654296875\n",
      "Epochs: 2508/5000----Loss: 318.4205017089844\n",
      "Epochs: 2509/5000----Loss: 318.42022705078125\n",
      "Epochs: 2510/5000----Loss: 318.4200134277344\n",
      "Epochs: 2511/5000----Loss: 318.4197998046875\n",
      "Epochs: 2512/5000----Loss: 318.4195251464844\n",
      "Epochs: 2513/5000----Loss: 318.4193420410156\n",
      "Epochs: 2514/5000----Loss: 318.41912841796875\n",
      "Epochs: 2515/5000----Loss: 318.4189758300781\n",
      "Epochs: 2516/5000----Loss: 318.4187316894531\n",
      "Epochs: 2517/5000----Loss: 318.41851806640625\n",
      "Epochs: 2518/5000----Loss: 318.418212890625\n",
      "Epochs: 2519/5000----Loss: 318.41802978515625\n",
      "Epochs: 2520/5000----Loss: 318.41796875\n",
      "Epochs: 2521/5000----Loss: 318.41766357421875\n",
      "Epochs: 2522/5000----Loss: 318.4175109863281\n",
      "Epochs: 2523/5000----Loss: 318.41729736328125\n",
      "Epochs: 2524/5000----Loss: 318.41705322265625\n",
      "Epochs: 2525/5000----Loss: 318.4168701171875\n",
      "Epochs: 2526/5000----Loss: 318.4165344238281\n",
      "Epochs: 2527/5000----Loss: 318.41644287109375\n",
      "Epochs: 2528/5000----Loss: 318.4161682128906\n",
      "Epochs: 2529/5000----Loss: 318.41595458984375\n",
      "Epochs: 2530/5000----Loss: 318.41583251953125\n",
      "Epochs: 2531/5000----Loss: 318.4156799316406\n",
      "Epochs: 2532/5000----Loss: 318.4153747558594\n",
      "Epochs: 2533/5000----Loss: 318.4151611328125\n",
      "Epochs: 2534/5000----Loss: 318.4150390625\n",
      "Epochs: 2535/5000----Loss: 318.4147644042969\n",
      "Epochs: 2536/5000----Loss: 318.41455078125\n",
      "Epochs: 2537/5000----Loss: 318.4143371582031\n",
      "Epochs: 2538/5000----Loss: 318.41412353515625\n",
      "Epochs: 2539/5000----Loss: 318.41387939453125\n",
      "Epochs: 2540/5000----Loss: 318.41375732421875\n",
      "Epochs: 2541/5000----Loss: 318.41363525390625\n",
      "Epochs: 2542/5000----Loss: 318.41339111328125\n",
      "Epochs: 2543/5000----Loss: 318.4131164550781\n",
      "Epochs: 2544/5000----Loss: 318.41290283203125\n",
      "Epochs: 2545/5000----Loss: 318.4128112792969\n",
      "Epochs: 2546/5000----Loss: 318.41259765625\n",
      "Epochs: 2547/5000----Loss: 318.4123840332031\n",
      "Epochs: 2548/5000----Loss: 318.41217041015625\n",
      "Epochs: 2549/5000----Loss: 318.4119567871094\n",
      "Epochs: 2550/5000----Loss: 318.41180419921875\n",
      "Epochs: 2551/5000----Loss: 318.41156005859375\n",
      "Epochs: 2552/5000----Loss: 318.4113464355469\n",
      "Epochs: 2553/5000----Loss: 318.4111328125\n",
      "Epochs: 2554/5000----Loss: 318.4109802246094\n",
      "Epochs: 2555/5000----Loss: 318.4107971191406\n",
      "Epochs: 2556/5000----Loss: 318.41058349609375\n",
      "Epochs: 2557/5000----Loss: 318.41033935546875\n",
      "Epochs: 2558/5000----Loss: 318.41021728515625\n",
      "Epochs: 2559/5000----Loss: 318.4099426269531\n",
      "Epochs: 2560/5000----Loss: 318.4097595214844\n",
      "Epochs: 2561/5000----Loss: 318.40948486328125\n",
      "Epochs: 2562/5000----Loss: 318.4092712402344\n",
      "Epochs: 2563/5000----Loss: 318.40911865234375\n",
      "Epochs: 2564/5000----Loss: 318.4089660644531\n",
      "Epochs: 2565/5000----Loss: 318.40875244140625\n",
      "Epochs: 2566/5000----Loss: 318.4085693359375\n",
      "Epochs: 2567/5000----Loss: 318.4082946777344\n",
      "Epochs: 2568/5000----Loss: 318.4081115722656\n",
      "Epochs: 2569/5000----Loss: 318.40789794921875\n",
      "Epochs: 2570/5000----Loss: 318.4078063964844\n",
      "Epochs: 2571/5000----Loss: 318.40753173828125\n",
      "Epochs: 2572/5000----Loss: 318.4073181152344\n",
      "Epochs: 2573/5000----Loss: 318.4072265625\n",
      "Epochs: 2574/5000----Loss: 318.40692138671875\n",
      "Epochs: 2575/5000----Loss: 318.4067687988281\n",
      "Epochs: 2576/5000----Loss: 318.40655517578125\n",
      "Epochs: 2577/5000----Loss: 318.40643310546875\n",
      "Epochs: 2578/5000----Loss: 318.4061584472656\n",
      "Epochs: 2579/5000----Loss: 318.4060363769531\n",
      "Epochs: 2580/5000----Loss: 318.40594482421875\n",
      "Epochs: 2581/5000----Loss: 318.40570068359375\n",
      "Epochs: 2582/5000----Loss: 318.40545654296875\n",
      "Epochs: 2583/5000----Loss: 318.40533447265625\n",
      "Epochs: 2584/5000----Loss: 318.40521240234375\n",
      "Epochs: 2585/5000----Loss: 318.4048767089844\n",
      "Epochs: 2586/5000----Loss: 318.40460205078125\n",
      "Epochs: 2587/5000----Loss: 318.40447998046875\n",
      "Epochs: 2588/5000----Loss: 318.40435791015625\n",
      "Epochs: 2589/5000----Loss: 318.40411376953125\n",
      "Epochs: 2590/5000----Loss: 318.40399169921875\n",
      "Epochs: 2591/5000----Loss: 318.40374755859375\n",
      "Epochs: 2592/5000----Loss: 318.40350341796875\n",
      "Epochs: 2593/5000----Loss: 318.4034118652344\n",
      "Epochs: 2594/5000----Loss: 318.4031066894531\n",
      "Epochs: 2595/5000----Loss: 318.40313720703125\n",
      "Epochs: 2596/5000----Loss: 318.40283203125\n",
      "Epochs: 2597/5000----Loss: 318.4027099609375\n",
      "Epochs: 2598/5000----Loss: 318.40252685546875\n",
      "Epochs: 2599/5000----Loss: 318.4021911621094\n",
      "Epochs: 2600/5000----Loss: 318.40216064453125\n",
      "Epochs: 2601/5000----Loss: 318.40185546875\n",
      "Epochs: 2602/5000----Loss: 318.40179443359375\n",
      "Epochs: 2603/5000----Loss: 318.4015197753906\n",
      "Epochs: 2604/5000----Loss: 318.4013671875\n",
      "Epochs: 2605/5000----Loss: 318.4012451171875\n",
      "Epochs: 2606/5000----Loss: 318.40106201171875\n",
      "Epochs: 2607/5000----Loss: 318.40087890625\n",
      "Epochs: 2608/5000----Loss: 318.40069580078125\n",
      "Epochs: 2609/5000----Loss: 318.4005126953125\n",
      "Epochs: 2610/5000----Loss: 318.40032958984375\n",
      "Epochs: 2611/5000----Loss: 318.4002380371094\n",
      "Epochs: 2612/5000----Loss: 318.39996337890625\n",
      "Epochs: 2613/5000----Loss: 318.3997802734375\n",
      "Epochs: 2614/5000----Loss: 318.399658203125\n",
      "Epochs: 2615/5000----Loss: 318.39935302734375\n",
      "Epochs: 2616/5000----Loss: 318.3992919921875\n",
      "Epochs: 2617/5000----Loss: 318.39898681640625\n",
      "Epochs: 2618/5000----Loss: 318.3988342285156\n",
      "Epochs: 2619/5000----Loss: 318.3986511230469\n",
      "Epochs: 2620/5000----Loss: 318.39849853515625\n",
      "Epochs: 2621/5000----Loss: 318.39825439453125\n",
      "Epochs: 2622/5000----Loss: 318.39813232421875\n",
      "Epochs: 2623/5000----Loss: 318.3979797363281\n",
      "Epochs: 2624/5000----Loss: 318.397705078125\n",
      "Epochs: 2625/5000----Loss: 318.39764404296875\n",
      "Epochs: 2626/5000----Loss: 318.3973693847656\n",
      "Epochs: 2627/5000----Loss: 318.3973083496094\n",
      "Epochs: 2628/5000----Loss: 318.3970642089844\n",
      "Epochs: 2629/5000----Loss: 318.39691162109375\n",
      "Epochs: 2630/5000----Loss: 318.3966369628906\n",
      "Epochs: 2631/5000----Loss: 318.39654541015625\n",
      "Epochs: 2632/5000----Loss: 318.396240234375\n",
      "Epochs: 2633/5000----Loss: 318.3962707519531\n",
      "Epochs: 2634/5000----Loss: 318.3960876464844\n",
      "Epochs: 2635/5000----Loss: 318.3957824707031\n",
      "Epochs: 2636/5000----Loss: 318.39569091796875\n",
      "Epochs: 2637/5000----Loss: 318.3954772949219\n",
      "Epochs: 2638/5000----Loss: 318.3952941894531\n",
      "Epochs: 2639/5000----Loss: 318.39508056640625\n",
      "Epochs: 2640/5000----Loss: 318.3950500488281\n",
      "Epochs: 2641/5000----Loss: 318.39483642578125\n",
      "Epochs: 2642/5000----Loss: 318.3945617675781\n",
      "Epochs: 2643/5000----Loss: 318.39453125\n",
      "Epochs: 2644/5000----Loss: 318.3943786621094\n",
      "Epochs: 2645/5000----Loss: 318.39410400390625\n",
      "Epochs: 2646/5000----Loss: 318.3939514160156\n",
      "Epochs: 2647/5000----Loss: 318.39385986328125\n",
      "Epochs: 2648/5000----Loss: 318.39361572265625\n",
      "Epochs: 2649/5000----Loss: 318.3934020996094\n",
      "Epochs: 2650/5000----Loss: 318.3933410644531\n",
      "Epochs: 2651/5000----Loss: 318.39300537109375\n",
      "Epochs: 2652/5000----Loss: 318.3929748535156\n",
      "Epochs: 2653/5000----Loss: 318.3927307128906\n",
      "Epochs: 2654/5000----Loss: 318.39263916015625\n",
      "Epochs: 2655/5000----Loss: 318.3924255371094\n",
      "Epochs: 2656/5000----Loss: 318.39227294921875\n",
      "Epochs: 2657/5000----Loss: 318.3920593261719\n",
      "Epochs: 2658/5000----Loss: 318.39202880859375\n",
      "Epochs: 2659/5000----Loss: 318.3917541503906\n",
      "Epochs: 2660/5000----Loss: 318.3916015625\n",
      "Epochs: 2661/5000----Loss: 318.3914794921875\n",
      "Epochs: 2662/5000----Loss: 318.3912658691406\n",
      "Epochs: 2663/5000----Loss: 318.3912353515625\n",
      "Epochs: 2664/5000----Loss: 318.3910217285156\n",
      "Epochs: 2665/5000----Loss: 318.3907775878906\n",
      "Epochs: 2666/5000----Loss: 318.3905944824219\n",
      "Epochs: 2667/5000----Loss: 318.39044189453125\n",
      "Epochs: 2668/5000----Loss: 318.3904113769531\n",
      "Epochs: 2669/5000----Loss: 318.3901672363281\n",
      "Epochs: 2670/5000----Loss: 318.3900146484375\n",
      "Epochs: 2671/5000----Loss: 318.389892578125\n",
      "Epochs: 2672/5000----Loss: 318.3896789550781\n",
      "Epochs: 2673/5000----Loss: 318.3894958496094\n",
      "Epochs: 2674/5000----Loss: 318.3893127441406\n",
      "Epochs: 2675/5000----Loss: 318.38909912109375\n",
      "Epochs: 2676/5000----Loss: 318.388916015625\n",
      "Epochs: 2677/5000----Loss: 318.3888244628906\n",
      "Epochs: 2678/5000----Loss: 318.388671875\n",
      "Epochs: 2679/5000----Loss: 318.38848876953125\n",
      "Epochs: 2680/5000----Loss: 318.38836669921875\n",
      "Epochs: 2681/5000----Loss: 318.3880920410156\n",
      "Epochs: 2682/5000----Loss: 318.38800048828125\n",
      "Epochs: 2683/5000----Loss: 318.38775634765625\n",
      "Epochs: 2684/5000----Loss: 318.3876037597656\n",
      "Epochs: 2685/5000----Loss: 318.387451171875\n",
      "Epochs: 2686/5000----Loss: 318.3873596191406\n",
      "Epochs: 2687/5000----Loss: 318.3871765136719\n",
      "Epochs: 2688/5000----Loss: 318.3870544433594\n",
      "Epochs: 2689/5000----Loss: 318.3868713378906\n",
      "Epochs: 2690/5000----Loss: 318.38665771484375\n",
      "Epochs: 2691/5000----Loss: 318.38653564453125\n",
      "Epochs: 2692/5000----Loss: 318.38641357421875\n",
      "Epochs: 2693/5000----Loss: 318.3862609863281\n",
      "Epochs: 2694/5000----Loss: 318.3860778808594\n",
      "Epochs: 2695/5000----Loss: 318.3858642578125\n",
      "Epochs: 2696/5000----Loss: 318.38568115234375\n",
      "Epochs: 2697/5000----Loss: 318.3857421875\n",
      "Epochs: 2698/5000----Loss: 318.38555908203125\n",
      "Epochs: 2699/5000----Loss: 318.38531494140625\n",
      "Epochs: 2700/5000----Loss: 318.38519287109375\n",
      "Epochs: 2701/5000----Loss: 318.3850402832031\n",
      "Epochs: 2702/5000----Loss: 318.3848876953125\n",
      "Epochs: 2703/5000----Loss: 318.384765625\n",
      "Epochs: 2704/5000----Loss: 318.384521484375\n",
      "Epochs: 2705/5000----Loss: 318.3843994140625\n",
      "Epochs: 2706/5000----Loss: 318.3841857910156\n",
      "Epochs: 2707/5000----Loss: 318.38409423828125\n",
      "Epochs: 2708/5000----Loss: 318.3840026855469\n",
      "Epochs: 2709/5000----Loss: 318.38385009765625\n",
      "Epochs: 2710/5000----Loss: 318.3836364746094\n",
      "Epochs: 2711/5000----Loss: 318.38360595703125\n",
      "Epochs: 2712/5000----Loss: 318.3834228515625\n",
      "Epochs: 2713/5000----Loss: 318.38323974609375\n",
      "Epochs: 2714/5000----Loss: 318.38311767578125\n",
      "Epochs: 2715/5000----Loss: 318.3828125\n",
      "Epochs: 2716/5000----Loss: 318.38262939453125\n",
      "Epochs: 2717/5000----Loss: 318.3825378417969\n",
      "Epochs: 2718/5000----Loss: 318.38238525390625\n",
      "Epochs: 2719/5000----Loss: 318.3822937011719\n",
      "Epochs: 2720/5000----Loss: 318.3820495605469\n",
      "Epochs: 2721/5000----Loss: 318.3819274902344\n",
      "Epochs: 2722/5000----Loss: 318.38177490234375\n",
      "Epochs: 2723/5000----Loss: 318.38177490234375\n",
      "Epochs: 2724/5000----Loss: 318.38153076171875\n",
      "Epochs: 2725/5000----Loss: 318.3815002441406\n",
      "Epochs: 2726/5000----Loss: 318.38140869140625\n",
      "Epochs: 2727/5000----Loss: 318.38104248046875\n",
      "Epochs: 2728/5000----Loss: 318.3809814453125\n",
      "Epochs: 2729/5000----Loss: 318.380859375\n",
      "Epochs: 2730/5000----Loss: 318.3807067871094\n",
      "Epochs: 2731/5000----Loss: 318.3803405761719\n",
      "Epochs: 2732/5000----Loss: 318.38031005859375\n",
      "Epochs: 2733/5000----Loss: 318.380126953125\n",
      "Epochs: 2734/5000----Loss: 318.3799743652344\n",
      "Epochs: 2735/5000----Loss: 318.3798522949219\n",
      "Epochs: 2736/5000----Loss: 318.379638671875\n",
      "Epochs: 2737/5000----Loss: 318.3794860839844\n",
      "Epochs: 2738/5000----Loss: 318.37939453125\n",
      "Epochs: 2739/5000----Loss: 318.37933349609375\n",
      "Epochs: 2740/5000----Loss: 318.3791809082031\n",
      "Epochs: 2741/5000----Loss: 318.37896728515625\n",
      "Epochs: 2742/5000----Loss: 318.3789367675781\n",
      "Epochs: 2743/5000----Loss: 318.3786926269531\n",
      "Epochs: 2744/5000----Loss: 318.3785400390625\n",
      "Epochs: 2745/5000----Loss: 318.37847900390625\n",
      "Epochs: 2746/5000----Loss: 318.3782958984375\n",
      "Epochs: 2747/5000----Loss: 318.3782043457031\n",
      "Epochs: 2748/5000----Loss: 318.37799072265625\n",
      "Epochs: 2749/5000----Loss: 318.3777770996094\n",
      "Epochs: 2750/5000----Loss: 318.37774658203125\n",
      "Epochs: 2751/5000----Loss: 318.37750244140625\n",
      "Epochs: 2752/5000----Loss: 318.3774108886719\n",
      "Epochs: 2753/5000----Loss: 318.37725830078125\n",
      "Epochs: 2754/5000----Loss: 318.3771057128906\n",
      "Epochs: 2755/5000----Loss: 318.3769836425781\n",
      "Epochs: 2756/5000----Loss: 318.3768615722656\n",
      "Epochs: 2757/5000----Loss: 318.3767395019531\n",
      "Epochs: 2758/5000----Loss: 318.3766784667969\n",
      "Epochs: 2759/5000----Loss: 318.37652587890625\n",
      "Epochs: 2760/5000----Loss: 318.3763122558594\n",
      "Epochs: 2761/5000----Loss: 318.3760681152344\n",
      "Epochs: 2762/5000----Loss: 318.37603759765625\n",
      "Epochs: 2763/5000----Loss: 318.3758850097656\n",
      "Epochs: 2764/5000----Loss: 318.375732421875\n",
      "Epochs: 2765/5000----Loss: 318.3756103515625\n",
      "Epochs: 2766/5000----Loss: 318.3754577636719\n",
      "Epochs: 2767/5000----Loss: 318.3753356933594\n",
      "Epochs: 2768/5000----Loss: 318.3752136230469\n",
      "Epochs: 2769/5000----Loss: 318.37506103515625\n",
      "Epochs: 2770/5000----Loss: 318.37493896484375\n",
      "Epochs: 2771/5000----Loss: 318.3747863769531\n",
      "Epochs: 2772/5000----Loss: 318.3746337890625\n",
      "Epochs: 2773/5000----Loss: 318.37451171875\n",
      "Epochs: 2774/5000----Loss: 318.37432861328125\n",
      "Epochs: 2775/5000----Loss: 318.37420654296875\n",
      "Epochs: 2776/5000----Loss: 318.3741149902344\n",
      "Epochs: 2777/5000----Loss: 318.3739013671875\n",
      "Epochs: 2778/5000----Loss: 318.3738708496094\n",
      "Epochs: 2779/5000----Loss: 318.37371826171875\n",
      "Epochs: 2780/5000----Loss: 318.3735046386719\n",
      "Epochs: 2781/5000----Loss: 318.37353515625\n",
      "Epochs: 2782/5000----Loss: 318.3731994628906\n",
      "Epochs: 2783/5000----Loss: 318.3730773925781\n",
      "Epochs: 2784/5000----Loss: 318.3729248046875\n",
      "Epochs: 2785/5000----Loss: 318.37274169921875\n",
      "Epochs: 2786/5000----Loss: 318.37261962890625\n",
      "Epochs: 2787/5000----Loss: 318.37249755859375\n",
      "Epochs: 2788/5000----Loss: 318.37237548828125\n",
      "Epochs: 2789/5000----Loss: 318.372314453125\n",
      "Epochs: 2790/5000----Loss: 318.3721008300781\n",
      "Epochs: 2791/5000----Loss: 318.3720397949219\n",
      "Epochs: 2792/5000----Loss: 318.3718566894531\n",
      "Epochs: 2793/5000----Loss: 318.3717041015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2794/5000----Loss: 318.3715515136719\n",
      "Epochs: 2795/5000----Loss: 318.3714904785156\n",
      "Epochs: 2796/5000----Loss: 318.37139892578125\n",
      "Epochs: 2797/5000----Loss: 318.3711853027344\n",
      "Epochs: 2798/5000----Loss: 318.3710632324219\n",
      "Epochs: 2799/5000----Loss: 318.3709716796875\n",
      "Epochs: 2800/5000----Loss: 318.3706970214844\n",
      "Epochs: 2801/5000----Loss: 318.37066650390625\n",
      "Epochs: 2802/5000----Loss: 318.37054443359375\n",
      "Epochs: 2803/5000----Loss: 318.3704528808594\n",
      "Epochs: 2804/5000----Loss: 318.37042236328125\n",
      "Epochs: 2805/5000----Loss: 318.3702697753906\n",
      "Epochs: 2806/5000----Loss: 318.3700256347656\n",
      "Epochs: 2807/5000----Loss: 318.36993408203125\n",
      "Epochs: 2808/5000----Loss: 318.36968994140625\n",
      "Epochs: 2809/5000----Loss: 318.3697509765625\n",
      "Epochs: 2810/5000----Loss: 318.36944580078125\n",
      "Epochs: 2811/5000----Loss: 318.36932373046875\n",
      "Epochs: 2812/5000----Loss: 318.3692932128906\n",
      "Epochs: 2813/5000----Loss: 318.3691711425781\n",
      "Epochs: 2814/5000----Loss: 318.3690185546875\n",
      "Epochs: 2815/5000----Loss: 318.3689270019531\n",
      "Epochs: 2816/5000----Loss: 318.36871337890625\n",
      "Epochs: 2817/5000----Loss: 318.3685607910156\n",
      "Epochs: 2818/5000----Loss: 318.36846923828125\n",
      "Epochs: 2819/5000----Loss: 318.3683776855469\n",
      "Epochs: 2820/5000----Loss: 318.36822509765625\n",
      "Epochs: 2821/5000----Loss: 318.3681945800781\n",
      "Epochs: 2822/5000----Loss: 318.3678894042969\n",
      "Epochs: 2823/5000----Loss: 318.36785888671875\n",
      "Epochs: 2824/5000----Loss: 318.3677978515625\n",
      "Epochs: 2825/5000----Loss: 318.36761474609375\n",
      "Epochs: 2826/5000----Loss: 318.36749267578125\n",
      "Epochs: 2827/5000----Loss: 318.367431640625\n",
      "Epochs: 2828/5000----Loss: 318.36724853515625\n",
      "Epochs: 2829/5000----Loss: 318.3671569824219\n",
      "Epochs: 2830/5000----Loss: 318.36700439453125\n",
      "Epochs: 2831/5000----Loss: 318.3669738769531\n",
      "Epochs: 2832/5000----Loss: 318.36676025390625\n",
      "Epochs: 2833/5000----Loss: 318.3666687011719\n",
      "Epochs: 2834/5000----Loss: 318.3664855957031\n",
      "Epochs: 2835/5000----Loss: 318.36627197265625\n",
      "Epochs: 2836/5000----Loss: 318.3662414550781\n",
      "Epochs: 2837/5000----Loss: 318.3661193847656\n",
      "Epochs: 2838/5000----Loss: 318.3659362792969\n",
      "Epochs: 2839/5000----Loss: 318.3657531738281\n",
      "Epochs: 2840/5000----Loss: 318.36572265625\n",
      "Epochs: 2841/5000----Loss: 318.36553955078125\n",
      "Epochs: 2842/5000----Loss: 318.3655090332031\n",
      "Epochs: 2843/5000----Loss: 318.36529541015625\n",
      "Epochs: 2844/5000----Loss: 318.36517333984375\n",
      "Epochs: 2845/5000----Loss: 318.3650817871094\n",
      "Epochs: 2846/5000----Loss: 318.3649597167969\n",
      "Epochs: 2847/5000----Loss: 318.3648986816406\n",
      "Epochs: 2848/5000----Loss: 318.3647766113281\n",
      "Epochs: 2849/5000----Loss: 318.36456298828125\n",
      "Epochs: 2850/5000----Loss: 318.3644714355469\n",
      "Epochs: 2851/5000----Loss: 318.3642883300781\n",
      "Epochs: 2852/5000----Loss: 318.36431884765625\n",
      "Epochs: 2853/5000----Loss: 318.36419677734375\n",
      "Epochs: 2854/5000----Loss: 318.36407470703125\n",
      "Epochs: 2855/5000----Loss: 318.3638610839844\n",
      "Epochs: 2856/5000----Loss: 318.36383056640625\n",
      "Epochs: 2857/5000----Loss: 318.3636169433594\n",
      "Epochs: 2858/5000----Loss: 318.3634948730469\n",
      "Epochs: 2859/5000----Loss: 318.36334228515625\n",
      "Epochs: 2860/5000----Loss: 318.36328125\n",
      "Epochs: 2861/5000----Loss: 318.3631591796875\n",
      "Epochs: 2862/5000----Loss: 318.36309814453125\n",
      "Epochs: 2863/5000----Loss: 318.3629150390625\n",
      "Epochs: 2864/5000----Loss: 318.3628234863281\n",
      "Epochs: 2865/5000----Loss: 318.3626708984375\n",
      "Epochs: 2866/5000----Loss: 318.3625793457031\n",
      "Epochs: 2867/5000----Loss: 318.36248779296875\n",
      "Epochs: 2868/5000----Loss: 318.36236572265625\n",
      "Epochs: 2869/5000----Loss: 318.36224365234375\n",
      "Epochs: 2870/5000----Loss: 318.36212158203125\n",
      "Epochs: 2871/5000----Loss: 318.36187744140625\n",
      "Epochs: 2872/5000----Loss: 318.36199951171875\n",
      "Epochs: 2873/5000----Loss: 318.3617858886719\n",
      "Epochs: 2874/5000----Loss: 318.36175537109375\n",
      "Epochs: 2875/5000----Loss: 318.361572265625\n",
      "Epochs: 2876/5000----Loss: 318.3614196777344\n",
      "Epochs: 2877/5000----Loss: 318.3612976074219\n",
      "Epochs: 2878/5000----Loss: 318.36126708984375\n",
      "Epochs: 2879/5000----Loss: 318.3611145019531\n",
      "Epochs: 2880/5000----Loss: 318.3609313964844\n",
      "Epochs: 2881/5000----Loss: 318.3608093261719\n",
      "Epochs: 2882/5000----Loss: 318.36077880859375\n",
      "Epochs: 2883/5000----Loss: 318.36065673828125\n",
      "Epochs: 2884/5000----Loss: 318.3605041503906\n",
      "Epochs: 2885/5000----Loss: 318.3602600097656\n",
      "Epochs: 2886/5000----Loss: 318.3601989746094\n",
      "Epochs: 2887/5000----Loss: 318.3600158691406\n",
      "Epochs: 2888/5000----Loss: 318.3599853515625\n",
      "Epochs: 2889/5000----Loss: 318.35986328125\n",
      "Epochs: 2890/5000----Loss: 318.3596496582031\n",
      "Epochs: 2891/5000----Loss: 318.35955810546875\n",
      "Epochs: 2892/5000----Loss: 318.35943603515625\n",
      "Epochs: 2893/5000----Loss: 318.3594665527344\n",
      "Epochs: 2894/5000----Loss: 318.3592224121094\n",
      "Epochs: 2895/5000----Loss: 318.35919189453125\n",
      "Epochs: 2896/5000----Loss: 318.3590393066406\n",
      "Epochs: 2897/5000----Loss: 318.3590393066406\n",
      "Epochs: 2898/5000----Loss: 318.3588562011719\n",
      "Epochs: 2899/5000----Loss: 318.3587646484375\n",
      "Epochs: 2900/5000----Loss: 318.35858154296875\n",
      "Epochs: 2901/5000----Loss: 318.3584289550781\n",
      "Epochs: 2902/5000----Loss: 318.35845947265625\n",
      "Epochs: 2903/5000----Loss: 318.35833740234375\n",
      "Epochs: 2904/5000----Loss: 318.3582763671875\n",
      "Epochs: 2905/5000----Loss: 318.35821533203125\n",
      "Epochs: 2906/5000----Loss: 318.3580017089844\n",
      "Epochs: 2907/5000----Loss: 318.3578796386719\n",
      "Epochs: 2908/5000----Loss: 318.35784912109375\n",
      "Epochs: 2909/5000----Loss: 318.3576965332031\n",
      "Epochs: 2910/5000----Loss: 318.35748291015625\n",
      "Epochs: 2911/5000----Loss: 318.357421875\n",
      "Epochs: 2912/5000----Loss: 318.3572692871094\n",
      "Epochs: 2913/5000----Loss: 318.3572082519531\n",
      "Epochs: 2914/5000----Loss: 318.3570251464844\n",
      "Epochs: 2915/5000----Loss: 318.35699462890625\n",
      "Epochs: 2916/5000----Loss: 318.35687255859375\n",
      "Epochs: 2917/5000----Loss: 318.35675048828125\n",
      "Epochs: 2918/5000----Loss: 318.35662841796875\n",
      "Epochs: 2919/5000----Loss: 318.3565673828125\n",
      "Epochs: 2920/5000----Loss: 318.3564758300781\n",
      "Epochs: 2921/5000----Loss: 318.35626220703125\n",
      "Epochs: 2922/5000----Loss: 318.3562316894531\n",
      "Epochs: 2923/5000----Loss: 318.3561096191406\n",
      "Epochs: 2924/5000----Loss: 318.3561096191406\n",
      "Epochs: 2925/5000----Loss: 318.35589599609375\n",
      "Epochs: 2926/5000----Loss: 318.3558044433594\n",
      "Epochs: 2927/5000----Loss: 318.355712890625\n",
      "Epochs: 2928/5000----Loss: 318.3556213378906\n",
      "Epochs: 2929/5000----Loss: 318.3554382324219\n",
      "Epochs: 2930/5000----Loss: 318.35528564453125\n",
      "Epochs: 2931/5000----Loss: 318.35528564453125\n",
      "Epochs: 2932/5000----Loss: 318.35516357421875\n",
      "Epochs: 2933/5000----Loss: 318.35504150390625\n",
      "Epochs: 2934/5000----Loss: 318.35504150390625\n",
      "Epochs: 2935/5000----Loss: 318.3548889160156\n",
      "Epochs: 2936/5000----Loss: 318.354736328125\n",
      "Epochs: 2937/5000----Loss: 318.3546142578125\n",
      "Epochs: 2938/5000----Loss: 318.35443115234375\n",
      "Epochs: 2939/5000----Loss: 318.35430908203125\n",
      "Epochs: 2940/5000----Loss: 318.3541564941406\n",
      "Epochs: 2941/5000----Loss: 318.3541564941406\n",
      "Epochs: 2942/5000----Loss: 318.35400390625\n",
      "Epochs: 2943/5000----Loss: 318.35394287109375\n",
      "Epochs: 2944/5000----Loss: 318.3538818359375\n",
      "Epochs: 2945/5000----Loss: 318.35369873046875\n",
      "Epochs: 2946/5000----Loss: 318.35357666015625\n",
      "Epochs: 2947/5000----Loss: 318.3536682128906\n",
      "Epochs: 2948/5000----Loss: 318.35345458984375\n",
      "Epochs: 2949/5000----Loss: 318.3533020019531\n",
      "Epochs: 2950/5000----Loss: 318.3532409667969\n",
      "Epochs: 2951/5000----Loss: 318.3531188964844\n",
      "Epochs: 2952/5000----Loss: 318.3529968261719\n",
      "Epochs: 2953/5000----Loss: 318.35296630859375\n",
      "Epochs: 2954/5000----Loss: 318.3528137207031\n",
      "Epochs: 2955/5000----Loss: 318.35284423828125\n",
      "Epochs: 2956/5000----Loss: 318.35260009765625\n",
      "Epochs: 2957/5000----Loss: 318.3525085449219\n",
      "Epochs: 2958/5000----Loss: 318.3524169921875\n",
      "Epochs: 2959/5000----Loss: 318.35223388671875\n",
      "Epochs: 2960/5000----Loss: 318.3522033691406\n",
      "Epochs: 2961/5000----Loss: 318.35198974609375\n",
      "Epochs: 2962/5000----Loss: 318.35198974609375\n",
      "Epochs: 2963/5000----Loss: 318.3519592285156\n",
      "Epochs: 2964/5000----Loss: 318.35174560546875\n",
      "Epochs: 2965/5000----Loss: 318.3517150878906\n",
      "Epochs: 2966/5000----Loss: 318.35162353515625\n",
      "Epochs: 2967/5000----Loss: 318.3515319824219\n",
      "Epochs: 2968/5000----Loss: 318.351318359375\n",
      "Epochs: 2969/5000----Loss: 318.35125732421875\n",
      "Epochs: 2970/5000----Loss: 318.351318359375\n",
      "Epochs: 2971/5000----Loss: 318.35113525390625\n",
      "Epochs: 2972/5000----Loss: 318.35089111328125\n",
      "Epochs: 2973/5000----Loss: 318.35089111328125\n",
      "Epochs: 2974/5000----Loss: 318.3508605957031\n",
      "Epochs: 2975/5000----Loss: 318.35064697265625\n",
      "Epochs: 2976/5000----Loss: 318.3506164550781\n",
      "Epochs: 2977/5000----Loss: 318.3505859375\n",
      "Epochs: 2978/5000----Loss: 318.35040283203125\n",
      "Epochs: 2979/5000----Loss: 318.3502502441406\n",
      "Epochs: 2980/5000----Loss: 318.3502502441406\n",
      "Epochs: 2981/5000----Loss: 318.35015869140625\n",
      "Epochs: 2982/5000----Loss: 318.35003662109375\n",
      "Epochs: 2983/5000----Loss: 318.3499450683594\n",
      "Epochs: 2984/5000----Loss: 318.3497619628906\n",
      "Epochs: 2985/5000----Loss: 318.34967041015625\n",
      "Epochs: 2986/5000----Loss: 318.34967041015625\n",
      "Epochs: 2987/5000----Loss: 318.34942626953125\n",
      "Epochs: 2988/5000----Loss: 318.349365234375\n",
      "Epochs: 2989/5000----Loss: 318.34918212890625\n",
      "Epochs: 2990/5000----Loss: 318.3490905761719\n",
      "Epochs: 2991/5000----Loss: 318.34906005859375\n",
      "Epochs: 2992/5000----Loss: 318.348876953125\n",
      "Epochs: 2993/5000----Loss: 318.34881591796875\n",
      "Epochs: 2994/5000----Loss: 318.3487548828125\n",
      "Epochs: 2995/5000----Loss: 318.3486633300781\n",
      "Epochs: 2996/5000----Loss: 318.3485107421875\n",
      "Epochs: 2997/5000----Loss: 318.34857177734375\n",
      "Epochs: 2998/5000----Loss: 318.34844970703125\n",
      "Epochs: 2999/5000----Loss: 318.3482360839844\n",
      "Epochs: 3000/5000----Loss: 318.3481750488281\n",
      "Epochs: 3001/5000----Loss: 318.3481750488281\n",
      "Epochs: 3002/5000----Loss: 318.3479919433594\n",
      "Epochs: 3003/5000----Loss: 318.34783935546875\n",
      "Epochs: 3004/5000----Loss: 318.3477783203125\n",
      "Epochs: 3005/5000----Loss: 318.34771728515625\n",
      "Epochs: 3006/5000----Loss: 318.3476867675781\n",
      "Epochs: 3007/5000----Loss: 318.3475341796875\n",
      "Epochs: 3008/5000----Loss: 318.34747314453125\n",
      "Epochs: 3009/5000----Loss: 318.3473205566406\n",
      "Epochs: 3010/5000----Loss: 318.3472595214844\n",
      "Epochs: 3011/5000----Loss: 318.34710693359375\n",
      "Epochs: 3012/5000----Loss: 318.3470153808594\n",
      "Epochs: 3013/5000----Loss: 318.34698486328125\n",
      "Epochs: 3014/5000----Loss: 318.34686279296875\n",
      "Epochs: 3015/5000----Loss: 318.3468017578125\n",
      "Epochs: 3016/5000----Loss: 318.34674072265625\n",
      "Epochs: 3017/5000----Loss: 318.3465576171875\n",
      "Epochs: 3018/5000----Loss: 318.34637451171875\n",
      "Epochs: 3019/5000----Loss: 318.346435546875\n",
      "Epochs: 3020/5000----Loss: 318.3462829589844\n",
      "Epochs: 3021/5000----Loss: 318.34625244140625\n",
      "Epochs: 3022/5000----Loss: 318.34613037109375\n",
      "Epochs: 3023/5000----Loss: 318.3460693359375\n",
      "Epochs: 3024/5000----Loss: 318.3460388183594\n",
      "Epochs: 3025/5000----Loss: 318.3458251953125\n",
      "Epochs: 3026/5000----Loss: 318.34576416015625\n",
      "Epochs: 3027/5000----Loss: 318.34564208984375\n",
      "Epochs: 3028/5000----Loss: 318.34564208984375\n",
      "Epochs: 3029/5000----Loss: 318.34552001953125\n",
      "Epochs: 3030/5000----Loss: 318.34539794921875\n",
      "Epochs: 3031/5000----Loss: 318.3453369140625\n",
      "Epochs: 3032/5000----Loss: 318.34527587890625\n",
      "Epochs: 3033/5000----Loss: 318.3452453613281\n",
      "Epochs: 3034/5000----Loss: 318.34503173828125\n",
      "Epochs: 3035/5000----Loss: 318.34490966796875\n",
      "Epochs: 3036/5000----Loss: 318.3449401855469\n",
      "Epochs: 3037/5000----Loss: 318.34478759765625\n",
      "Epochs: 3038/5000----Loss: 318.34466552734375\n",
      "Epochs: 3039/5000----Loss: 318.3445129394531\n",
      "Epochs: 3040/5000----Loss: 318.3444519042969\n",
      "Epochs: 3041/5000----Loss: 318.34429931640625\n",
      "Epochs: 3042/5000----Loss: 318.34429931640625\n",
      "Epochs: 3043/5000----Loss: 318.3440856933594\n",
      "Epochs: 3044/5000----Loss: 318.343994140625\n",
      "Epochs: 3045/5000----Loss: 318.343994140625\n",
      "Epochs: 3046/5000----Loss: 318.34381103515625\n",
      "Epochs: 3047/5000----Loss: 318.34381103515625\n",
      "Epochs: 3048/5000----Loss: 318.3436584472656\n",
      "Epochs: 3049/5000----Loss: 318.34368896484375\n",
      "Epochs: 3050/5000----Loss: 318.3434753417969\n",
      "Epochs: 3051/5000----Loss: 318.3434143066406\n",
      "Epochs: 3052/5000----Loss: 318.3433532714844\n",
      "Epochs: 3053/5000----Loss: 318.34332275390625\n",
      "Epochs: 3054/5000----Loss: 318.3431701660156\n",
      "Epochs: 3055/5000----Loss: 318.34320068359375\n",
      "Epochs: 3056/5000----Loss: 318.3431091308594\n",
      "Epochs: 3057/5000----Loss: 318.3429260253906\n",
      "Epochs: 3058/5000----Loss: 318.3428955078125\n",
      "Epochs: 3059/5000----Loss: 318.3428039550781\n",
      "Epochs: 3060/5000----Loss: 318.3426818847656\n",
      "Epochs: 3061/5000----Loss: 318.342529296875\n",
      "Epochs: 3062/5000----Loss: 318.3424987792969\n",
      "Epochs: 3063/5000----Loss: 318.3424072265625\n",
      "Epochs: 3064/5000----Loss: 318.34228515625\n",
      "Epochs: 3065/5000----Loss: 318.34228515625\n",
      "Epochs: 3066/5000----Loss: 318.3421325683594\n",
      "Epochs: 3067/5000----Loss: 318.3420715332031\n",
      "Epochs: 3068/5000----Loss: 318.3420715332031\n",
      "Epochs: 3069/5000----Loss: 318.3419494628906\n",
      "Epochs: 3070/5000----Loss: 318.34173583984375\n",
      "Epochs: 3071/5000----Loss: 318.3416748046875\n",
      "Epochs: 3072/5000----Loss: 318.34161376953125\n",
      "Epochs: 3073/5000----Loss: 318.341552734375\n",
      "Epochs: 3074/5000----Loss: 318.3414001464844\n",
      "Epochs: 3075/5000----Loss: 318.3413391113281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3076/5000----Loss: 318.34136962890625\n",
      "Epochs: 3077/5000----Loss: 318.34124755859375\n",
      "Epochs: 3078/5000----Loss: 318.3410949707031\n",
      "Epochs: 3079/5000----Loss: 318.34112548828125\n",
      "Epochs: 3080/5000----Loss: 318.341064453125\n",
      "Epochs: 3081/5000----Loss: 318.3409118652344\n",
      "Epochs: 3082/5000----Loss: 318.3408508300781\n",
      "Epochs: 3083/5000----Loss: 318.34075927734375\n",
      "Epochs: 3084/5000----Loss: 318.3406677246094\n",
      "Epochs: 3085/5000----Loss: 318.34051513671875\n",
      "Epochs: 3086/5000----Loss: 318.3404846191406\n",
      "Epochs: 3087/5000----Loss: 318.3404846191406\n",
      "Epochs: 3088/5000----Loss: 318.34039306640625\n",
      "Epochs: 3089/5000----Loss: 318.34014892578125\n",
      "Epochs: 3090/5000----Loss: 318.34002685546875\n",
      "Epochs: 3091/5000----Loss: 318.33990478515625\n",
      "Epochs: 3092/5000----Loss: 318.33978271484375\n",
      "Epochs: 3093/5000----Loss: 318.33984375\n",
      "Epochs: 3094/5000----Loss: 318.3396911621094\n",
      "Epochs: 3095/5000----Loss: 318.339599609375\n",
      "Epochs: 3096/5000----Loss: 318.3395690917969\n",
      "Epochs: 3097/5000----Loss: 318.3394470214844\n",
      "Epochs: 3098/5000----Loss: 318.33941650390625\n",
      "Epochs: 3099/5000----Loss: 318.3392639160156\n",
      "Epochs: 3100/5000----Loss: 318.3392028808594\n",
      "Epochs: 3101/5000----Loss: 318.33905029296875\n",
      "Epochs: 3102/5000----Loss: 318.33905029296875\n",
      "Epochs: 3103/5000----Loss: 318.3389587402344\n",
      "Epochs: 3104/5000----Loss: 318.3388671875\n",
      "Epochs: 3105/5000----Loss: 318.33880615234375\n",
      "Epochs: 3106/5000----Loss: 318.33868408203125\n",
      "Epochs: 3107/5000----Loss: 318.338623046875\n",
      "Epochs: 3108/5000----Loss: 318.3385009765625\n",
      "Epochs: 3109/5000----Loss: 318.3384704589844\n",
      "Epochs: 3110/5000----Loss: 318.33843994140625\n",
      "Epochs: 3111/5000----Loss: 318.33837890625\n",
      "Epochs: 3112/5000----Loss: 318.338134765625\n",
      "Epochs: 3113/5000----Loss: 318.3382263183594\n",
      "Epochs: 3114/5000----Loss: 318.3381042480469\n",
      "Epochs: 3115/5000----Loss: 318.3379821777344\n",
      "Epochs: 3116/5000----Loss: 318.3378601074219\n",
      "Epochs: 3117/5000----Loss: 318.3377990722656\n",
      "Epochs: 3118/5000----Loss: 318.3377380371094\n",
      "Epochs: 3119/5000----Loss: 318.33770751953125\n",
      "Epochs: 3120/5000----Loss: 318.33758544921875\n",
      "Epochs: 3121/5000----Loss: 318.3374938964844\n",
      "Epochs: 3122/5000----Loss: 318.3374938964844\n",
      "Epochs: 3123/5000----Loss: 318.33740234375\n",
      "Epochs: 3124/5000----Loss: 318.3373107910156\n",
      "Epochs: 3125/5000----Loss: 318.33721923828125\n",
      "Epochs: 3126/5000----Loss: 318.337158203125\n",
      "Epochs: 3127/5000----Loss: 318.33697509765625\n",
      "Epochs: 3128/5000----Loss: 318.3370056152344\n",
      "Epochs: 3129/5000----Loss: 318.3369140625\n",
      "Epochs: 3130/5000----Loss: 318.33685302734375\n",
      "Epochs: 3131/5000----Loss: 318.33673095703125\n",
      "Epochs: 3132/5000----Loss: 318.33660888671875\n",
      "Epochs: 3133/5000----Loss: 318.3365478515625\n",
      "Epochs: 3134/5000----Loss: 318.33648681640625\n",
      "Epochs: 3135/5000----Loss: 318.3363952636719\n",
      "Epochs: 3136/5000----Loss: 318.33636474609375\n",
      "Epochs: 3137/5000----Loss: 318.3363037109375\n",
      "Epochs: 3138/5000----Loss: 318.3361511230469\n",
      "Epochs: 3139/5000----Loss: 318.33612060546875\n",
      "Epochs: 3140/5000----Loss: 318.33612060546875\n",
      "Epochs: 3141/5000----Loss: 318.3358154296875\n",
      "Epochs: 3142/5000----Loss: 318.3357849121094\n",
      "Epochs: 3143/5000----Loss: 318.3356628417969\n",
      "Epochs: 3144/5000----Loss: 318.33551025390625\n",
      "Epochs: 3145/5000----Loss: 318.3355712890625\n",
      "Epochs: 3146/5000----Loss: 318.33551025390625\n",
      "Epochs: 3147/5000----Loss: 318.3354187011719\n",
      "Epochs: 3148/5000----Loss: 318.3352966308594\n",
      "Epochs: 3149/5000----Loss: 318.3352355957031\n",
      "Epochs: 3150/5000----Loss: 318.3351745605469\n",
      "Epochs: 3151/5000----Loss: 318.3350524902344\n",
      "Epochs: 3152/5000----Loss: 318.33502197265625\n",
      "Epochs: 3153/5000----Loss: 318.33489990234375\n",
      "Epochs: 3154/5000----Loss: 318.3348693847656\n",
      "Epochs: 3155/5000----Loss: 318.3347473144531\n",
      "Epochs: 3156/5000----Loss: 318.3347473144531\n",
      "Epochs: 3157/5000----Loss: 318.3345947265625\n",
      "Epochs: 3158/5000----Loss: 318.3345642089844\n",
      "Epochs: 3159/5000----Loss: 318.33441162109375\n",
      "Epochs: 3160/5000----Loss: 318.33453369140625\n",
      "Epochs: 3161/5000----Loss: 318.33441162109375\n",
      "Epochs: 3162/5000----Loss: 318.3343200683594\n",
      "Epochs: 3163/5000----Loss: 318.3342590332031\n",
      "Epochs: 3164/5000----Loss: 318.3341369628906\n",
      "Epochs: 3165/5000----Loss: 318.3340759277344\n",
      "Epochs: 3166/5000----Loss: 318.333984375\n",
      "Epochs: 3167/5000----Loss: 318.3338623046875\n",
      "Epochs: 3168/5000----Loss: 318.3338928222656\n",
      "Epochs: 3169/5000----Loss: 318.3337707519531\n",
      "Epochs: 3170/5000----Loss: 318.333740234375\n",
      "Epochs: 3171/5000----Loss: 318.3335876464844\n",
      "Epochs: 3172/5000----Loss: 318.33355712890625\n",
      "Epochs: 3173/5000----Loss: 318.33343505859375\n",
      "Epochs: 3174/5000----Loss: 318.3334045410156\n",
      "Epochs: 3175/5000----Loss: 318.333251953125\n",
      "Epochs: 3176/5000----Loss: 318.3332824707031\n",
      "Epochs: 3177/5000----Loss: 318.3331298828125\n",
      "Epochs: 3178/5000----Loss: 318.3330993652344\n",
      "Epochs: 3179/5000----Loss: 318.3330383300781\n",
      "Epochs: 3180/5000----Loss: 318.33306884765625\n",
      "Epochs: 3181/5000----Loss: 318.33282470703125\n",
      "Epochs: 3182/5000----Loss: 318.33282470703125\n",
      "Epochs: 3183/5000----Loss: 318.33270263671875\n",
      "Epochs: 3184/5000----Loss: 318.3326110839844\n",
      "Epochs: 3185/5000----Loss: 318.33258056640625\n",
      "Epochs: 3186/5000----Loss: 318.33258056640625\n",
      "Epochs: 3187/5000----Loss: 318.33245849609375\n",
      "Epochs: 3188/5000----Loss: 318.33233642578125\n",
      "Epochs: 3189/5000----Loss: 318.33233642578125\n",
      "Epochs: 3190/5000----Loss: 318.3321533203125\n",
      "Epochs: 3191/5000----Loss: 318.33221435546875\n",
      "Epochs: 3192/5000----Loss: 318.33203125\n",
      "Epochs: 3193/5000----Loss: 318.3319091796875\n",
      "Epochs: 3194/5000----Loss: 318.3318176269531\n",
      "Epochs: 3195/5000----Loss: 318.3318176269531\n",
      "Epochs: 3196/5000----Loss: 318.3316345214844\n",
      "Epochs: 3197/5000----Loss: 318.3315734863281\n",
      "Epochs: 3198/5000----Loss: 318.33154296875\n",
      "Epochs: 3199/5000----Loss: 318.33148193359375\n",
      "Epochs: 3200/5000----Loss: 318.33148193359375\n",
      "Epochs: 3201/5000----Loss: 318.331298828125\n",
      "Epochs: 3202/5000----Loss: 318.3312683105469\n",
      "Epochs: 3203/5000----Loss: 318.33123779296875\n",
      "Epochs: 3204/5000----Loss: 318.33111572265625\n",
      "Epochs: 3205/5000----Loss: 318.33099365234375\n",
      "Epochs: 3206/5000----Loss: 318.3310546875\n",
      "Epochs: 3207/5000----Loss: 318.3309020996094\n",
      "Epochs: 3208/5000----Loss: 318.3309326171875\n",
      "Epochs: 3209/5000----Loss: 318.33087158203125\n",
      "Epochs: 3210/5000----Loss: 318.3307800292969\n",
      "Epochs: 3211/5000----Loss: 318.33056640625\n",
      "Epochs: 3212/5000----Loss: 318.33062744140625\n",
      "Epochs: 3213/5000----Loss: 318.3305358886719\n",
      "Epochs: 3214/5000----Loss: 318.3304443359375\n",
      "Epochs: 3215/5000----Loss: 318.33026123046875\n",
      "Epochs: 3216/5000----Loss: 318.3303527832031\n",
      "Epochs: 3217/5000----Loss: 318.330322265625\n",
      "Epochs: 3218/5000----Loss: 318.3301696777344\n",
      "Epochs: 3219/5000----Loss: 318.33013916015625\n",
      "Epochs: 3220/5000----Loss: 318.33001708984375\n",
      "Epochs: 3221/5000----Loss: 318.3299865722656\n",
      "Epochs: 3222/5000----Loss: 318.32989501953125\n",
      "Epochs: 3223/5000----Loss: 318.32977294921875\n",
      "Epochs: 3224/5000----Loss: 318.32977294921875\n",
      "Epochs: 3225/5000----Loss: 318.32965087890625\n",
      "Epochs: 3226/5000----Loss: 318.3296203613281\n",
      "Epochs: 3227/5000----Loss: 318.32958984375\n",
      "Epochs: 3228/5000----Loss: 318.3294372558594\n",
      "Epochs: 3229/5000----Loss: 318.32940673828125\n",
      "Epochs: 3230/5000----Loss: 318.3293151855469\n",
      "Epochs: 3231/5000----Loss: 318.32928466796875\n",
      "Epochs: 3232/5000----Loss: 318.32916259765625\n",
      "Epochs: 3233/5000----Loss: 318.3291320800781\n",
      "Epochs: 3234/5000----Loss: 318.3291320800781\n",
      "Epochs: 3235/5000----Loss: 318.3290710449219\n",
      "Epochs: 3236/5000----Loss: 318.32891845703125\n",
      "Epochs: 3237/5000----Loss: 318.32879638671875\n",
      "Epochs: 3238/5000----Loss: 318.3288269042969\n",
      "Epochs: 3239/5000----Loss: 318.3287353515625\n",
      "Epochs: 3240/5000----Loss: 318.32867431640625\n",
      "Epochs: 3241/5000----Loss: 318.32861328125\n",
      "Epochs: 3242/5000----Loss: 318.3284912109375\n",
      "Epochs: 3243/5000----Loss: 318.32843017578125\n",
      "Epochs: 3244/5000----Loss: 318.32830810546875\n",
      "Epochs: 3245/5000----Loss: 318.3282165527344\n",
      "Epochs: 3246/5000----Loss: 318.328125\n",
      "Epochs: 3247/5000----Loss: 318.32806396484375\n",
      "Epochs: 3248/5000----Loss: 318.3280944824219\n",
      "Epochs: 3249/5000----Loss: 318.32794189453125\n",
      "Epochs: 3250/5000----Loss: 318.32794189453125\n",
      "Epochs: 3251/5000----Loss: 318.3278503417969\n",
      "Epochs: 3252/5000----Loss: 318.3277282714844\n",
      "Epochs: 3253/5000----Loss: 318.3276672363281\n",
      "Epochs: 3254/5000----Loss: 318.32763671875\n",
      "Epochs: 3255/5000----Loss: 318.3274841308594\n",
      "Epochs: 3256/5000----Loss: 318.3274841308594\n",
      "Epochs: 3257/5000----Loss: 318.3275451660156\n",
      "Epochs: 3258/5000----Loss: 318.3274230957031\n",
      "Epochs: 3259/5000----Loss: 318.32733154296875\n",
      "Epochs: 3260/5000----Loss: 318.3272705078125\n",
      "Epochs: 3261/5000----Loss: 318.3271484375\n",
      "Epochs: 3262/5000----Loss: 318.3271789550781\n",
      "Epochs: 3263/5000----Loss: 318.32696533203125\n",
      "Epochs: 3264/5000----Loss: 318.32696533203125\n",
      "Epochs: 3265/5000----Loss: 318.32696533203125\n",
      "Epochs: 3266/5000----Loss: 318.326904296875\n",
      "Epochs: 3267/5000----Loss: 318.3268127441406\n",
      "Epochs: 3268/5000----Loss: 318.32659912109375\n",
      "Epochs: 3269/5000----Loss: 318.3266906738281\n",
      "Epochs: 3270/5000----Loss: 318.32659912109375\n",
      "Epochs: 3271/5000----Loss: 318.3265075683594\n",
      "Epochs: 3272/5000----Loss: 318.32647705078125\n",
      "Epochs: 3273/5000----Loss: 318.32647705078125\n",
      "Epochs: 3274/5000----Loss: 318.32623291015625\n",
      "Epochs: 3275/5000----Loss: 318.3262939453125\n",
      "Epochs: 3276/5000----Loss: 318.32623291015625\n",
      "Epochs: 3277/5000----Loss: 318.32611083984375\n",
      "Epochs: 3278/5000----Loss: 318.32611083984375\n",
      "Epochs: 3279/5000----Loss: 318.325927734375\n",
      "Epochs: 3280/5000----Loss: 318.32598876953125\n",
      "Epochs: 3281/5000----Loss: 318.32586669921875\n",
      "Epochs: 3282/5000----Loss: 318.32586669921875\n",
      "Epochs: 3283/5000----Loss: 318.3257751464844\n",
      "Epochs: 3284/5000----Loss: 318.32562255859375\n",
      "Epochs: 3285/5000----Loss: 318.32562255859375\n",
      "Epochs: 3286/5000----Loss: 318.3255920410156\n",
      "Epochs: 3287/5000----Loss: 318.3255310058594\n",
      "Epochs: 3288/5000----Loss: 318.325439453125\n",
      "Epochs: 3289/5000----Loss: 318.3253479003906\n",
      "Epochs: 3290/5000----Loss: 318.32537841796875\n",
      "Epochs: 3291/5000----Loss: 318.32525634765625\n",
      "Epochs: 3292/5000----Loss: 318.3252258300781\n",
      "Epochs: 3293/5000----Loss: 318.3251037597656\n",
      "Epochs: 3294/5000----Loss: 318.3249206542969\n",
      "Epochs: 3295/5000----Loss: 318.3249206542969\n",
      "Epochs: 3296/5000----Loss: 318.3248596191406\n",
      "Epochs: 3297/5000----Loss: 318.32476806640625\n",
      "Epochs: 3298/5000----Loss: 318.32470703125\n",
      "Epochs: 3299/5000----Loss: 318.3246765136719\n",
      "Epochs: 3300/5000----Loss: 318.32470703125\n",
      "Epochs: 3301/5000----Loss: 318.32452392578125\n",
      "Epochs: 3302/5000----Loss: 318.3244934082031\n",
      "Epochs: 3303/5000----Loss: 318.3244934082031\n",
      "Epochs: 3304/5000----Loss: 318.3243408203125\n",
      "Epochs: 3305/5000----Loss: 318.3243408203125\n",
      "Epochs: 3306/5000----Loss: 318.32427978515625\n",
      "Epochs: 3307/5000----Loss: 318.32415771484375\n",
      "Epochs: 3308/5000----Loss: 318.3240966796875\n",
      "Epochs: 3309/5000----Loss: 318.32403564453125\n",
      "Epochs: 3310/5000----Loss: 318.32391357421875\n",
      "Epochs: 3311/5000----Loss: 318.3238830566406\n",
      "Epochs: 3312/5000----Loss: 318.3238220214844\n",
      "Epochs: 3313/5000----Loss: 318.3238220214844\n",
      "Epochs: 3314/5000----Loss: 318.3238525390625\n",
      "Epochs: 3315/5000----Loss: 318.3236999511719\n",
      "Epochs: 3316/5000----Loss: 318.3236083984375\n",
      "Epochs: 3317/5000----Loss: 318.3235168457031\n",
      "Epochs: 3318/5000----Loss: 318.3236083984375\n",
      "Epochs: 3319/5000----Loss: 318.32342529296875\n",
      "Epochs: 3320/5000----Loss: 318.3233947753906\n",
      "Epochs: 3321/5000----Loss: 318.32330322265625\n",
      "Epochs: 3322/5000----Loss: 318.3232116699219\n",
      "Epochs: 3323/5000----Loss: 318.32305908203125\n",
      "Epochs: 3324/5000----Loss: 318.3231201171875\n",
      "Epochs: 3325/5000----Loss: 318.32305908203125\n",
      "Epochs: 3326/5000----Loss: 318.322998046875\n",
      "Epochs: 3327/5000----Loss: 318.3229064941406\n",
      "Epochs: 3328/5000----Loss: 318.3228454589844\n",
      "Epochs: 3329/5000----Loss: 318.3228454589844\n",
      "Epochs: 3330/5000----Loss: 318.3228454589844\n",
      "Epochs: 3331/5000----Loss: 318.3227844238281\n",
      "Epochs: 3332/5000----Loss: 318.3226318359375\n",
      "Epochs: 3333/5000----Loss: 318.3226623535156\n",
      "Epochs: 3334/5000----Loss: 318.3226013183594\n",
      "Epochs: 3335/5000----Loss: 318.32244873046875\n",
      "Epochs: 3336/5000----Loss: 318.32232666015625\n",
      "Epochs: 3337/5000----Loss: 318.3224182128906\n",
      "Epochs: 3338/5000----Loss: 318.32232666015625\n",
      "Epochs: 3339/5000----Loss: 318.3222351074219\n",
      "Epochs: 3340/5000----Loss: 318.32220458984375\n",
      "Epochs: 3341/5000----Loss: 318.3221435546875\n",
      "Epochs: 3342/5000----Loss: 318.3219909667969\n",
      "Epochs: 3343/5000----Loss: 318.3220520019531\n",
      "Epochs: 3344/5000----Loss: 318.3218688964844\n",
      "Epochs: 3345/5000----Loss: 318.3218688964844\n",
      "Epochs: 3346/5000----Loss: 318.32177734375\n",
      "Epochs: 3347/5000----Loss: 318.32171630859375\n",
      "Epochs: 3348/5000----Loss: 318.3216552734375\n",
      "Epochs: 3349/5000----Loss: 318.3215026855469\n",
      "Epochs: 3350/5000----Loss: 318.32159423828125\n",
      "Epochs: 3351/5000----Loss: 318.32147216796875\n",
      "Epochs: 3352/5000----Loss: 318.3215026855469\n",
      "Epochs: 3353/5000----Loss: 318.3213195800781\n",
      "Epochs: 3354/5000----Loss: 318.3212585449219\n",
      "Epochs: 3355/5000----Loss: 318.32122802734375\n",
      "Epochs: 3356/5000----Loss: 318.32122802734375\n",
      "Epochs: 3357/5000----Loss: 318.32110595703125\n",
      "Epochs: 3358/5000----Loss: 318.3210754394531\n",
      "Epochs: 3359/5000----Loss: 318.3208923339844\n",
      "Epochs: 3360/5000----Loss: 318.3209228515625\n",
      "Epochs: 3361/5000----Loss: 318.32086181640625\n",
      "Epochs: 3362/5000----Loss: 318.32073974609375\n",
      "Epochs: 3363/5000----Loss: 318.3206787109375\n",
      "Epochs: 3364/5000----Loss: 318.3206787109375\n",
      "Epochs: 3365/5000----Loss: 318.3205871582031\n",
      "Epochs: 3366/5000----Loss: 318.320556640625\n",
      "Epochs: 3367/5000----Loss: 318.3205261230469\n",
      "Epochs: 3368/5000----Loss: 318.32049560546875\n",
      "Epochs: 3369/5000----Loss: 318.32037353515625\n",
      "Epochs: 3370/5000----Loss: 318.3203430175781\n",
      "Epochs: 3371/5000----Loss: 318.32025146484375\n",
      "Epochs: 3372/5000----Loss: 318.3201904296875\n",
      "Epochs: 3373/5000----Loss: 318.32012939453125\n",
      "Epochs: 3374/5000----Loss: 318.32012939453125\n",
      "Epochs: 3375/5000----Loss: 318.320068359375\n",
      "Epochs: 3376/5000----Loss: 318.3199462890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3377/5000----Loss: 318.3199768066406\n",
      "Epochs: 3378/5000----Loss: 318.32000732421875\n",
      "Epochs: 3379/5000----Loss: 318.31982421875\n",
      "Epochs: 3380/5000----Loss: 318.3197937011719\n",
      "Epochs: 3381/5000----Loss: 318.3197021484375\n",
      "Epochs: 3382/5000----Loss: 318.3196716308594\n",
      "Epochs: 3383/5000----Loss: 318.3196105957031\n",
      "Epochs: 3384/5000----Loss: 318.3196105957031\n",
      "Epochs: 3385/5000----Loss: 318.31951904296875\n",
      "Epochs: 3386/5000----Loss: 318.3194885253906\n",
      "Epochs: 3387/5000----Loss: 318.3194274902344\n",
      "Epochs: 3388/5000----Loss: 318.3193664550781\n",
      "Epochs: 3389/5000----Loss: 318.31927490234375\n",
      "Epochs: 3390/5000----Loss: 318.3192138671875\n",
      "Epochs: 3391/5000----Loss: 318.3191223144531\n",
      "Epochs: 3392/5000----Loss: 318.3190612792969\n",
      "Epochs: 3393/5000----Loss: 318.31903076171875\n",
      "Epochs: 3394/5000----Loss: 318.3190002441406\n",
      "Epochs: 3395/5000----Loss: 318.3189392089844\n",
      "Epochs: 3396/5000----Loss: 318.3188781738281\n",
      "Epochs: 3397/5000----Loss: 318.31866455078125\n",
      "Epochs: 3398/5000----Loss: 318.3187561035156\n",
      "Epochs: 3399/5000----Loss: 318.3186950683594\n",
      "Epochs: 3400/5000----Loss: 318.3185119628906\n",
      "Epochs: 3401/5000----Loss: 318.3184814453125\n",
      "Epochs: 3402/5000----Loss: 318.3185119628906\n",
      "Epochs: 3403/5000----Loss: 318.3184509277344\n",
      "Epochs: 3404/5000----Loss: 318.318359375\n",
      "Epochs: 3405/5000----Loss: 318.31829833984375\n",
      "Epochs: 3406/5000----Loss: 318.3182067871094\n",
      "Epochs: 3407/5000----Loss: 318.31817626953125\n",
      "Epochs: 3408/5000----Loss: 318.3182067871094\n",
      "Epochs: 3409/5000----Loss: 318.3180847167969\n",
      "Epochs: 3410/5000----Loss: 318.318115234375\n",
      "Epochs: 3411/5000----Loss: 318.31805419921875\n",
      "Epochs: 3412/5000----Loss: 318.3179626464844\n",
      "Epochs: 3413/5000----Loss: 318.31793212890625\n",
      "Epochs: 3414/5000----Loss: 318.31781005859375\n",
      "Epochs: 3415/5000----Loss: 318.31768798828125\n",
      "Epochs: 3416/5000----Loss: 318.3176574707031\n",
      "Epochs: 3417/5000----Loss: 318.3176574707031\n",
      "Epochs: 3418/5000----Loss: 318.317626953125\n",
      "Epochs: 3419/5000----Loss: 318.3175354003906\n",
      "Epochs: 3420/5000----Loss: 318.31744384765625\n",
      "Epochs: 3421/5000----Loss: 318.3175354003906\n",
      "Epochs: 3422/5000----Loss: 318.31744384765625\n",
      "Epochs: 3423/5000----Loss: 318.3172912597656\n",
      "Epochs: 3424/5000----Loss: 318.3172912597656\n",
      "Epochs: 3425/5000----Loss: 318.3172302246094\n",
      "Epochs: 3426/5000----Loss: 318.31719970703125\n",
      "Epochs: 3427/5000----Loss: 318.31707763671875\n",
      "Epochs: 3428/5000----Loss: 318.317138671875\n",
      "Epochs: 3429/5000----Loss: 318.3169860839844\n",
      "Epochs: 3430/5000----Loss: 318.31695556640625\n",
      "Epochs: 3431/5000----Loss: 318.3169250488281\n",
      "Epochs: 3432/5000----Loss: 318.31689453125\n",
      "Epochs: 3433/5000----Loss: 318.3168029785156\n",
      "Epochs: 3434/5000----Loss: 318.3168029785156\n",
      "Epochs: 3435/5000----Loss: 318.3167419433594\n",
      "Epochs: 3436/5000----Loss: 318.3167419433594\n",
      "Epochs: 3437/5000----Loss: 318.3166198730469\n",
      "Epochs: 3438/5000----Loss: 318.3165283203125\n",
      "Epochs: 3439/5000----Loss: 318.31646728515625\n",
      "Epochs: 3440/5000----Loss: 318.3165283203125\n",
      "Epochs: 3441/5000----Loss: 318.3163757324219\n",
      "Epochs: 3442/5000----Loss: 318.31634521484375\n",
      "Epochs: 3443/5000----Loss: 318.3163146972656\n",
      "Epochs: 3444/5000----Loss: 318.3163146972656\n",
      "Epochs: 3445/5000----Loss: 318.3163146972656\n",
      "Epochs: 3446/5000----Loss: 318.31610107421875\n",
      "Epochs: 3447/5000----Loss: 318.31610107421875\n",
      "Epochs: 3448/5000----Loss: 318.31597900390625\n",
      "Epochs: 3449/5000----Loss: 318.3160095214844\n",
      "Epochs: 3450/5000----Loss: 318.3158264160156\n",
      "Epochs: 3451/5000----Loss: 318.3157958984375\n",
      "Epochs: 3452/5000----Loss: 318.31573486328125\n",
      "Epochs: 3453/5000----Loss: 318.31573486328125\n",
      "Epochs: 3454/5000----Loss: 318.315673828125\n",
      "Epochs: 3455/5000----Loss: 318.3155212402344\n",
      "Epochs: 3456/5000----Loss: 318.31561279296875\n",
      "Epochs: 3457/5000----Loss: 318.31561279296875\n",
      "Epochs: 3458/5000----Loss: 318.31549072265625\n",
      "Epochs: 3459/5000----Loss: 318.31536865234375\n",
      "Epochs: 3460/5000----Loss: 318.3152770996094\n",
      "Epochs: 3461/5000----Loss: 318.3153076171875\n",
      "Epochs: 3462/5000----Loss: 318.31524658203125\n",
      "Epochs: 3463/5000----Loss: 318.315185546875\n",
      "Epochs: 3464/5000----Loss: 318.3152160644531\n",
      "Epochs: 3465/5000----Loss: 318.31512451171875\n",
      "Epochs: 3466/5000----Loss: 318.31500244140625\n",
      "Epochs: 3467/5000----Loss: 318.3150634765625\n",
      "Epochs: 3468/5000----Loss: 318.31494140625\n",
      "Epochs: 3469/5000----Loss: 318.3148498535156\n",
      "Epochs: 3470/5000----Loss: 318.31488037109375\n",
      "Epochs: 3471/5000----Loss: 318.3147888183594\n",
      "Epochs: 3472/5000----Loss: 318.314697265625\n",
      "Epochs: 3473/5000----Loss: 318.3146667480469\n",
      "Epochs: 3474/5000----Loss: 318.3146667480469\n",
      "Epochs: 3475/5000----Loss: 318.3145751953125\n",
      "Epochs: 3476/5000----Loss: 318.31439208984375\n",
      "Epochs: 3477/5000----Loss: 318.31451416015625\n",
      "Epochs: 3478/5000----Loss: 318.3144226074219\n",
      "Epochs: 3479/5000----Loss: 318.31439208984375\n",
      "Epochs: 3480/5000----Loss: 318.3143005371094\n",
      "Epochs: 3481/5000----Loss: 318.31427001953125\n",
      "Epochs: 3482/5000----Loss: 318.314208984375\n",
      "Epochs: 3483/5000----Loss: 318.31427001953125\n",
      "Epochs: 3484/5000----Loss: 318.3141174316406\n",
      "Epochs: 3485/5000----Loss: 318.31402587890625\n",
      "Epochs: 3486/5000----Loss: 318.31414794921875\n",
      "Epochs: 3487/5000----Loss: 318.3140563964844\n",
      "Epochs: 3488/5000----Loss: 318.3139343261719\n",
      "Epochs: 3489/5000----Loss: 318.3139343261719\n",
      "Epochs: 3490/5000----Loss: 318.3138427734375\n",
      "Epochs: 3491/5000----Loss: 318.313720703125\n",
      "Epochs: 3492/5000----Loss: 318.3136901855469\n",
      "Epochs: 3493/5000----Loss: 318.313720703125\n",
      "Epochs: 3494/5000----Loss: 318.313720703125\n",
      "Epochs: 3495/5000----Loss: 318.3136291503906\n",
      "Epochs: 3496/5000----Loss: 318.3136291503906\n",
      "Epochs: 3497/5000----Loss: 318.3134765625\n",
      "Epochs: 3498/5000----Loss: 318.3134460449219\n",
      "Epochs: 3499/5000----Loss: 318.3133850097656\n",
      "Epochs: 3500/5000----Loss: 318.3133544921875\n",
      "Epochs: 3501/5000----Loss: 318.3131408691406\n",
      "Epochs: 3502/5000----Loss: 318.3130798339844\n",
      "Epochs: 3503/5000----Loss: 318.3131103515625\n",
      "Epochs: 3504/5000----Loss: 318.31304931640625\n",
      "Epochs: 3505/5000----Loss: 318.31304931640625\n",
      "Epochs: 3506/5000----Loss: 318.3129577636719\n",
      "Epochs: 3507/5000----Loss: 318.31292724609375\n",
      "Epochs: 3508/5000----Loss: 318.3128662109375\n",
      "Epochs: 3509/5000----Loss: 318.31280517578125\n",
      "Epochs: 3510/5000----Loss: 318.3127746582031\n",
      "Epochs: 3511/5000----Loss: 318.31268310546875\n",
      "Epochs: 3512/5000----Loss: 318.3127746582031\n",
      "Epochs: 3513/5000----Loss: 318.3125915527344\n",
      "Epochs: 3514/5000----Loss: 318.3125\n",
      "Epochs: 3515/5000----Loss: 318.3125305175781\n",
      "Epochs: 3516/5000----Loss: 318.3125\n",
      "Epochs: 3517/5000----Loss: 318.3124694824219\n",
      "Epochs: 3518/5000----Loss: 318.31243896484375\n",
      "Epochs: 3519/5000----Loss: 318.3123474121094\n",
      "Epochs: 3520/5000----Loss: 318.31231689453125\n",
      "Epochs: 3521/5000----Loss: 318.312255859375\n",
      "Epochs: 3522/5000----Loss: 318.31219482421875\n",
      "Epochs: 3523/5000----Loss: 318.3122253417969\n",
      "Epochs: 3524/5000----Loss: 318.31219482421875\n",
      "Epochs: 3525/5000----Loss: 318.3119812011719\n",
      "Epochs: 3526/5000----Loss: 318.31201171875\n",
      "Epochs: 3527/5000----Loss: 318.3119201660156\n",
      "Epochs: 3528/5000----Loss: 318.3118591308594\n",
      "Epochs: 3529/5000----Loss: 318.31182861328125\n",
      "Epochs: 3530/5000----Loss: 318.31195068359375\n",
      "Epochs: 3531/5000----Loss: 318.31182861328125\n",
      "Epochs: 3532/5000----Loss: 318.3118591308594\n",
      "Epochs: 3533/5000----Loss: 318.31170654296875\n",
      "Epochs: 3534/5000----Loss: 318.31158447265625\n",
      "Epochs: 3535/5000----Loss: 318.3116149902344\n",
      "Epochs: 3536/5000----Loss: 318.31158447265625\n",
      "Epochs: 3537/5000----Loss: 318.3114929199219\n",
      "Epochs: 3538/5000----Loss: 318.3114318847656\n",
      "Epochs: 3539/5000----Loss: 318.31134033203125\n",
      "Epochs: 3540/5000----Loss: 318.31134033203125\n",
      "Epochs: 3541/5000----Loss: 318.3112487792969\n",
      "Epochs: 3542/5000----Loss: 318.3113098144531\n",
      "Epochs: 3543/5000----Loss: 318.3113098144531\n",
      "Epochs: 3544/5000----Loss: 318.3112487792969\n",
      "Epochs: 3545/5000----Loss: 318.31121826171875\n",
      "Epochs: 3546/5000----Loss: 318.31097412109375\n",
      "Epochs: 3547/5000----Loss: 318.3109436035156\n",
      "Epochs: 3548/5000----Loss: 318.31097412109375\n",
      "Epochs: 3549/5000----Loss: 318.3108215332031\n",
      "Epochs: 3550/5000----Loss: 318.31085205078125\n",
      "Epochs: 3551/5000----Loss: 318.3108215332031\n",
      "Epochs: 3552/5000----Loss: 318.31072998046875\n",
      "Epochs: 3553/5000----Loss: 318.31072998046875\n",
      "Epochs: 3554/5000----Loss: 318.310546875\n",
      "Epochs: 3555/5000----Loss: 318.3106689453125\n",
      "Epochs: 3556/5000----Loss: 318.31060791015625\n",
      "Epochs: 3557/5000----Loss: 318.31048583984375\n",
      "Epochs: 3558/5000----Loss: 318.3105163574219\n",
      "Epochs: 3559/5000----Loss: 318.3104248046875\n",
      "Epochs: 3560/5000----Loss: 318.3103942871094\n",
      "Epochs: 3561/5000----Loss: 318.31036376953125\n",
      "Epochs: 3562/5000----Loss: 318.3101806640625\n",
      "Epochs: 3563/5000----Loss: 318.31024169921875\n",
      "Epochs: 3564/5000----Loss: 318.310302734375\n",
      "Epochs: 3565/5000----Loss: 318.31011962890625\n",
      "Epochs: 3566/5000----Loss: 318.30999755859375\n",
      "Epochs: 3567/5000----Loss: 318.3100891113281\n",
      "Epochs: 3568/5000----Loss: 318.3099365234375\n",
      "Epochs: 3569/5000----Loss: 318.30999755859375\n",
      "Epochs: 3570/5000----Loss: 318.3099060058594\n",
      "Epochs: 3571/5000----Loss: 318.30987548828125\n",
      "Epochs: 3572/5000----Loss: 318.30975341796875\n",
      "Epochs: 3573/5000----Loss: 318.30975341796875\n",
      "Epochs: 3574/5000----Loss: 318.3097839355469\n",
      "Epochs: 3575/5000----Loss: 318.30963134765625\n",
      "Epochs: 3576/5000----Loss: 318.3096923828125\n",
      "Epochs: 3577/5000----Loss: 318.3096618652344\n",
      "Epochs: 3578/5000----Loss: 318.30950927734375\n",
      "Epochs: 3579/5000----Loss: 318.3094787597656\n",
      "Epochs: 3580/5000----Loss: 318.3094482421875\n",
      "Epochs: 3581/5000----Loss: 318.3094177246094\n",
      "Epochs: 3582/5000----Loss: 318.30938720703125\n",
      "Epochs: 3583/5000----Loss: 318.30938720703125\n",
      "Epochs: 3584/5000----Loss: 318.309326171875\n",
      "Epochs: 3585/5000----Loss: 318.3092041015625\n",
      "Epochs: 3586/5000----Loss: 318.30926513671875\n",
      "Epochs: 3587/5000----Loss: 318.30914306640625\n",
      "Epochs: 3588/5000----Loss: 318.30908203125\n",
      "Epochs: 3589/5000----Loss: 318.30908203125\n",
      "Epochs: 3590/5000----Loss: 318.30902099609375\n",
      "Epochs: 3591/5000----Loss: 318.30902099609375\n",
      "Epochs: 3592/5000----Loss: 318.3089904785156\n",
      "Epochs: 3593/5000----Loss: 318.3087463378906\n",
      "Epochs: 3594/5000----Loss: 318.30877685546875\n",
      "Epochs: 3595/5000----Loss: 318.30865478515625\n",
      "Epochs: 3596/5000----Loss: 318.30865478515625\n",
      "Epochs: 3597/5000----Loss: 318.30859375\n",
      "Epochs: 3598/5000----Loss: 318.3085632324219\n",
      "Epochs: 3599/5000----Loss: 318.30865478515625\n",
      "Epochs: 3600/5000----Loss: 318.3084411621094\n",
      "Epochs: 3601/5000----Loss: 318.3084716796875\n",
      "Epochs: 3602/5000----Loss: 318.3083190917969\n",
      "Epochs: 3603/5000----Loss: 318.3083190917969\n",
      "Epochs: 3604/5000----Loss: 318.3083190917969\n",
      "Epochs: 3605/5000----Loss: 318.3083190917969\n",
      "Epochs: 3606/5000----Loss: 318.30828857421875\n",
      "Epochs: 3607/5000----Loss: 318.3082275390625\n",
      "Epochs: 3608/5000----Loss: 318.30816650390625\n",
      "Epochs: 3609/5000----Loss: 318.3080749511719\n",
      "Epochs: 3610/5000----Loss: 318.3080139160156\n",
      "Epochs: 3611/5000----Loss: 318.30810546875\n",
      "Epochs: 3612/5000----Loss: 318.30792236328125\n",
      "Epochs: 3613/5000----Loss: 318.30792236328125\n",
      "Epochs: 3614/5000----Loss: 318.3078918457031\n",
      "Epochs: 3615/5000----Loss: 318.307861328125\n",
      "Epochs: 3616/5000----Loss: 318.307861328125\n",
      "Epochs: 3617/5000----Loss: 318.30780029296875\n",
      "Epochs: 3618/5000----Loss: 318.30767822265625\n",
      "Epochs: 3619/5000----Loss: 318.3075866699219\n",
      "Epochs: 3620/5000----Loss: 318.3076171875\n",
      "Epochs: 3621/5000----Loss: 318.30767822265625\n",
      "Epochs: 3622/5000----Loss: 318.30743408203125\n",
      "Epochs: 3623/5000----Loss: 318.3074951171875\n",
      "Epochs: 3624/5000----Loss: 318.3075256347656\n",
      "Epochs: 3625/5000----Loss: 318.3074951171875\n",
      "Epochs: 3626/5000----Loss: 318.3074035644531\n",
      "Epochs: 3627/5000----Loss: 318.3072814941406\n",
      "Epochs: 3628/5000----Loss: 318.307373046875\n",
      "Epochs: 3629/5000----Loss: 318.30718994140625\n",
      "Epochs: 3630/5000----Loss: 318.30731201171875\n",
      "Epochs: 3631/5000----Loss: 318.3071594238281\n",
      "Epochs: 3632/5000----Loss: 318.3070983886719\n",
      "Epochs: 3633/5000----Loss: 318.30718994140625\n",
      "Epochs: 3634/5000----Loss: 318.3070068359375\n",
      "Epochs: 3635/5000----Loss: 318.3070983886719\n",
      "Epochs: 3636/5000----Loss: 318.30694580078125\n",
      "Epochs: 3637/5000----Loss: 318.30694580078125\n",
      "Epochs: 3638/5000----Loss: 318.30694580078125\n",
      "Epochs: 3639/5000----Loss: 318.30670166015625\n",
      "Epochs: 3640/5000----Loss: 318.3067321777344\n",
      "Epochs: 3641/5000----Loss: 318.306640625\n",
      "Epochs: 3642/5000----Loss: 318.3066711425781\n",
      "Epochs: 3643/5000----Loss: 318.30657958984375\n",
      "Epochs: 3644/5000----Loss: 318.3064880371094\n",
      "Epochs: 3645/5000----Loss: 318.30657958984375\n",
      "Epochs: 3646/5000----Loss: 318.3065490722656\n",
      "Epochs: 3647/5000----Loss: 318.306396484375\n",
      "Epochs: 3648/5000----Loss: 318.3063049316406\n",
      "Epochs: 3649/5000----Loss: 318.30633544921875\n",
      "Epochs: 3650/5000----Loss: 318.3062438964844\n",
      "Epochs: 3651/5000----Loss: 318.30633544921875\n",
      "Epochs: 3652/5000----Loss: 318.30621337890625\n",
      "Epochs: 3653/5000----Loss: 318.3061828613281\n",
      "Epochs: 3654/5000----Loss: 318.30609130859375\n",
      "Epochs: 3655/5000----Loss: 318.3062438964844\n",
      "Epochs: 3656/5000----Loss: 318.305908203125\n",
      "Epochs: 3657/5000----Loss: 318.30609130859375\n",
      "Epochs: 3658/5000----Loss: 318.3060302734375\n",
      "Epochs: 3659/5000----Loss: 318.30596923828125\n",
      "Epochs: 3660/5000----Loss: 318.3058166503906\n",
      "Epochs: 3661/5000----Loss: 318.30584716796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3662/5000----Loss: 318.30572509765625\n",
      "Epochs: 3663/5000----Loss: 318.3057861328125\n",
      "Epochs: 3664/5000----Loss: 318.30572509765625\n",
      "Epochs: 3665/5000----Loss: 318.3056335449219\n",
      "Epochs: 3666/5000----Loss: 318.30560302734375\n",
      "Epochs: 3667/5000----Loss: 318.3056640625\n",
      "Epochs: 3668/5000----Loss: 318.30560302734375\n",
      "Epochs: 3669/5000----Loss: 318.3055725097656\n",
      "Epochs: 3670/5000----Loss: 318.3054504394531\n",
      "Epochs: 3671/5000----Loss: 318.30548095703125\n",
      "Epochs: 3672/5000----Loss: 318.30535888671875\n",
      "Epochs: 3673/5000----Loss: 318.3053894042969\n",
      "Epochs: 3674/5000----Loss: 318.30535888671875\n",
      "Epochs: 3675/5000----Loss: 318.30523681640625\n",
      "Epochs: 3676/5000----Loss: 318.3051452636719\n",
      "Epochs: 3677/5000----Loss: 318.30523681640625\n",
      "Epochs: 3678/5000----Loss: 318.3051452636719\n",
      "Epochs: 3679/5000----Loss: 318.30511474609375\n",
      "Epochs: 3680/5000----Loss: 318.30511474609375\n",
      "Epochs: 3681/5000----Loss: 318.3050842285156\n",
      "Epochs: 3682/5000----Loss: 318.3049621582031\n",
      "Epochs: 3683/5000----Loss: 318.30499267578125\n",
      "Epochs: 3684/5000----Loss: 318.30487060546875\n",
      "Epochs: 3685/5000----Loss: 318.3049011230469\n",
      "Epochs: 3686/5000----Loss: 318.3046875\n",
      "Epochs: 3687/5000----Loss: 318.3047180175781\n",
      "Epochs: 3688/5000----Loss: 318.3046875\n",
      "Epochs: 3689/5000----Loss: 318.30462646484375\n",
      "Epochs: 3690/5000----Loss: 318.3045959472656\n",
      "Epochs: 3691/5000----Loss: 318.3045959472656\n",
      "Epochs: 3692/5000----Loss: 318.3044738769531\n",
      "Epochs: 3693/5000----Loss: 318.3045654296875\n",
      "Epochs: 3694/5000----Loss: 318.304443359375\n",
      "Epochs: 3695/5000----Loss: 318.30438232421875\n",
      "Epochs: 3696/5000----Loss: 318.3042907714844\n",
      "Epochs: 3697/5000----Loss: 318.3043518066406\n",
      "Epochs: 3698/5000----Loss: 318.3043212890625\n",
      "Epochs: 3699/5000----Loss: 318.30426025390625\n",
      "Epochs: 3700/5000----Loss: 318.30413818359375\n",
      "Epochs: 3701/5000----Loss: 318.3041687011719\n",
      "Epochs: 3702/5000----Loss: 318.30413818359375\n",
      "Epochs: 3703/5000----Loss: 318.30413818359375\n",
      "Epochs: 3704/5000----Loss: 318.30401611328125\n",
      "Epochs: 3705/5000----Loss: 318.30401611328125\n",
      "Epochs: 3706/5000----Loss: 318.30401611328125\n",
      "Epochs: 3707/5000----Loss: 318.3038635253906\n",
      "Epochs: 3708/5000----Loss: 318.3038635253906\n",
      "Epochs: 3709/5000----Loss: 318.3038024902344\n",
      "Epochs: 3710/5000----Loss: 318.3036804199219\n",
      "Epochs: 3711/5000----Loss: 318.3038330078125\n",
      "Epochs: 3712/5000----Loss: 318.30364990234375\n",
      "Epochs: 3713/5000----Loss: 318.3036804199219\n",
      "Epochs: 3714/5000----Loss: 318.30364990234375\n",
      "Epochs: 3715/5000----Loss: 318.30364990234375\n",
      "Epochs: 3716/5000----Loss: 318.303466796875\n",
      "Epochs: 3717/5000----Loss: 318.3034973144531\n",
      "Epochs: 3718/5000----Loss: 318.303466796875\n",
      "Epochs: 3719/5000----Loss: 318.303466796875\n",
      "Epochs: 3720/5000----Loss: 318.3033752441406\n",
      "Epochs: 3721/5000----Loss: 318.3033752441406\n",
      "Epochs: 3722/5000----Loss: 318.3033142089844\n",
      "Epochs: 3723/5000----Loss: 318.30328369140625\n",
      "Epochs: 3724/5000----Loss: 318.3031921386719\n",
      "Epochs: 3725/5000----Loss: 318.30328369140625\n",
      "Epochs: 3726/5000----Loss: 318.30316162109375\n",
      "Epochs: 3727/5000----Loss: 318.3031921386719\n",
      "Epochs: 3728/5000----Loss: 318.30316162109375\n",
      "Epochs: 3729/5000----Loss: 318.302978515625\n",
      "Epochs: 3730/5000----Loss: 318.30303955078125\n",
      "Epochs: 3731/5000----Loss: 318.3030090332031\n",
      "Epochs: 3732/5000----Loss: 318.3028869628906\n",
      "Epochs: 3733/5000----Loss: 318.30279541015625\n",
      "Epochs: 3734/5000----Loss: 318.30279541015625\n",
      "Epochs: 3735/5000----Loss: 318.30279541015625\n",
      "Epochs: 3736/5000----Loss: 318.3025817871094\n",
      "Epochs: 3737/5000----Loss: 318.3026123046875\n",
      "Epochs: 3738/5000----Loss: 318.30267333984375\n",
      "Epochs: 3739/5000----Loss: 318.3026428222656\n",
      "Epochs: 3740/5000----Loss: 318.30242919921875\n",
      "Epochs: 3741/5000----Loss: 318.3025207519531\n",
      "Epochs: 3742/5000----Loss: 318.3024597167969\n",
      "Epochs: 3743/5000----Loss: 318.30242919921875\n",
      "Epochs: 3744/5000----Loss: 318.3023986816406\n",
      "Epochs: 3745/5000----Loss: 318.3023376464844\n",
      "Epochs: 3746/5000----Loss: 318.30230712890625\n",
      "Epochs: 3747/5000----Loss: 318.30224609375\n",
      "Epochs: 3748/5000----Loss: 318.30230712890625\n",
      "Epochs: 3749/5000----Loss: 318.3021545410156\n",
      "Epochs: 3750/5000----Loss: 318.3022766113281\n",
      "Epochs: 3751/5000----Loss: 318.3021240234375\n",
      "Epochs: 3752/5000----Loss: 318.3021240234375\n",
      "Epochs: 3753/5000----Loss: 318.302001953125\n",
      "Epochs: 3754/5000----Loss: 318.3019714355469\n",
      "Epochs: 3755/5000----Loss: 318.30194091796875\n",
      "Epochs: 3756/5000----Loss: 318.3020324707031\n",
      "Epochs: 3757/5000----Loss: 318.30194091796875\n",
      "Epochs: 3758/5000----Loss: 318.3019104003906\n",
      "Epochs: 3759/5000----Loss: 318.3019104003906\n",
      "Epochs: 3760/5000----Loss: 318.3017272949219\n",
      "Epochs: 3761/5000----Loss: 318.30169677734375\n",
      "Epochs: 3762/5000----Loss: 318.3016357421875\n",
      "Epochs: 3763/5000----Loss: 318.3017272949219\n",
      "Epochs: 3764/5000----Loss: 318.30169677734375\n",
      "Epochs: 3765/5000----Loss: 318.3016052246094\n",
      "Epochs: 3766/5000----Loss: 318.30157470703125\n",
      "Epochs: 3767/5000----Loss: 318.30157470703125\n",
      "Epochs: 3768/5000----Loss: 318.301513671875\n",
      "Epochs: 3769/5000----Loss: 318.30145263671875\n",
      "Epochs: 3770/5000----Loss: 318.30145263671875\n",
      "Epochs: 3771/5000----Loss: 318.3013610839844\n",
      "Epochs: 3772/5000----Loss: 318.30133056640625\n",
      "Epochs: 3773/5000----Loss: 318.30133056640625\n",
      "Epochs: 3774/5000----Loss: 318.3013000488281\n",
      "Epochs: 3775/5000----Loss: 318.3011779785156\n",
      "Epochs: 3776/5000----Loss: 318.30120849609375\n",
      "Epochs: 3777/5000----Loss: 318.3011779785156\n",
      "Epochs: 3778/5000----Loss: 318.3011474609375\n",
      "Epochs: 3779/5000----Loss: 318.3009948730469\n",
      "Epochs: 3780/5000----Loss: 318.30084228515625\n",
      "Epochs: 3781/5000----Loss: 318.30096435546875\n",
      "Epochs: 3782/5000----Loss: 318.3009338378906\n",
      "Epochs: 3783/5000----Loss: 318.30072021484375\n",
      "Epochs: 3784/5000----Loss: 318.3007507324219\n",
      "Epochs: 3785/5000----Loss: 318.30084228515625\n",
      "Epochs: 3786/5000----Loss: 318.3008117675781\n",
      "Epochs: 3787/5000----Loss: 318.3006896972656\n",
      "Epochs: 3788/5000----Loss: 318.30072021484375\n",
      "Epochs: 3789/5000----Loss: 318.30059814453125\n",
      "Epochs: 3790/5000----Loss: 318.3006286621094\n",
      "Epochs: 3791/5000----Loss: 318.300537109375\n",
      "Epochs: 3792/5000----Loss: 318.3004150390625\n",
      "Epochs: 3793/5000----Loss: 318.30047607421875\n",
      "Epochs: 3794/5000----Loss: 318.30047607421875\n",
      "Epochs: 3795/5000----Loss: 318.3003845214844\n",
      "Epochs: 3796/5000----Loss: 318.3004455566406\n",
      "Epochs: 3797/5000----Loss: 318.30035400390625\n",
      "Epochs: 3798/5000----Loss: 318.30029296875\n",
      "Epochs: 3799/5000----Loss: 318.3002014160156\n",
      "Epochs: 3800/5000----Loss: 318.30029296875\n",
      "Epochs: 3801/5000----Loss: 318.30010986328125\n",
      "Epochs: 3802/5000----Loss: 318.3001708984375\n",
      "Epochs: 3803/5000----Loss: 318.3001708984375\n",
      "Epochs: 3804/5000----Loss: 318.3000793457031\n",
      "Epochs: 3805/5000----Loss: 318.30010986328125\n",
      "Epochs: 3806/5000----Loss: 318.2999267578125\n",
      "Epochs: 3807/5000----Loss: 318.29998779296875\n",
      "Epochs: 3808/5000----Loss: 318.2999267578125\n",
      "Epochs: 3809/5000----Loss: 318.2998962402344\n",
      "Epochs: 3810/5000----Loss: 318.2998352050781\n",
      "Epochs: 3811/5000----Loss: 318.2998046875\n",
      "Epochs: 3812/5000----Loss: 318.2998046875\n",
      "Epochs: 3813/5000----Loss: 318.29974365234375\n",
      "Epochs: 3814/5000----Loss: 318.29974365234375\n",
      "Epochs: 3815/5000----Loss: 318.2996520996094\n",
      "Epochs: 3816/5000----Loss: 318.2996520996094\n",
      "Epochs: 3817/5000----Loss: 318.2996520996094\n",
      "Epochs: 3818/5000----Loss: 318.2995910644531\n",
      "Epochs: 3819/5000----Loss: 318.29949951171875\n",
      "Epochs: 3820/5000----Loss: 318.29949951171875\n",
      "Epochs: 3821/5000----Loss: 318.29949951171875\n",
      "Epochs: 3822/5000----Loss: 318.2995300292969\n",
      "Epochs: 3823/5000----Loss: 318.29937744140625\n",
      "Epochs: 3824/5000----Loss: 318.29937744140625\n",
      "Epochs: 3825/5000----Loss: 318.29937744140625\n",
      "Epochs: 3826/5000----Loss: 318.29925537109375\n",
      "Epochs: 3827/5000----Loss: 318.29913330078125\n",
      "Epochs: 3828/5000----Loss: 318.29913330078125\n",
      "Epochs: 3829/5000----Loss: 318.2991638183594\n",
      "Epochs: 3830/5000----Loss: 318.2990417480469\n",
      "Epochs: 3831/5000----Loss: 318.29901123046875\n",
      "Epochs: 3832/5000----Loss: 318.2989807128906\n",
      "Epochs: 3833/5000----Loss: 318.29888916015625\n",
      "Epochs: 3834/5000----Loss: 318.29901123046875\n",
      "Epochs: 3835/5000----Loss: 318.29901123046875\n",
      "Epochs: 3836/5000----Loss: 318.29876708984375\n",
      "Epochs: 3837/5000----Loss: 318.2989501953125\n",
      "Epochs: 3838/5000----Loss: 318.29888916015625\n",
      "Epochs: 3839/5000----Loss: 318.2987365722656\n",
      "Epochs: 3840/5000----Loss: 318.2986755371094\n",
      "Epochs: 3841/5000----Loss: 318.29864501953125\n",
      "Epochs: 3842/5000----Loss: 318.298583984375\n",
      "Epochs: 3843/5000----Loss: 318.2986145019531\n",
      "Epochs: 3844/5000----Loss: 318.2985534667969\n",
      "Epochs: 3845/5000----Loss: 318.298583984375\n",
      "Epochs: 3846/5000----Loss: 318.29852294921875\n",
      "Epochs: 3847/5000----Loss: 318.29852294921875\n",
      "Epochs: 3848/5000----Loss: 318.29840087890625\n",
      "Epochs: 3849/5000----Loss: 318.2983703613281\n",
      "Epochs: 3850/5000----Loss: 318.29840087890625\n",
      "Epochs: 3851/5000----Loss: 318.29840087890625\n",
      "Epochs: 3852/5000----Loss: 318.2983093261719\n",
      "Epochs: 3853/5000----Loss: 318.2983703613281\n",
      "Epochs: 3854/5000----Loss: 318.2982177734375\n",
      "Epochs: 3855/5000----Loss: 318.2982482910156\n",
      "Epochs: 3856/5000----Loss: 318.29815673828125\n",
      "Epochs: 3857/5000----Loss: 318.2981262207031\n",
      "Epochs: 3858/5000----Loss: 318.29803466796875\n",
      "Epochs: 3859/5000----Loss: 318.29803466796875\n",
      "Epochs: 3860/5000----Loss: 318.2980041503906\n",
      "Epochs: 3861/5000----Loss: 318.29791259765625\n",
      "Epochs: 3862/5000----Loss: 318.2979736328125\n",
      "Epochs: 3863/5000----Loss: 318.29791259765625\n",
      "Epochs: 3864/5000----Loss: 318.2978210449219\n",
      "Epochs: 3865/5000----Loss: 318.29791259765625\n",
      "Epochs: 3866/5000----Loss: 318.2978820800781\n",
      "Epochs: 3867/5000----Loss: 318.2978515625\n",
      "Epochs: 3868/5000----Loss: 318.29766845703125\n",
      "Epochs: 3869/5000----Loss: 318.2977600097656\n",
      "Epochs: 3870/5000----Loss: 318.2976989746094\n",
      "Epochs: 3871/5000----Loss: 318.29766845703125\n",
      "Epochs: 3872/5000----Loss: 318.29754638671875\n",
      "Epochs: 3873/5000----Loss: 318.2973937988281\n",
      "Epochs: 3874/5000----Loss: 318.29742431640625\n",
      "Epochs: 3875/5000----Loss: 318.29736328125\n",
      "Epochs: 3876/5000----Loss: 318.2974853515625\n",
      "Epochs: 3877/5000----Loss: 318.29742431640625\n",
      "Epochs: 3878/5000----Loss: 318.29742431640625\n",
      "Epochs: 3879/5000----Loss: 318.2972717285156\n",
      "Epochs: 3880/5000----Loss: 318.29730224609375\n",
      "Epochs: 3881/5000----Loss: 318.2971496582031\n",
      "Epochs: 3882/5000----Loss: 318.29718017578125\n",
      "Epochs: 3883/5000----Loss: 318.2972717285156\n",
      "Epochs: 3884/5000----Loss: 318.29705810546875\n",
      "Epochs: 3885/5000----Loss: 318.297119140625\n",
      "Epochs: 3886/5000----Loss: 318.29705810546875\n",
      "Epochs: 3887/5000----Loss: 318.2970275878906\n",
      "Epochs: 3888/5000----Loss: 318.29693603515625\n",
      "Epochs: 3889/5000----Loss: 318.29693603515625\n",
      "Epochs: 3890/5000----Loss: 318.29693603515625\n",
      "Epochs: 3891/5000----Loss: 318.296875\n",
      "Epochs: 3892/5000----Loss: 318.296875\n",
      "Epochs: 3893/5000----Loss: 318.2967529296875\n",
      "Epochs: 3894/5000----Loss: 318.2967529296875\n",
      "Epochs: 3895/5000----Loss: 318.296875\n",
      "Epochs: 3896/5000----Loss: 318.29669189453125\n",
      "Epochs: 3897/5000----Loss: 318.29669189453125\n",
      "Epochs: 3898/5000----Loss: 318.296630859375\n",
      "Epochs: 3899/5000----Loss: 318.2966003417969\n",
      "Epochs: 3900/5000----Loss: 318.2965393066406\n",
      "Epochs: 3901/5000----Loss: 318.2965393066406\n",
      "Epochs: 3902/5000----Loss: 318.29656982421875\n",
      "Epochs: 3903/5000----Loss: 318.2964782714844\n",
      "Epochs: 3904/5000----Loss: 318.29644775390625\n",
      "Epochs: 3905/5000----Loss: 318.29644775390625\n",
      "Epochs: 3906/5000----Loss: 318.2964172363281\n",
      "Epochs: 3907/5000----Loss: 318.2963562011719\n",
      "Epochs: 3908/5000----Loss: 318.2962646484375\n",
      "Epochs: 3909/5000----Loss: 318.2962646484375\n",
      "Epochs: 3910/5000----Loss: 318.29620361328125\n",
      "Epochs: 3911/5000----Loss: 318.29620361328125\n",
      "Epochs: 3912/5000----Loss: 318.29620361328125\n",
      "Epochs: 3913/5000----Loss: 318.2961120605469\n",
      "Epochs: 3914/5000----Loss: 318.296142578125\n",
      "Epochs: 3915/5000----Loss: 318.29608154296875\n",
      "Epochs: 3916/5000----Loss: 318.2961120605469\n",
      "Epochs: 3917/5000----Loss: 318.2959899902344\n",
      "Epochs: 3918/5000----Loss: 318.2960510253906\n",
      "Epochs: 3919/5000----Loss: 318.2959289550781\n",
      "Epochs: 3920/5000----Loss: 318.2958068847656\n",
      "Epochs: 3921/5000----Loss: 318.2957763671875\n",
      "Epochs: 3922/5000----Loss: 318.29571533203125\n",
      "Epochs: 3923/5000----Loss: 318.2957458496094\n",
      "Epochs: 3924/5000----Loss: 318.29571533203125\n",
      "Epochs: 3925/5000----Loss: 318.2957458496094\n",
      "Epochs: 3926/5000----Loss: 318.29571533203125\n",
      "Epochs: 3927/5000----Loss: 318.295654296875\n",
      "Epochs: 3928/5000----Loss: 318.2955627441406\n",
      "Epochs: 3929/5000----Loss: 318.2955627441406\n",
      "Epochs: 3930/5000----Loss: 318.2954406738281\n",
      "Epochs: 3931/5000----Loss: 318.2955017089844\n",
      "Epochs: 3932/5000----Loss: 318.29547119140625\n",
      "Epochs: 3933/5000----Loss: 318.29541015625\n",
      "Epochs: 3934/5000----Loss: 318.2954406738281\n",
      "Epochs: 3935/5000----Loss: 318.2952880859375\n",
      "Epochs: 3936/5000----Loss: 318.29534912109375\n",
      "Epochs: 3937/5000----Loss: 318.29534912109375\n",
      "Epochs: 3938/5000----Loss: 318.2953186035156\n",
      "Epochs: 3939/5000----Loss: 318.2952575683594\n",
      "Epochs: 3940/5000----Loss: 318.2952575683594\n",
      "Epochs: 3941/5000----Loss: 318.29510498046875\n",
      "Epochs: 3942/5000----Loss: 318.29510498046875\n",
      "Epochs: 3943/5000----Loss: 318.29510498046875\n",
      "Epochs: 3944/5000----Loss: 318.2950439453125\n",
      "Epochs: 3945/5000----Loss: 318.2950744628906\n",
      "Epochs: 3946/5000----Loss: 318.2950439453125\n",
      "Epochs: 3947/5000----Loss: 318.2949523925781\n",
      "Epochs: 3948/5000----Loss: 318.2948913574219\n",
      "Epochs: 3949/5000----Loss: 318.29498291015625\n",
      "Epochs: 3950/5000----Loss: 318.2947998046875\n",
      "Epochs: 3951/5000----Loss: 318.2947998046875\n",
      "Epochs: 3952/5000----Loss: 318.29473876953125\n",
      "Epochs: 3953/5000----Loss: 318.2948303222656\n",
      "Epochs: 3954/5000----Loss: 318.29486083984375\n",
      "Epochs: 3955/5000----Loss: 318.294677734375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3956/5000----Loss: 318.29473876953125\n",
      "Epochs: 3957/5000----Loss: 318.2947082519531\n",
      "Epochs: 3958/5000----Loss: 318.29461669921875\n",
      "Epochs: 3959/5000----Loss: 318.2945556640625\n",
      "Epochs: 3960/5000----Loss: 318.2945861816406\n",
      "Epochs: 3961/5000----Loss: 318.2945251464844\n",
      "Epochs: 3962/5000----Loss: 318.2945251464844\n",
      "Epochs: 3963/5000----Loss: 318.29437255859375\n",
      "Epochs: 3964/5000----Loss: 318.29443359375\n",
      "Epochs: 3965/5000----Loss: 318.2943420410156\n",
      "Epochs: 3966/5000----Loss: 318.2942810058594\n",
      "Epochs: 3967/5000----Loss: 318.2941589355469\n",
      "Epochs: 3968/5000----Loss: 318.29412841796875\n",
      "Epochs: 3969/5000----Loss: 318.2942199707031\n",
      "Epochs: 3970/5000----Loss: 318.2940979003906\n",
      "Epochs: 3971/5000----Loss: 318.29412841796875\n",
      "Epochs: 3972/5000----Loss: 318.29412841796875\n",
      "Epochs: 3973/5000----Loss: 318.29400634765625\n",
      "Epochs: 3974/5000----Loss: 318.29400634765625\n",
      "Epochs: 3975/5000----Loss: 318.2940368652344\n",
      "Epochs: 3976/5000----Loss: 318.2939758300781\n",
      "Epochs: 3977/5000----Loss: 318.29388427734375\n",
      "Epochs: 3978/5000----Loss: 318.2939147949219\n",
      "Epochs: 3979/5000----Loss: 318.29376220703125\n",
      "Epochs: 3980/5000----Loss: 318.2938537597656\n",
      "Epochs: 3981/5000----Loss: 318.2937927246094\n",
      "Epochs: 3982/5000----Loss: 318.29376220703125\n",
      "Epochs: 3983/5000----Loss: 318.29376220703125\n",
      "Epochs: 3984/5000----Loss: 318.2937316894531\n",
      "Epochs: 3985/5000----Loss: 318.2935791015625\n",
      "Epochs: 3986/5000----Loss: 318.2936706542969\n",
      "Epochs: 3987/5000----Loss: 318.293701171875\n",
      "Epochs: 3988/5000----Loss: 318.2936096191406\n",
      "Epochs: 3989/5000----Loss: 318.2936096191406\n",
      "Epochs: 3990/5000----Loss: 318.2935791015625\n",
      "Epochs: 3991/5000----Loss: 318.29345703125\n",
      "Epochs: 3992/5000----Loss: 318.29351806640625\n",
      "Epochs: 3993/5000----Loss: 318.29339599609375\n",
      "Epochs: 3994/5000----Loss: 318.29345703125\n",
      "Epochs: 3995/5000----Loss: 318.29327392578125\n",
      "Epochs: 3996/5000----Loss: 318.2933349609375\n",
      "Epochs: 3997/5000----Loss: 318.2933044433594\n",
      "Epochs: 3998/5000----Loss: 318.2933044433594\n",
      "Epochs: 3999/5000----Loss: 318.29327392578125\n",
      "Epochs: 4000/5000----Loss: 318.29327392578125\n",
      "Epochs: 4001/5000----Loss: 318.29315185546875\n",
      "Epochs: 4002/5000----Loss: 318.293212890625\n",
      "Epochs: 4003/5000----Loss: 318.2930603027344\n",
      "Epochs: 4004/5000----Loss: 318.2930603027344\n",
      "Epochs: 4005/5000----Loss: 318.2930603027344\n",
      "Epochs: 4006/5000----Loss: 318.2930908203125\n",
      "Epochs: 4007/5000----Loss: 318.2929992675781\n",
      "Epochs: 4008/5000----Loss: 318.29302978515625\n",
      "Epochs: 4009/5000----Loss: 318.2928771972656\n",
      "Epochs: 4010/5000----Loss: 318.29290771484375\n",
      "Epochs: 4011/5000----Loss: 318.29290771484375\n",
      "Epochs: 4012/5000----Loss: 318.2926940917969\n",
      "Epochs: 4013/5000----Loss: 318.29266357421875\n",
      "Epochs: 4014/5000----Loss: 318.29266357421875\n",
      "Epochs: 4015/5000----Loss: 318.29266357421875\n",
      "Epochs: 4016/5000----Loss: 318.29254150390625\n",
      "Epochs: 4017/5000----Loss: 318.2926330566406\n",
      "Epochs: 4018/5000----Loss: 318.2926330566406\n",
      "Epochs: 4019/5000----Loss: 318.29254150390625\n",
      "Epochs: 4020/5000----Loss: 318.29241943359375\n",
      "Epochs: 4021/5000----Loss: 318.29254150390625\n",
      "Epochs: 4022/5000----Loss: 318.2924499511719\n",
      "Epochs: 4023/5000----Loss: 318.2924499511719\n",
      "Epochs: 4024/5000----Loss: 318.29241943359375\n",
      "Epochs: 4025/5000----Loss: 318.29229736328125\n",
      "Epochs: 4026/5000----Loss: 318.29229736328125\n",
      "Epochs: 4027/5000----Loss: 318.2922668457031\n",
      "Epochs: 4028/5000----Loss: 318.29229736328125\n",
      "Epochs: 4029/5000----Loss: 318.2922668457031\n",
      "Epochs: 4030/5000----Loss: 318.29217529296875\n",
      "Epochs: 4031/5000----Loss: 318.29217529296875\n",
      "Epochs: 4032/5000----Loss: 318.29217529296875\n",
      "Epochs: 4033/5000----Loss: 318.2920837402344\n",
      "Epochs: 4034/5000----Loss: 318.2920837402344\n",
      "Epochs: 4035/5000----Loss: 318.29205322265625\n",
      "Epochs: 4036/5000----Loss: 318.2921142578125\n",
      "Epochs: 4037/5000----Loss: 318.2919921875\n",
      "Epochs: 4038/5000----Loss: 318.2919006347656\n",
      "Epochs: 4039/5000----Loss: 318.2919006347656\n",
      "Epochs: 4040/5000----Loss: 318.2919006347656\n",
      "Epochs: 4041/5000----Loss: 318.2918701171875\n",
      "Epochs: 4042/5000----Loss: 318.29180908203125\n",
      "Epochs: 4043/5000----Loss: 318.291748046875\n",
      "Epochs: 4044/5000----Loss: 318.2917785644531\n",
      "Epochs: 4045/5000----Loss: 318.29168701171875\n",
      "Epochs: 4046/5000----Loss: 318.291748046875\n",
      "Epochs: 4047/5000----Loss: 318.291748046875\n",
      "Epochs: 4048/5000----Loss: 318.29156494140625\n",
      "Epochs: 4049/5000----Loss: 318.2915954589844\n",
      "Epochs: 4050/5000----Loss: 318.29156494140625\n",
      "Epochs: 4051/5000----Loss: 318.29150390625\n",
      "Epochs: 4052/5000----Loss: 318.2915954589844\n",
      "Epochs: 4053/5000----Loss: 318.2916259765625\n",
      "Epochs: 4054/5000----Loss: 318.2913818359375\n",
      "Epochs: 4055/5000----Loss: 318.2914123535156\n",
      "Epochs: 4056/5000----Loss: 318.29132080078125\n",
      "Epochs: 4057/5000----Loss: 318.29132080078125\n",
      "Epochs: 4058/5000----Loss: 318.2913513183594\n",
      "Epochs: 4059/5000----Loss: 318.29119873046875\n",
      "Epochs: 4060/5000----Loss: 318.29119873046875\n",
      "Epochs: 4061/5000----Loss: 318.2911376953125\n",
      "Epochs: 4062/5000----Loss: 318.2911071777344\n",
      "Epochs: 4063/5000----Loss: 318.2911682128906\n",
      "Epochs: 4064/5000----Loss: 318.29107666015625\n",
      "Epochs: 4065/5000----Loss: 318.29107666015625\n",
      "Epochs: 4066/5000----Loss: 318.29107666015625\n",
      "Epochs: 4067/5000----Loss: 318.291015625\n",
      "Epochs: 4068/5000----Loss: 318.2909240722656\n",
      "Epochs: 4069/5000----Loss: 318.2910461425781\n",
      "Epochs: 4070/5000----Loss: 318.2908935546875\n",
      "Epochs: 4071/5000----Loss: 318.29083251953125\n",
      "Epochs: 4072/5000----Loss: 318.2908020019531\n",
      "Epochs: 4073/5000----Loss: 318.29083251953125\n",
      "Epochs: 4074/5000----Loss: 318.2907409667969\n",
      "Epochs: 4075/5000----Loss: 318.29071044921875\n",
      "Epochs: 4076/5000----Loss: 318.29071044921875\n",
      "Epochs: 4077/5000----Loss: 318.29071044921875\n",
      "Epochs: 4078/5000----Loss: 318.2907409667969\n",
      "Epochs: 4079/5000----Loss: 318.2906799316406\n",
      "Epochs: 4080/5000----Loss: 318.29058837890625\n",
      "Epochs: 4081/5000----Loss: 318.29058837890625\n",
      "Epochs: 4082/5000----Loss: 318.29058837890625\n",
      "Epochs: 4083/5000----Loss: 318.2904357910156\n",
      "Epochs: 4084/5000----Loss: 318.2904357910156\n",
      "Epochs: 4085/5000----Loss: 318.2904052734375\n",
      "Epochs: 4086/5000----Loss: 318.29046630859375\n",
      "Epochs: 4087/5000----Loss: 318.29046630859375\n",
      "Epochs: 4088/5000----Loss: 318.2904052734375\n",
      "Epochs: 4089/5000----Loss: 318.29034423828125\n",
      "Epochs: 4090/5000----Loss: 318.29034423828125\n",
      "Epochs: 4091/5000----Loss: 318.290283203125\n",
      "Epochs: 4092/5000----Loss: 318.29022216796875\n",
      "Epochs: 4093/5000----Loss: 318.290283203125\n",
      "Epochs: 4094/5000----Loss: 318.29022216796875\n",
      "Epochs: 4095/5000----Loss: 318.29010009765625\n",
      "Epochs: 4096/5000----Loss: 318.2900695800781\n",
      "Epochs: 4097/5000----Loss: 318.2901306152344\n",
      "Epochs: 4098/5000----Loss: 318.29010009765625\n",
      "Epochs: 4099/5000----Loss: 318.2900390625\n",
      "Epochs: 4100/5000----Loss: 318.2900390625\n",
      "Epochs: 4101/5000----Loss: 318.2900390625\n",
      "Epochs: 4102/5000----Loss: 318.2899475097656\n",
      "Epochs: 4103/5000----Loss: 318.2899475097656\n",
      "Epochs: 4104/5000----Loss: 318.28997802734375\n",
      "Epochs: 4105/5000----Loss: 318.289794921875\n",
      "Epochs: 4106/5000----Loss: 318.2897644042969\n",
      "Epochs: 4107/5000----Loss: 318.28973388671875\n",
      "Epochs: 4108/5000----Loss: 318.2896728515625\n",
      "Epochs: 4109/5000----Loss: 318.28973388671875\n",
      "Epochs: 4110/5000----Loss: 318.28961181640625\n",
      "Epochs: 4111/5000----Loss: 318.2897033691406\n",
      "Epochs: 4112/5000----Loss: 318.28955078125\n",
      "Epochs: 4113/5000----Loss: 318.2895812988281\n",
      "Epochs: 4114/5000----Loss: 318.28948974609375\n",
      "Epochs: 4115/5000----Loss: 318.2894287109375\n",
      "Epochs: 4116/5000----Loss: 318.28948974609375\n",
      "Epochs: 4117/5000----Loss: 318.2894592285156\n",
      "Epochs: 4118/5000----Loss: 318.2893981933594\n",
      "Epochs: 4119/5000----Loss: 318.28936767578125\n",
      "Epochs: 4120/5000----Loss: 318.2893371582031\n",
      "Epochs: 4121/5000----Loss: 318.2893981933594\n",
      "Epochs: 4122/5000----Loss: 318.2893371582031\n",
      "Epochs: 4123/5000----Loss: 318.28924560546875\n",
      "Epochs: 4124/5000----Loss: 318.2891845703125\n",
      "Epochs: 4125/5000----Loss: 318.2892150878906\n",
      "Epochs: 4126/5000----Loss: 318.2891540527344\n",
      "Epochs: 4127/5000----Loss: 318.28912353515625\n",
      "Epochs: 4128/5000----Loss: 318.2890930175781\n",
      "Epochs: 4129/5000----Loss: 318.28912353515625\n",
      "Epochs: 4130/5000----Loss: 318.2890625\n",
      "Epochs: 4131/5000----Loss: 318.28900146484375\n",
      "Epochs: 4132/5000----Loss: 318.28887939453125\n",
      "Epochs: 4133/5000----Loss: 318.28900146484375\n",
      "Epochs: 4134/5000----Loss: 318.2888488769531\n",
      "Epochs: 4135/5000----Loss: 318.2889099121094\n",
      "Epochs: 4136/5000----Loss: 318.2888488769531\n",
      "Epochs: 4137/5000----Loss: 318.2888488769531\n",
      "Epochs: 4138/5000----Loss: 318.288818359375\n",
      "Epochs: 4139/5000----Loss: 318.288818359375\n",
      "Epochs: 4140/5000----Loss: 318.28887939453125\n",
      "Epochs: 4141/5000----Loss: 318.2887268066406\n",
      "Epochs: 4142/5000----Loss: 318.28875732421875\n",
      "Epochs: 4143/5000----Loss: 318.28875732421875\n",
      "Epochs: 4144/5000----Loss: 318.28863525390625\n",
      "Epochs: 4145/5000----Loss: 318.2886657714844\n",
      "Epochs: 4146/5000----Loss: 318.28863525390625\n",
      "Epochs: 4147/5000----Loss: 318.28857421875\n",
      "Epochs: 4148/5000----Loss: 318.2884826660156\n",
      "Epochs: 4149/5000----Loss: 318.28851318359375\n",
      "Epochs: 4150/5000----Loss: 318.2884826660156\n",
      "Epochs: 4151/5000----Loss: 318.288330078125\n",
      "Epochs: 4152/5000----Loss: 318.2883605957031\n",
      "Epochs: 4153/5000----Loss: 318.288330078125\n",
      "Epochs: 4154/5000----Loss: 318.2882995605469\n",
      "Epochs: 4155/5000----Loss: 318.28839111328125\n",
      "Epochs: 4156/5000----Loss: 318.2882080078125\n",
      "Epochs: 4157/5000----Loss: 318.2882385253906\n",
      "Epochs: 4158/5000----Loss: 318.2882080078125\n",
      "Epochs: 4159/5000----Loss: 318.2880554199219\n",
      "Epochs: 4160/5000----Loss: 318.2880859375\n",
      "Epochs: 4161/5000----Loss: 318.28802490234375\n",
      "Epochs: 4162/5000----Loss: 318.28802490234375\n",
      "Epochs: 4163/5000----Loss: 318.2880554199219\n",
      "Epochs: 4164/5000----Loss: 318.28802490234375\n",
      "Epochs: 4165/5000----Loss: 318.28802490234375\n",
      "Epochs: 4166/5000----Loss: 318.28802490234375\n",
      "Epochs: 4167/5000----Loss: 318.28790283203125\n",
      "Epochs: 4168/5000----Loss: 318.28790283203125\n",
      "Epochs: 4169/5000----Loss: 318.28790283203125\n",
      "Epochs: 4170/5000----Loss: 318.2878723144531\n",
      "Epochs: 4171/5000----Loss: 318.2879333496094\n",
      "Epochs: 4172/5000----Loss: 318.2877502441406\n",
      "Epochs: 4173/5000----Loss: 318.2878112792969\n",
      "Epochs: 4174/5000----Loss: 318.28778076171875\n",
      "Epochs: 4175/5000----Loss: 318.28765869140625\n",
      "Epochs: 4176/5000----Loss: 318.28765869140625\n",
      "Epochs: 4177/5000----Loss: 318.2877197265625\n",
      "Epochs: 4178/5000----Loss: 318.28759765625\n",
      "Epochs: 4179/5000----Loss: 318.2875671386719\n",
      "Epochs: 4180/5000----Loss: 318.2875061035156\n",
      "Epochs: 4181/5000----Loss: 318.28753662109375\n",
      "Epochs: 4182/5000----Loss: 318.2875061035156\n",
      "Epochs: 4183/5000----Loss: 318.28741455078125\n",
      "Epochs: 4184/5000----Loss: 318.28753662109375\n",
      "Epochs: 4185/5000----Loss: 318.2873840332031\n",
      "Epochs: 4186/5000----Loss: 318.28741455078125\n",
      "Epochs: 4187/5000----Loss: 318.287353515625\n",
      "Epochs: 4188/5000----Loss: 318.28729248046875\n",
      "Epochs: 4189/5000----Loss: 318.287353515625\n",
      "Epochs: 4190/5000----Loss: 318.2873229980469\n",
      "Epochs: 4191/5000----Loss: 318.28717041015625\n",
      "Epochs: 4192/5000----Loss: 318.2872314453125\n",
      "Epochs: 4193/5000----Loss: 318.28729248046875\n",
      "Epochs: 4194/5000----Loss: 318.28717041015625\n",
      "Epochs: 4195/5000----Loss: 318.2870788574219\n",
      "Epochs: 4196/5000----Loss: 318.2870788574219\n",
      "Epochs: 4197/5000----Loss: 318.2871398925781\n",
      "Epochs: 4198/5000----Loss: 318.28704833984375\n",
      "Epochs: 4199/5000----Loss: 318.2869873046875\n",
      "Epochs: 4200/5000----Loss: 318.2870178222656\n",
      "Epochs: 4201/5000----Loss: 318.28692626953125\n",
      "Epochs: 4202/5000----Loss: 318.2868957519531\n",
      "Epochs: 4203/5000----Loss: 318.2868347167969\n",
      "Epochs: 4204/5000----Loss: 318.2867126464844\n",
      "Epochs: 4205/5000----Loss: 318.2867126464844\n",
      "Epochs: 4206/5000----Loss: 318.2867126464844\n",
      "Epochs: 4207/5000----Loss: 318.2866516113281\n",
      "Epochs: 4208/5000----Loss: 318.28668212890625\n",
      "Epochs: 4209/5000----Loss: 318.2866516113281\n",
      "Epochs: 4210/5000----Loss: 318.2865905761719\n",
      "Epochs: 4211/5000----Loss: 318.2866516113281\n",
      "Epochs: 4212/5000----Loss: 318.28656005859375\n",
      "Epochs: 4213/5000----Loss: 318.2865905761719\n",
      "Epochs: 4214/5000----Loss: 318.2865905761719\n",
      "Epochs: 4215/5000----Loss: 318.2864685058594\n",
      "Epochs: 4216/5000----Loss: 318.28662109375\n",
      "Epochs: 4217/5000----Loss: 318.2864685058594\n",
      "Epochs: 4218/5000----Loss: 318.2863464355469\n",
      "Epochs: 4219/5000----Loss: 318.28643798828125\n",
      "Epochs: 4220/5000----Loss: 318.2862854003906\n",
      "Epochs: 4221/5000----Loss: 318.28631591796875\n",
      "Epochs: 4222/5000----Loss: 318.28631591796875\n",
      "Epochs: 4223/5000----Loss: 318.28619384765625\n",
      "Epochs: 4224/5000----Loss: 318.2862243652344\n",
      "Epochs: 4225/5000----Loss: 318.2861022949219\n",
      "Epochs: 4226/5000----Loss: 318.28619384765625\n",
      "Epochs: 4227/5000----Loss: 318.28631591796875\n",
      "Epochs: 4228/5000----Loss: 318.28619384765625\n",
      "Epochs: 4229/5000----Loss: 318.2861328125\n",
      "Epochs: 4230/5000----Loss: 318.2861328125\n",
      "Epochs: 4231/5000----Loss: 318.2861022949219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4232/5000----Loss: 318.2860412597656\n",
      "Epochs: 4233/5000----Loss: 318.28607177734375\n",
      "Epochs: 4234/5000----Loss: 318.28594970703125\n",
      "Epochs: 4235/5000----Loss: 318.28607177734375\n",
      "Epochs: 4236/5000----Loss: 318.2860107421875\n",
      "Epochs: 4237/5000----Loss: 318.2859191894531\n",
      "Epochs: 4238/5000----Loss: 318.2859191894531\n",
      "Epochs: 4239/5000----Loss: 318.28582763671875\n",
      "Epochs: 4240/5000----Loss: 318.28594970703125\n",
      "Epochs: 4241/5000----Loss: 318.28582763671875\n",
      "Epochs: 4242/5000----Loss: 318.2858581542969\n",
      "Epochs: 4243/5000----Loss: 318.28582763671875\n",
      "Epochs: 4244/5000----Loss: 318.2856750488281\n",
      "Epochs: 4245/5000----Loss: 318.2856140136719\n",
      "Epochs: 4246/5000----Loss: 318.28558349609375\n",
      "Epochs: 4247/5000----Loss: 318.28558349609375\n",
      "Epochs: 4248/5000----Loss: 318.28558349609375\n",
      "Epochs: 4249/5000----Loss: 318.28546142578125\n",
      "Epochs: 4250/5000----Loss: 318.2854919433594\n",
      "Epochs: 4251/5000----Loss: 318.28546142578125\n",
      "Epochs: 4252/5000----Loss: 318.28546142578125\n",
      "Epochs: 4253/5000----Loss: 318.285400390625\n",
      "Epochs: 4254/5000----Loss: 318.28533935546875\n",
      "Epochs: 4255/5000----Loss: 318.2852783203125\n",
      "Epochs: 4256/5000----Loss: 318.2852783203125\n",
      "Epochs: 4257/5000----Loss: 318.2853088378906\n",
      "Epochs: 4258/5000----Loss: 318.2852478027344\n",
      "Epochs: 4259/5000----Loss: 318.28521728515625\n",
      "Epochs: 4260/5000----Loss: 318.28509521484375\n",
      "Epochs: 4261/5000----Loss: 318.28521728515625\n",
      "Epochs: 4262/5000----Loss: 318.2852478027344\n",
      "Epochs: 4263/5000----Loss: 318.2851257324219\n",
      "Epochs: 4264/5000----Loss: 318.2851867675781\n",
      "Epochs: 4265/5000----Loss: 318.2850036621094\n",
      "Epochs: 4266/5000----Loss: 318.2850341796875\n",
      "Epochs: 4267/5000----Loss: 318.2850341796875\n",
      "Epochs: 4268/5000----Loss: 318.2849426269531\n",
      "Epochs: 4269/5000----Loss: 318.2848815917969\n",
      "Epochs: 4270/5000----Loss: 318.28485107421875\n",
      "Epochs: 4271/5000----Loss: 318.2848815917969\n",
      "Epochs: 4272/5000----Loss: 318.28485107421875\n",
      "Epochs: 4273/5000----Loss: 318.28485107421875\n",
      "Epochs: 4274/5000----Loss: 318.2847900390625\n",
      "Epochs: 4275/5000----Loss: 318.28485107421875\n",
      "Epochs: 4276/5000----Loss: 318.28485107421875\n",
      "Epochs: 4277/5000----Loss: 318.28472900390625\n",
      "Epochs: 4278/5000----Loss: 318.28472900390625\n",
      "Epochs: 4279/5000----Loss: 318.28460693359375\n",
      "Epochs: 4280/5000----Loss: 318.28472900390625\n",
      "Epochs: 4281/5000----Loss: 318.28466796875\n",
      "Epochs: 4282/5000----Loss: 318.28460693359375\n",
      "Epochs: 4283/5000----Loss: 318.28460693359375\n",
      "Epochs: 4284/5000----Loss: 318.28460693359375\n",
      "Epochs: 4285/5000----Loss: 318.28460693359375\n",
      "Epochs: 4286/5000----Loss: 318.2844543457031\n",
      "Epochs: 4287/5000----Loss: 318.2844543457031\n",
      "Epochs: 4288/5000----Loss: 318.2845153808594\n",
      "Epochs: 4289/5000----Loss: 318.28436279296875\n",
      "Epochs: 4290/5000----Loss: 318.28436279296875\n",
      "Epochs: 4291/5000----Loss: 318.2842712402344\n",
      "Epochs: 4292/5000----Loss: 318.28424072265625\n",
      "Epochs: 4293/5000----Loss: 318.28424072265625\n",
      "Epochs: 4294/5000----Loss: 318.2842102050781\n",
      "Epochs: 4295/5000----Loss: 318.28411865234375\n",
      "Epochs: 4296/5000----Loss: 318.28411865234375\n",
      "Epochs: 4297/5000----Loss: 318.2841491699219\n",
      "Epochs: 4298/5000----Loss: 318.2840881347656\n",
      "Epochs: 4299/5000----Loss: 318.28411865234375\n",
      "Epochs: 4300/5000----Loss: 318.2840270996094\n",
      "Epochs: 4301/5000----Loss: 318.28399658203125\n",
      "Epochs: 4302/5000----Loss: 318.2839660644531\n",
      "Epochs: 4303/5000----Loss: 318.28399658203125\n",
      "Epochs: 4304/5000----Loss: 318.2839660644531\n",
      "Epochs: 4305/5000----Loss: 318.283935546875\n",
      "Epochs: 4306/5000----Loss: 318.2838439941406\n",
      "Epochs: 4307/5000----Loss: 318.283935546875\n",
      "Epochs: 4308/5000----Loss: 318.28387451171875\n",
      "Epochs: 4309/5000----Loss: 318.28387451171875\n",
      "Epochs: 4310/5000----Loss: 318.2837829589844\n",
      "Epochs: 4311/5000----Loss: 318.28369140625\n",
      "Epochs: 4312/5000----Loss: 318.2837829589844\n",
      "Epochs: 4313/5000----Loss: 318.28363037109375\n",
      "Epochs: 4314/5000----Loss: 318.28363037109375\n",
      "Epochs: 4315/5000----Loss: 318.2836608886719\n",
      "Epochs: 4316/5000----Loss: 318.2835693359375\n",
      "Epochs: 4317/5000----Loss: 318.28369140625\n",
      "Epochs: 4318/5000----Loss: 318.2835693359375\n",
      "Epochs: 4319/5000----Loss: 318.2835388183594\n",
      "Epochs: 4320/5000----Loss: 318.28350830078125\n",
      "Epochs: 4321/5000----Loss: 318.2835388183594\n",
      "Epochs: 4322/5000----Loss: 318.28350830078125\n",
      "Epochs: 4323/5000----Loss: 318.28350830078125\n",
      "Epochs: 4324/5000----Loss: 318.2834167480469\n",
      "Epochs: 4325/5000----Loss: 318.28338623046875\n",
      "Epochs: 4326/5000----Loss: 318.28338623046875\n",
      "Epochs: 4327/5000----Loss: 318.2832946777344\n",
      "Epochs: 4328/5000----Loss: 318.2834167480469\n",
      "Epochs: 4329/5000----Loss: 318.28326416015625\n",
      "Epochs: 4330/5000----Loss: 318.2832336425781\n",
      "Epochs: 4331/5000----Loss: 318.28326416015625\n",
      "Epochs: 4332/5000----Loss: 318.28326416015625\n",
      "Epochs: 4333/5000----Loss: 318.283203125\n",
      "Epochs: 4334/5000----Loss: 318.28314208984375\n",
      "Epochs: 4335/5000----Loss: 318.28314208984375\n",
      "Epochs: 4336/5000----Loss: 318.28314208984375\n",
      "Epochs: 4337/5000----Loss: 318.282958984375\n",
      "Epochs: 4338/5000----Loss: 318.2830505371094\n",
      "Epochs: 4339/5000----Loss: 318.28289794921875\n",
      "Epochs: 4340/5000----Loss: 318.28289794921875\n",
      "Epochs: 4341/5000----Loss: 318.28289794921875\n",
      "Epochs: 4342/5000----Loss: 318.2828063964844\n",
      "Epochs: 4343/5000----Loss: 318.28277587890625\n",
      "Epochs: 4344/5000----Loss: 318.28277587890625\n",
      "Epochs: 4345/5000----Loss: 318.28277587890625\n",
      "Epochs: 4346/5000----Loss: 318.28277587890625\n",
      "Epochs: 4347/5000----Loss: 318.2827453613281\n",
      "Epochs: 4348/5000----Loss: 318.2826843261719\n",
      "Epochs: 4349/5000----Loss: 318.2826843261719\n",
      "Epochs: 4350/5000----Loss: 318.2826843261719\n",
      "Epochs: 4351/5000----Loss: 318.2826843261719\n",
      "Epochs: 4352/5000----Loss: 318.2825622558594\n",
      "Epochs: 4353/5000----Loss: 318.28253173828125\n",
      "Epochs: 4354/5000----Loss: 318.28253173828125\n",
      "Epochs: 4355/5000----Loss: 318.2824401855469\n",
      "Epochs: 4356/5000----Loss: 318.2825012207031\n",
      "Epochs: 4357/5000----Loss: 318.282470703125\n",
      "Epochs: 4358/5000----Loss: 318.282470703125\n",
      "Epochs: 4359/5000----Loss: 318.2823791503906\n",
      "Epochs: 4360/5000----Loss: 318.2823486328125\n",
      "Epochs: 4361/5000----Loss: 318.2823181152344\n",
      "Epochs: 4362/5000----Loss: 318.28228759765625\n",
      "Epochs: 4363/5000----Loss: 318.2823181152344\n",
      "Epochs: 4364/5000----Loss: 318.28228759765625\n",
      "Epochs: 4365/5000----Loss: 318.2822570800781\n",
      "Epochs: 4366/5000----Loss: 318.2821350097656\n",
      "Epochs: 4367/5000----Loss: 318.2821960449219\n",
      "Epochs: 4368/5000----Loss: 318.2821960449219\n",
      "Epochs: 4369/5000----Loss: 318.2821044921875\n",
      "Epochs: 4370/5000----Loss: 318.28204345703125\n",
      "Epochs: 4371/5000----Loss: 318.2821044921875\n",
      "Epochs: 4372/5000----Loss: 318.28216552734375\n",
      "Epochs: 4373/5000----Loss: 318.28204345703125\n",
      "Epochs: 4374/5000----Loss: 318.28204345703125\n",
      "Epochs: 4375/5000----Loss: 318.28204345703125\n",
      "Epochs: 4376/5000----Loss: 318.281982421875\n",
      "Epochs: 4377/5000----Loss: 318.28192138671875\n",
      "Epochs: 4378/5000----Loss: 318.28192138671875\n",
      "Epochs: 4379/5000----Loss: 318.2819519042969\n",
      "Epochs: 4380/5000----Loss: 318.2818603515625\n",
      "Epochs: 4381/5000----Loss: 318.2818298339844\n",
      "Epochs: 4382/5000----Loss: 318.2818908691406\n",
      "Epochs: 4383/5000----Loss: 318.28179931640625\n",
      "Epochs: 4384/5000----Loss: 318.28167724609375\n",
      "Epochs: 4385/5000----Loss: 318.2815856933594\n",
      "Epochs: 4386/5000----Loss: 318.2816467285156\n",
      "Epochs: 4387/5000----Loss: 318.2815856933594\n",
      "Epochs: 4388/5000----Loss: 318.28167724609375\n",
      "Epochs: 4389/5000----Loss: 318.28155517578125\n",
      "Epochs: 4390/5000----Loss: 318.281494140625\n",
      "Epochs: 4391/5000----Loss: 318.28155517578125\n",
      "Epochs: 4392/5000----Loss: 318.28143310546875\n",
      "Epochs: 4393/5000----Loss: 318.28155517578125\n",
      "Epochs: 4394/5000----Loss: 318.28143310546875\n",
      "Epochs: 4395/5000----Loss: 318.28143310546875\n",
      "Epochs: 4396/5000----Loss: 318.28131103515625\n",
      "Epochs: 4397/5000----Loss: 318.28143310546875\n",
      "Epochs: 4398/5000----Loss: 318.2813720703125\n",
      "Epochs: 4399/5000----Loss: 318.2813415527344\n",
      "Epochs: 4400/5000----Loss: 318.28131103515625\n",
      "Epochs: 4401/5000----Loss: 318.28125\n",
      "Epochs: 4402/5000----Loss: 318.28125\n",
      "Epochs: 4403/5000----Loss: 318.28118896484375\n",
      "Epochs: 4404/5000----Loss: 318.2812194824219\n",
      "Epochs: 4405/5000----Loss: 318.28118896484375\n",
      "Epochs: 4406/5000----Loss: 318.2810974121094\n",
      "Epochs: 4407/5000----Loss: 318.28106689453125\n",
      "Epochs: 4408/5000----Loss: 318.2811279296875\n",
      "Epochs: 4409/5000----Loss: 318.28106689453125\n",
      "Epochs: 4410/5000----Loss: 318.281005859375\n",
      "Epochs: 4411/5000----Loss: 318.28106689453125\n",
      "Epochs: 4412/5000----Loss: 318.2809753417969\n",
      "Epochs: 4413/5000----Loss: 318.2809753417969\n",
      "Epochs: 4414/5000----Loss: 318.28106689453125\n",
      "Epochs: 4415/5000----Loss: 318.28094482421875\n",
      "Epochs: 4416/5000----Loss: 318.2808837890625\n",
      "Epochs: 4417/5000----Loss: 318.28076171875\n",
      "Epochs: 4418/5000----Loss: 318.28076171875\n",
      "Epochs: 4419/5000----Loss: 318.2809143066406\n",
      "Epochs: 4420/5000----Loss: 318.2807312011719\n",
      "Epochs: 4421/5000----Loss: 318.28076171875\n",
      "Epochs: 4422/5000----Loss: 318.28070068359375\n",
      "Epochs: 4423/5000----Loss: 318.2807922363281\n",
      "Epochs: 4424/5000----Loss: 318.2807312011719\n",
      "Epochs: 4425/5000----Loss: 318.28070068359375\n",
      "Epochs: 4426/5000----Loss: 318.2806701660156\n",
      "Epochs: 4427/5000----Loss: 318.28057861328125\n",
      "Epochs: 4428/5000----Loss: 318.2806701660156\n",
      "Epochs: 4429/5000----Loss: 318.2805480957031\n",
      "Epochs: 4430/5000----Loss: 318.28045654296875\n",
      "Epochs: 4431/5000----Loss: 318.2805480957031\n",
      "Epochs: 4432/5000----Loss: 318.28045654296875\n",
      "Epochs: 4433/5000----Loss: 318.2803039550781\n",
      "Epochs: 4434/5000----Loss: 318.28033447265625\n",
      "Epochs: 4435/5000----Loss: 318.28033447265625\n",
      "Epochs: 4436/5000----Loss: 318.28033447265625\n",
      "Epochs: 4437/5000----Loss: 318.2801513671875\n",
      "Epochs: 4438/5000----Loss: 318.2802429199219\n",
      "Epochs: 4439/5000----Loss: 318.2802734375\n",
      "Epochs: 4440/5000----Loss: 318.2801818847656\n",
      "Epochs: 4441/5000----Loss: 318.2801818847656\n",
      "Epochs: 4442/5000----Loss: 318.28021240234375\n",
      "Epochs: 4443/5000----Loss: 318.28009033203125\n",
      "Epochs: 4444/5000----Loss: 318.280029296875\n",
      "Epochs: 4445/5000----Loss: 318.280029296875\n",
      "Epochs: 4446/5000----Loss: 318.27996826171875\n",
      "Epochs: 4447/5000----Loss: 318.280029296875\n",
      "Epochs: 4448/5000----Loss: 318.2800598144531\n",
      "Epochs: 4449/5000----Loss: 318.27996826171875\n",
      "Epochs: 4450/5000----Loss: 318.2799072265625\n",
      "Epochs: 4451/5000----Loss: 318.27984619140625\n",
      "Epochs: 4452/5000----Loss: 318.2798767089844\n",
      "Epochs: 4453/5000----Loss: 318.2798767089844\n",
      "Epochs: 4454/5000----Loss: 318.2797546386719\n",
      "Epochs: 4455/5000----Loss: 318.27984619140625\n",
      "Epochs: 4456/5000----Loss: 318.2797546386719\n",
      "Epochs: 4457/5000----Loss: 318.27972412109375\n",
      "Epochs: 4458/5000----Loss: 318.27972412109375\n",
      "Epochs: 4459/5000----Loss: 318.2796630859375\n",
      "Epochs: 4460/5000----Loss: 318.27972412109375\n",
      "Epochs: 4461/5000----Loss: 318.2796325683594\n",
      "Epochs: 4462/5000----Loss: 318.27960205078125\n",
      "Epochs: 4463/5000----Loss: 318.27960205078125\n",
      "Epochs: 4464/5000----Loss: 318.27960205078125\n",
      "Epochs: 4465/5000----Loss: 318.27960205078125\n",
      "Epochs: 4466/5000----Loss: 318.279541015625\n",
      "Epochs: 4467/5000----Loss: 318.27960205078125\n",
      "Epochs: 4468/5000----Loss: 318.27947998046875\n",
      "Epochs: 4469/5000----Loss: 318.27947998046875\n",
      "Epochs: 4470/5000----Loss: 318.27947998046875\n",
      "Epochs: 4471/5000----Loss: 318.2794189453125\n",
      "Epochs: 4472/5000----Loss: 318.2794189453125\n",
      "Epochs: 4473/5000----Loss: 318.2794189453125\n",
      "Epochs: 4474/5000----Loss: 318.27935791015625\n",
      "Epochs: 4475/5000----Loss: 318.27935791015625\n",
      "Epochs: 4476/5000----Loss: 318.2792663574219\n",
      "Epochs: 4477/5000----Loss: 318.27923583984375\n",
      "Epochs: 4478/5000----Loss: 318.2791748046875\n",
      "Epochs: 4479/5000----Loss: 318.2791442871094\n",
      "Epochs: 4480/5000----Loss: 318.2790222167969\n",
      "Epochs: 4481/5000----Loss: 318.2790222167969\n",
      "Epochs: 4482/5000----Loss: 318.27899169921875\n",
      "Epochs: 4483/5000----Loss: 318.27899169921875\n",
      "Epochs: 4484/5000----Loss: 318.279052734375\n",
      "Epochs: 4485/5000----Loss: 318.27899169921875\n",
      "Epochs: 4486/5000----Loss: 318.2789001464844\n",
      "Epochs: 4487/5000----Loss: 318.27899169921875\n",
      "Epochs: 4488/5000----Loss: 318.27899169921875\n",
      "Epochs: 4489/5000----Loss: 318.2789001464844\n",
      "Epochs: 4490/5000----Loss: 318.27880859375\n",
      "Epochs: 4491/5000----Loss: 318.27886962890625\n",
      "Epochs: 4492/5000----Loss: 318.27880859375\n",
      "Epochs: 4493/5000----Loss: 318.2787780761719\n",
      "Epochs: 4494/5000----Loss: 318.27874755859375\n",
      "Epochs: 4495/5000----Loss: 318.2788391113281\n",
      "Epochs: 4496/5000----Loss: 318.27874755859375\n",
      "Epochs: 4497/5000----Loss: 318.2787780761719\n",
      "Epochs: 4498/5000----Loss: 318.2786865234375\n",
      "Epochs: 4499/5000----Loss: 318.2786560058594\n",
      "Epochs: 4500/5000----Loss: 318.2785949707031\n",
      "Epochs: 4501/5000----Loss: 318.2785949707031\n",
      "Epochs: 4502/5000----Loss: 318.27850341796875\n",
      "Epochs: 4503/5000----Loss: 318.2784118652344\n",
      "Epochs: 4504/5000----Loss: 318.27850341796875\n",
      "Epochs: 4505/5000----Loss: 318.2784423828125\n",
      "Epochs: 4506/5000----Loss: 318.278564453125\n",
      "Epochs: 4507/5000----Loss: 318.2784118652344\n",
      "Epochs: 4508/5000----Loss: 318.27838134765625\n",
      "Epochs: 4509/5000----Loss: 318.2783508300781\n",
      "Epochs: 4510/5000----Loss: 318.2784423828125\n",
      "Epochs: 4511/5000----Loss: 318.2783508300781\n",
      "Epochs: 4512/5000----Loss: 318.27838134765625\n",
      "Epochs: 4513/5000----Loss: 318.2783203125\n",
      "Epochs: 4514/5000----Loss: 318.27825927734375\n",
      "Epochs: 4515/5000----Loss: 318.27825927734375\n",
      "Epochs: 4516/5000----Loss: 318.2781982421875\n",
      "Epochs: 4517/5000----Loss: 318.2782287597656\n",
      "Epochs: 4518/5000----Loss: 318.2781677246094\n",
      "Epochs: 4519/5000----Loss: 318.2781066894531\n",
      "Epochs: 4520/5000----Loss: 318.27813720703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4521/5000----Loss: 318.27801513671875\n",
      "Epochs: 4522/5000----Loss: 318.278076171875\n",
      "Epochs: 4523/5000----Loss: 318.27801513671875\n",
      "Epochs: 4524/5000----Loss: 318.27789306640625\n",
      "Epochs: 4525/5000----Loss: 318.27789306640625\n",
      "Epochs: 4526/5000----Loss: 318.2779846191406\n",
      "Epochs: 4527/5000----Loss: 318.27789306640625\n",
      "Epochs: 4528/5000----Loss: 318.27789306640625\n",
      "Epochs: 4529/5000----Loss: 318.27783203125\n",
      "Epochs: 4530/5000----Loss: 318.2778015136719\n",
      "Epochs: 4531/5000----Loss: 318.27777099609375\n",
      "Epochs: 4532/5000----Loss: 318.2777404785156\n",
      "Epochs: 4533/5000----Loss: 318.27777099609375\n",
      "Epochs: 4534/5000----Loss: 318.27764892578125\n",
      "Epochs: 4535/5000----Loss: 318.2777099609375\n",
      "Epochs: 4536/5000----Loss: 318.2775573730469\n",
      "Epochs: 4537/5000----Loss: 318.2776184082031\n",
      "Epochs: 4538/5000----Loss: 318.27752685546875\n",
      "Epochs: 4539/5000----Loss: 318.2776184082031\n",
      "Epochs: 4540/5000----Loss: 318.277587890625\n",
      "Epochs: 4541/5000----Loss: 318.277587890625\n",
      "Epochs: 4542/5000----Loss: 318.2774963378906\n",
      "Epochs: 4543/5000----Loss: 318.2774658203125\n",
      "Epochs: 4544/5000----Loss: 318.27740478515625\n",
      "Epochs: 4545/5000----Loss: 318.27740478515625\n",
      "Epochs: 4546/5000----Loss: 318.27740478515625\n",
      "Epochs: 4547/5000----Loss: 318.27740478515625\n",
      "Epochs: 4548/5000----Loss: 318.27734375\n",
      "Epochs: 4549/5000----Loss: 318.2773742675781\n",
      "Epochs: 4550/5000----Loss: 318.27728271484375\n",
      "Epochs: 4551/5000----Loss: 318.27716064453125\n",
      "Epochs: 4552/5000----Loss: 318.2772521972656\n",
      "Epochs: 4553/5000----Loss: 318.27728271484375\n",
      "Epochs: 4554/5000----Loss: 318.27716064453125\n",
      "Epochs: 4555/5000----Loss: 318.2772216796875\n",
      "Epochs: 4556/5000----Loss: 318.27716064453125\n",
      "Epochs: 4557/5000----Loss: 318.27716064453125\n",
      "Epochs: 4558/5000----Loss: 318.27716064453125\n",
      "Epochs: 4559/5000----Loss: 318.2770690917969\n",
      "Epochs: 4560/5000----Loss: 318.27703857421875\n",
      "Epochs: 4561/5000----Loss: 318.2770080566406\n",
      "Epochs: 4562/5000----Loss: 318.2770080566406\n",
      "Epochs: 4563/5000----Loss: 318.2769775390625\n",
      "Epochs: 4564/5000----Loss: 318.27703857421875\n",
      "Epochs: 4565/5000----Loss: 318.27691650390625\n",
      "Epochs: 4566/5000----Loss: 318.27703857421875\n",
      "Epochs: 4567/5000----Loss: 318.2769470214844\n",
      "Epochs: 4568/5000----Loss: 318.27685546875\n",
      "Epochs: 4569/5000----Loss: 318.27685546875\n",
      "Epochs: 4570/5000----Loss: 318.2767639160156\n",
      "Epochs: 4571/5000----Loss: 318.2767028808594\n",
      "Epochs: 4572/5000----Loss: 318.2767028808594\n",
      "Epochs: 4573/5000----Loss: 318.2767028808594\n",
      "Epochs: 4574/5000----Loss: 318.276611328125\n",
      "Epochs: 4575/5000----Loss: 318.27655029296875\n",
      "Epochs: 4576/5000----Loss: 318.2765808105469\n",
      "Epochs: 4577/5000----Loss: 318.2765197753906\n",
      "Epochs: 4578/5000----Loss: 318.2763977050781\n",
      "Epochs: 4579/5000----Loss: 318.2765197753906\n",
      "Epochs: 4580/5000----Loss: 318.2765808105469\n",
      "Epochs: 4581/5000----Loss: 318.27655029296875\n",
      "Epochs: 4582/5000----Loss: 318.2765808105469\n",
      "Epochs: 4583/5000----Loss: 318.2763671875\n",
      "Epochs: 4584/5000----Loss: 318.2763671875\n",
      "Epochs: 4585/5000----Loss: 318.2763671875\n",
      "Epochs: 4586/5000----Loss: 318.2763671875\n",
      "Epochs: 4587/5000----Loss: 318.27642822265625\n",
      "Epochs: 4588/5000----Loss: 318.2763671875\n",
      "Epochs: 4589/5000----Loss: 318.2762756347656\n",
      "Epochs: 4590/5000----Loss: 318.2762756347656\n",
      "Epochs: 4591/5000----Loss: 318.2762451171875\n",
      "Epochs: 4592/5000----Loss: 318.2762145996094\n",
      "Epochs: 4593/5000----Loss: 318.2760925292969\n",
      "Epochs: 4594/5000----Loss: 318.27618408203125\n",
      "Epochs: 4595/5000----Loss: 318.2760925292969\n",
      "Epochs: 4596/5000----Loss: 318.27606201171875\n",
      "Epochs: 4597/5000----Loss: 318.2760925292969\n",
      "Epochs: 4598/5000----Loss: 318.27618408203125\n",
      "Epochs: 4599/5000----Loss: 318.27606201171875\n",
      "Epochs: 4600/5000----Loss: 318.2759704589844\n",
      "Epochs: 4601/5000----Loss: 318.2759704589844\n",
      "Epochs: 4602/5000----Loss: 318.27593994140625\n",
      "Epochs: 4603/5000----Loss: 318.2759704589844\n",
      "Epochs: 4604/5000----Loss: 318.27593994140625\n",
      "Epochs: 4605/5000----Loss: 318.27593994140625\n",
      "Epochs: 4606/5000----Loss: 318.27587890625\n",
      "Epochs: 4607/5000----Loss: 318.27581787109375\n",
      "Epochs: 4608/5000----Loss: 318.2757568359375\n",
      "Epochs: 4609/5000----Loss: 318.27581787109375\n",
      "Epochs: 4610/5000----Loss: 318.2757568359375\n",
      "Epochs: 4611/5000----Loss: 318.2757873535156\n",
      "Epochs: 4612/5000----Loss: 318.2757873535156\n",
      "Epochs: 4613/5000----Loss: 318.27581787109375\n",
      "Epochs: 4614/5000----Loss: 318.2756652832031\n",
      "Epochs: 4615/5000----Loss: 318.2757263183594\n",
      "Epochs: 4616/5000----Loss: 318.27557373046875\n",
      "Epochs: 4617/5000----Loss: 318.2756042480469\n",
      "Epochs: 4618/5000----Loss: 318.27557373046875\n",
      "Epochs: 4619/5000----Loss: 318.27557373046875\n",
      "Epochs: 4620/5000----Loss: 318.27557373046875\n",
      "Epochs: 4621/5000----Loss: 318.275390625\n",
      "Epochs: 4622/5000----Loss: 318.275390625\n",
      "Epochs: 4623/5000----Loss: 318.275390625\n",
      "Epochs: 4624/5000----Loss: 318.2754211425781\n",
      "Epochs: 4625/5000----Loss: 318.2752990722656\n",
      "Epochs: 4626/5000----Loss: 318.27532958984375\n",
      "Epochs: 4627/5000----Loss: 318.27532958984375\n",
      "Epochs: 4628/5000----Loss: 318.2752990722656\n",
      "Epochs: 4629/5000----Loss: 318.2752990722656\n",
      "Epochs: 4630/5000----Loss: 318.27532958984375\n",
      "Epochs: 4631/5000----Loss: 318.2751770019531\n",
      "Epochs: 4632/5000----Loss: 318.2752685546875\n",
      "Epochs: 4633/5000----Loss: 318.275146484375\n",
      "Epochs: 4634/5000----Loss: 318.2751770019531\n",
      "Epochs: 4635/5000----Loss: 318.2751770019531\n",
      "Epochs: 4636/5000----Loss: 318.27508544921875\n",
      "Epochs: 4637/5000----Loss: 318.2750549316406\n",
      "Epochs: 4638/5000----Loss: 318.27496337890625\n",
      "Epochs: 4639/5000----Loss: 318.27496337890625\n",
      "Epochs: 4640/5000----Loss: 318.27496337890625\n",
      "Epochs: 4641/5000----Loss: 318.2748718261719\n",
      "Epochs: 4642/5000----Loss: 318.27496337890625\n",
      "Epochs: 4643/5000----Loss: 318.2750244140625\n",
      "Epochs: 4644/5000----Loss: 318.27490234375\n",
      "Epochs: 4645/5000----Loss: 318.2748107910156\n",
      "Epochs: 4646/5000----Loss: 318.27484130859375\n",
      "Epochs: 4647/5000----Loss: 318.2747497558594\n",
      "Epochs: 4648/5000----Loss: 318.27471923828125\n",
      "Epochs: 4649/5000----Loss: 318.2748107910156\n",
      "Epochs: 4650/5000----Loss: 318.2747497558594\n",
      "Epochs: 4651/5000----Loss: 318.27484130859375\n",
      "Epochs: 4652/5000----Loss: 318.27471923828125\n",
      "Epochs: 4653/5000----Loss: 318.274658203125\n",
      "Epochs: 4654/5000----Loss: 318.27459716796875\n",
      "Epochs: 4655/5000----Loss: 318.27459716796875\n",
      "Epochs: 4656/5000----Loss: 318.274658203125\n",
      "Epochs: 4657/5000----Loss: 318.2745056152344\n",
      "Epochs: 4658/5000----Loss: 318.2746887207031\n",
      "Epochs: 4659/5000----Loss: 318.2745056152344\n",
      "Epochs: 4660/5000----Loss: 318.27447509765625\n",
      "Epochs: 4661/5000----Loss: 318.27447509765625\n",
      "Epochs: 4662/5000----Loss: 318.27447509765625\n",
      "Epochs: 4663/5000----Loss: 318.2744140625\n",
      "Epochs: 4664/5000----Loss: 318.27435302734375\n",
      "Epochs: 4665/5000----Loss: 318.27435302734375\n",
      "Epochs: 4666/5000----Loss: 318.27423095703125\n",
      "Epochs: 4667/5000----Loss: 318.2743225097656\n",
      "Epochs: 4668/5000----Loss: 318.2742614746094\n",
      "Epochs: 4669/5000----Loss: 318.27423095703125\n",
      "Epochs: 4670/5000----Loss: 318.2742004394531\n",
      "Epochs: 4671/5000----Loss: 318.274169921875\n",
      "Epochs: 4672/5000----Loss: 318.274169921875\n",
      "Epochs: 4673/5000----Loss: 318.2740478515625\n",
      "Epochs: 4674/5000----Loss: 318.2742004394531\n",
      "Epochs: 4675/5000----Loss: 318.2740783691406\n",
      "Epochs: 4676/5000----Loss: 318.27410888671875\n",
      "Epochs: 4677/5000----Loss: 318.2740173339844\n",
      "Epochs: 4678/5000----Loss: 318.2740173339844\n",
      "Epochs: 4679/5000----Loss: 318.27398681640625\n",
      "Epochs: 4680/5000----Loss: 318.2738952636719\n",
      "Epochs: 4681/5000----Loss: 318.27398681640625\n",
      "Epochs: 4682/5000----Loss: 318.2738952636719\n",
      "Epochs: 4683/5000----Loss: 318.27398681640625\n",
      "Epochs: 4684/5000----Loss: 318.2738952636719\n",
      "Epochs: 4685/5000----Loss: 318.2738342285156\n",
      "Epochs: 4686/5000----Loss: 318.27386474609375\n",
      "Epochs: 4687/5000----Loss: 318.27386474609375\n",
      "Epochs: 4688/5000----Loss: 318.27386474609375\n",
      "Epochs: 4689/5000----Loss: 318.27374267578125\n",
      "Epochs: 4690/5000----Loss: 318.273681640625\n",
      "Epochs: 4691/5000----Loss: 318.273681640625\n",
      "Epochs: 4692/5000----Loss: 318.27374267578125\n",
      "Epochs: 4693/5000----Loss: 318.2737121582031\n",
      "Epochs: 4694/5000----Loss: 318.27362060546875\n",
      "Epochs: 4695/5000----Loss: 318.2736511230469\n",
      "Epochs: 4696/5000----Loss: 318.27362060546875\n",
      "Epochs: 4697/5000----Loss: 318.2735900878906\n",
      "Epochs: 4698/5000----Loss: 318.2735290527344\n",
      "Epochs: 4699/5000----Loss: 318.27349853515625\n",
      "Epochs: 4700/5000----Loss: 318.2735595703125\n",
      "Epochs: 4701/5000----Loss: 318.27349853515625\n",
      "Epochs: 4702/5000----Loss: 318.2734375\n",
      "Epochs: 4703/5000----Loss: 318.2734069824219\n",
      "Epochs: 4704/5000----Loss: 318.2734375\n",
      "Epochs: 4705/5000----Loss: 318.27337646484375\n",
      "Epochs: 4706/5000----Loss: 318.27337646484375\n",
      "Epochs: 4707/5000----Loss: 318.2734680175781\n",
      "Epochs: 4708/5000----Loss: 318.27325439453125\n",
      "Epochs: 4709/5000----Loss: 318.27337646484375\n",
      "Epochs: 4710/5000----Loss: 318.273193359375\n",
      "Epochs: 4711/5000----Loss: 318.27313232421875\n",
      "Epochs: 4712/5000----Loss: 318.27313232421875\n",
      "Epochs: 4713/5000----Loss: 318.2731628417969\n",
      "Epochs: 4714/5000----Loss: 318.27313232421875\n",
      "Epochs: 4715/5000----Loss: 318.2731628417969\n",
      "Epochs: 4716/5000----Loss: 318.2730407714844\n",
      "Epochs: 4717/5000----Loss: 318.2730407714844\n",
      "Epochs: 4718/5000----Loss: 318.27301025390625\n",
      "Epochs: 4719/5000----Loss: 318.2729187011719\n",
      "Epochs: 4720/5000----Loss: 318.27294921875\n",
      "Epochs: 4721/5000----Loss: 318.2729187011719\n",
      "Epochs: 4722/5000----Loss: 318.2728576660156\n",
      "Epochs: 4723/5000----Loss: 318.2728576660156\n",
      "Epochs: 4724/5000----Loss: 318.27288818359375\n",
      "Epochs: 4725/5000----Loss: 318.2728576660156\n",
      "Epochs: 4726/5000----Loss: 318.2728576660156\n",
      "Epochs: 4727/5000----Loss: 318.2728271484375\n",
      "Epochs: 4728/5000----Loss: 318.27288818359375\n",
      "Epochs: 4729/5000----Loss: 318.27276611328125\n",
      "Epochs: 4730/5000----Loss: 318.272705078125\n",
      "Epochs: 4731/5000----Loss: 318.272705078125\n",
      "Epochs: 4732/5000----Loss: 318.2727355957031\n",
      "Epochs: 4733/5000----Loss: 318.2725830078125\n",
      "Epochs: 4734/5000----Loss: 318.2726135253906\n",
      "Epochs: 4735/5000----Loss: 318.2725830078125\n",
      "Epochs: 4736/5000----Loss: 318.2725524902344\n",
      "Epochs: 4737/5000----Loss: 318.2724609375\n",
      "Epochs: 4738/5000----Loss: 318.27252197265625\n",
      "Epochs: 4739/5000----Loss: 318.2724609375\n",
      "Epochs: 4740/5000----Loss: 318.2724914550781\n",
      "Epochs: 4741/5000----Loss: 318.2726135253906\n",
      "Epochs: 4742/5000----Loss: 318.2723388671875\n",
      "Epochs: 4743/5000----Loss: 318.2724304199219\n",
      "Epochs: 4744/5000----Loss: 318.27239990234375\n",
      "Epochs: 4745/5000----Loss: 318.27227783203125\n",
      "Epochs: 4746/5000----Loss: 318.2723388671875\n",
      "Epochs: 4747/5000----Loss: 318.2723083496094\n",
      "Epochs: 4748/5000----Loss: 318.2723693847656\n",
      "Epochs: 4749/5000----Loss: 318.27227783203125\n",
      "Epochs: 4750/5000----Loss: 318.27227783203125\n",
      "Epochs: 4751/5000----Loss: 318.272216796875\n",
      "Epochs: 4752/5000----Loss: 318.27227783203125\n",
      "Epochs: 4753/5000----Loss: 318.27215576171875\n",
      "Epochs: 4754/5000----Loss: 318.27215576171875\n",
      "Epochs: 4755/5000----Loss: 318.27215576171875\n",
      "Epochs: 4756/5000----Loss: 318.2720031738281\n",
      "Epochs: 4757/5000----Loss: 318.27203369140625\n",
      "Epochs: 4758/5000----Loss: 318.27203369140625\n",
      "Epochs: 4759/5000----Loss: 318.2720031738281\n",
      "Epochs: 4760/5000----Loss: 318.27191162109375\n",
      "Epochs: 4761/5000----Loss: 318.27178955078125\n",
      "Epochs: 4762/5000----Loss: 318.2718200683594\n",
      "Epochs: 4763/5000----Loss: 318.27191162109375\n",
      "Epochs: 4764/5000----Loss: 318.27191162109375\n",
      "Epochs: 4765/5000----Loss: 318.2718811035156\n",
      "Epochs: 4766/5000----Loss: 318.2718505859375\n",
      "Epochs: 4767/5000----Loss: 318.27178955078125\n",
      "Epochs: 4768/5000----Loss: 318.27178955078125\n",
      "Epochs: 4769/5000----Loss: 318.271728515625\n",
      "Epochs: 4770/5000----Loss: 318.271728515625\n",
      "Epochs: 4771/5000----Loss: 318.27166748046875\n",
      "Epochs: 4772/5000----Loss: 318.271728515625\n",
      "Epochs: 4773/5000----Loss: 318.2716979980469\n",
      "Epochs: 4774/5000----Loss: 318.2717590332031\n",
      "Epochs: 4775/5000----Loss: 318.2716064453125\n",
      "Epochs: 4776/5000----Loss: 318.27154541015625\n",
      "Epochs: 4777/5000----Loss: 318.27154541015625\n",
      "Epochs: 4778/5000----Loss: 318.27154541015625\n",
      "Epochs: 4779/5000----Loss: 318.2715759277344\n",
      "Epochs: 4780/5000----Loss: 318.27142333984375\n",
      "Epochs: 4781/5000----Loss: 318.271484375\n",
      "Epochs: 4782/5000----Loss: 318.2714538574219\n",
      "Epochs: 4783/5000----Loss: 318.27142333984375\n",
      "Epochs: 4784/5000----Loss: 318.27130126953125\n",
      "Epochs: 4785/5000----Loss: 318.27130126953125\n",
      "Epochs: 4786/5000----Loss: 318.2713928222656\n",
      "Epochs: 4787/5000----Loss: 318.2713623046875\n",
      "Epochs: 4788/5000----Loss: 318.271240234375\n",
      "Epochs: 4789/5000----Loss: 318.27130126953125\n",
      "Epochs: 4790/5000----Loss: 318.27117919921875\n",
      "Epochs: 4791/5000----Loss: 318.2712707519531\n",
      "Epochs: 4792/5000----Loss: 318.27130126953125\n",
      "Epochs: 4793/5000----Loss: 318.27117919921875\n",
      "Epochs: 4794/5000----Loss: 318.2711486816406\n",
      "Epochs: 4795/5000----Loss: 318.27117919921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4796/5000----Loss: 318.2711486816406\n",
      "Epochs: 4797/5000----Loss: 318.2711181640625\n",
      "Epochs: 4798/5000----Loss: 318.2710876464844\n",
      "Epochs: 4799/5000----Loss: 318.27117919921875\n",
      "Epochs: 4800/5000----Loss: 318.2710266113281\n",
      "Epochs: 4801/5000----Loss: 318.27093505859375\n",
      "Epochs: 4802/5000----Loss: 318.27099609375\n",
      "Epochs: 4803/5000----Loss: 318.2708740234375\n",
      "Epochs: 4804/5000----Loss: 318.2709045410156\n",
      "Epochs: 4805/5000----Loss: 318.2707824707031\n",
      "Epochs: 4806/5000----Loss: 318.2709045410156\n",
      "Epochs: 4807/5000----Loss: 318.270751953125\n",
      "Epochs: 4808/5000----Loss: 318.2706604003906\n",
      "Epochs: 4809/5000----Loss: 318.2706604003906\n",
      "Epochs: 4810/5000----Loss: 318.2707824707031\n",
      "Epochs: 4811/5000----Loss: 318.2707824707031\n",
      "Epochs: 4812/5000----Loss: 318.2706604003906\n",
      "Epochs: 4813/5000----Loss: 318.2706298828125\n",
      "Epochs: 4814/5000----Loss: 318.27069091796875\n",
      "Epochs: 4815/5000----Loss: 318.27069091796875\n",
      "Epochs: 4816/5000----Loss: 318.27056884765625\n",
      "Epochs: 4817/5000----Loss: 318.27056884765625\n",
      "Epochs: 4818/5000----Loss: 318.27056884765625\n",
      "Epochs: 4819/5000----Loss: 318.2705383300781\n",
      "Epochs: 4820/5000----Loss: 318.2705383300781\n",
      "Epochs: 4821/5000----Loss: 318.27044677734375\n",
      "Epochs: 4822/5000----Loss: 318.2705078125\n",
      "Epochs: 4823/5000----Loss: 318.27056884765625\n",
      "Epochs: 4824/5000----Loss: 318.2704162597656\n",
      "Epochs: 4825/5000----Loss: 318.27032470703125\n",
      "Epochs: 4826/5000----Loss: 318.2704162597656\n",
      "Epochs: 4827/5000----Loss: 318.2703857421875\n",
      "Epochs: 4828/5000----Loss: 318.270263671875\n",
      "Epochs: 4829/5000----Loss: 318.27032470703125\n",
      "Epochs: 4830/5000----Loss: 318.2702331542969\n",
      "Epochs: 4831/5000----Loss: 318.27020263671875\n",
      "Epochs: 4832/5000----Loss: 318.2702941894531\n",
      "Epochs: 4833/5000----Loss: 318.2701110839844\n",
      "Epochs: 4834/5000----Loss: 318.27020263671875\n",
      "Epochs: 4835/5000----Loss: 318.27020263671875\n",
      "Epochs: 4836/5000----Loss: 318.2701110839844\n",
      "Epochs: 4837/5000----Loss: 318.2701416015625\n",
      "Epochs: 4838/5000----Loss: 318.27008056640625\n",
      "Epochs: 4839/5000----Loss: 318.2701110839844\n",
      "Epochs: 4840/5000----Loss: 318.27008056640625\n",
      "Epochs: 4841/5000----Loss: 318.26995849609375\n",
      "Epochs: 4842/5000----Loss: 318.26995849609375\n",
      "Epochs: 4843/5000----Loss: 318.27001953125\n",
      "Epochs: 4844/5000----Loss: 318.2699890136719\n",
      "Epochs: 4845/5000----Loss: 318.26995849609375\n",
      "Epochs: 4846/5000----Loss: 318.26995849609375\n",
      "Epochs: 4847/5000----Loss: 318.2699279785156\n",
      "Epochs: 4848/5000----Loss: 318.26983642578125\n",
      "Epochs: 4849/5000----Loss: 318.269775390625\n",
      "Epochs: 4850/5000----Loss: 318.26971435546875\n",
      "Epochs: 4851/5000----Loss: 318.26971435546875\n",
      "Epochs: 4852/5000----Loss: 318.2697448730469\n",
      "Epochs: 4853/5000----Loss: 318.2696228027344\n",
      "Epochs: 4854/5000----Loss: 318.2696533203125\n",
      "Epochs: 4855/5000----Loss: 318.2696533203125\n",
      "Epochs: 4856/5000----Loss: 318.26953125\n",
      "Epochs: 4857/5000----Loss: 318.2695007324219\n",
      "Epochs: 4858/5000----Loss: 318.26947021484375\n",
      "Epochs: 4859/5000----Loss: 318.26959228515625\n",
      "Epochs: 4860/5000----Loss: 318.2693786621094\n",
      "Epochs: 4861/5000----Loss: 318.26953125\n",
      "Epochs: 4862/5000----Loss: 318.2694091796875\n",
      "Epochs: 4863/5000----Loss: 318.26947021484375\n",
      "Epochs: 4864/5000----Loss: 318.26934814453125\n",
      "Epochs: 4865/5000----Loss: 318.2694091796875\n",
      "Epochs: 4866/5000----Loss: 318.26934814453125\n",
      "Epochs: 4867/5000----Loss: 318.2694396972656\n",
      "Epochs: 4868/5000----Loss: 318.26934814453125\n",
      "Epochs: 4869/5000----Loss: 318.269287109375\n",
      "Epochs: 4870/5000----Loss: 318.2692565917969\n",
      "Epochs: 4871/5000----Loss: 318.26934814453125\n",
      "Epochs: 4872/5000----Loss: 318.2691955566406\n",
      "Epochs: 4873/5000----Loss: 318.26922607421875\n",
      "Epochs: 4874/5000----Loss: 318.26922607421875\n",
      "Epochs: 4875/5000----Loss: 318.2691955566406\n",
      "Epochs: 4876/5000----Loss: 318.26910400390625\n",
      "Epochs: 4877/5000----Loss: 318.2691650390625\n",
      "Epochs: 4878/5000----Loss: 318.26910400390625\n",
      "Epochs: 4879/5000----Loss: 318.2691955566406\n",
      "Epochs: 4880/5000----Loss: 318.2690124511719\n",
      "Epochs: 4881/5000----Loss: 318.26898193359375\n",
      "Epochs: 4882/5000----Loss: 318.2689514160156\n",
      "Epochs: 4883/5000----Loss: 318.2689208984375\n",
      "Epochs: 4884/5000----Loss: 318.2689208984375\n",
      "Epochs: 4885/5000----Loss: 318.26898193359375\n",
      "Epochs: 4886/5000----Loss: 318.26885986328125\n",
      "Epochs: 4887/5000----Loss: 318.2688293457031\n",
      "Epochs: 4888/5000----Loss: 318.26885986328125\n",
      "Epochs: 4889/5000----Loss: 318.26885986328125\n",
      "Epochs: 4890/5000----Loss: 318.26885986328125\n",
      "Epochs: 4891/5000----Loss: 318.26885986328125\n",
      "Epochs: 4892/5000----Loss: 318.2688293457031\n",
      "Epochs: 4893/5000----Loss: 318.2687683105469\n",
      "Epochs: 4894/5000----Loss: 318.26873779296875\n",
      "Epochs: 4895/5000----Loss: 318.2687683105469\n",
      "Epochs: 4896/5000----Loss: 318.2686462402344\n",
      "Epochs: 4897/5000----Loss: 318.2685546875\n",
      "Epochs: 4898/5000----Loss: 318.26861572265625\n",
      "Epochs: 4899/5000----Loss: 318.2685241699219\n",
      "Epochs: 4900/5000----Loss: 318.26861572265625\n",
      "Epochs: 4901/5000----Loss: 318.26849365234375\n",
      "Epochs: 4902/5000----Loss: 318.2685241699219\n",
      "Epochs: 4903/5000----Loss: 318.2684020996094\n",
      "Epochs: 4904/5000----Loss: 318.26837158203125\n",
      "Epochs: 4905/5000----Loss: 318.2684020996094\n",
      "Epochs: 4906/5000----Loss: 318.26837158203125\n",
      "Epochs: 4907/5000----Loss: 318.26837158203125\n",
      "Epochs: 4908/5000----Loss: 318.2683410644531\n",
      "Epochs: 4909/5000----Loss: 318.26837158203125\n",
      "Epochs: 4910/5000----Loss: 318.2683410644531\n",
      "Epochs: 4911/5000----Loss: 318.2684326171875\n",
      "Epochs: 4912/5000----Loss: 318.26824951171875\n",
      "Epochs: 4913/5000----Loss: 318.2681884765625\n",
      "Epochs: 4914/5000----Loss: 318.2681579589844\n",
      "Epochs: 4915/5000----Loss: 318.2682189941406\n",
      "Epochs: 4916/5000----Loss: 318.2682189941406\n",
      "Epochs: 4917/5000----Loss: 318.2681884765625\n",
      "Epochs: 4918/5000----Loss: 318.26812744140625\n",
      "Epochs: 4919/5000----Loss: 318.2680969238281\n",
      "Epochs: 4920/5000----Loss: 318.26806640625\n",
      "Epochs: 4921/5000----Loss: 318.26806640625\n",
      "Epochs: 4922/5000----Loss: 318.26800537109375\n",
      "Epochs: 4923/5000----Loss: 318.26800537109375\n",
      "Epochs: 4924/5000----Loss: 318.2679138183594\n",
      "Epochs: 4925/5000----Loss: 318.2679138183594\n",
      "Epochs: 4926/5000----Loss: 318.26800537109375\n",
      "Epochs: 4927/5000----Loss: 318.26788330078125\n",
      "Epochs: 4928/5000----Loss: 318.2678527832031\n",
      "Epochs: 4929/5000----Loss: 318.26788330078125\n",
      "Epochs: 4930/5000----Loss: 318.2677917480469\n",
      "Epochs: 4931/5000----Loss: 318.2678527832031\n",
      "Epochs: 4932/5000----Loss: 318.26776123046875\n",
      "Epochs: 4933/5000----Loss: 318.26788330078125\n",
      "Epochs: 4934/5000----Loss: 318.26776123046875\n",
      "Epochs: 4935/5000----Loss: 318.26776123046875\n",
      "Epochs: 4936/5000----Loss: 318.26776123046875\n",
      "Epochs: 4937/5000----Loss: 318.2677001953125\n",
      "Epochs: 4938/5000----Loss: 318.26763916015625\n",
      "Epochs: 4939/5000----Loss: 318.2677001953125\n",
      "Epochs: 4940/5000----Loss: 318.2676086425781\n",
      "Epochs: 4941/5000----Loss: 318.2676086425781\n",
      "Epochs: 4942/5000----Loss: 318.2675476074219\n",
      "Epochs: 4943/5000----Loss: 318.2674865722656\n",
      "Epochs: 4944/5000----Loss: 318.26739501953125\n",
      "Epochs: 4945/5000----Loss: 318.267333984375\n",
      "Epochs: 4946/5000----Loss: 318.26739501953125\n",
      "Epochs: 4947/5000----Loss: 318.2673645019531\n",
      "Epochs: 4948/5000----Loss: 318.2674255371094\n",
      "Epochs: 4949/5000----Loss: 318.267333984375\n",
      "Epochs: 4950/5000----Loss: 318.2673034667969\n",
      "Epochs: 4951/5000----Loss: 318.2673645019531\n",
      "Epochs: 4952/5000----Loss: 318.26727294921875\n",
      "Epochs: 4953/5000----Loss: 318.2672424316406\n",
      "Epochs: 4954/5000----Loss: 318.2672424316406\n",
      "Epochs: 4955/5000----Loss: 318.2672119140625\n",
      "Epochs: 4956/5000----Loss: 318.2671813964844\n",
      "Epochs: 4957/5000----Loss: 318.26715087890625\n",
      "Epochs: 4958/5000----Loss: 318.2672119140625\n",
      "Epochs: 4959/5000----Loss: 318.2671203613281\n",
      "Epochs: 4960/5000----Loss: 318.26715087890625\n",
      "Epochs: 4961/5000----Loss: 318.26702880859375\n",
      "Epochs: 4962/5000----Loss: 318.26708984375\n",
      "Epochs: 4963/5000----Loss: 318.26708984375\n",
      "Epochs: 4964/5000----Loss: 318.26702880859375\n",
      "Epochs: 4965/5000----Loss: 318.2669982910156\n",
      "Epochs: 4966/5000----Loss: 318.2669982910156\n",
      "Epochs: 4967/5000----Loss: 318.2669372558594\n",
      "Epochs: 4968/5000----Loss: 318.2669372558594\n",
      "Epochs: 4969/5000----Loss: 318.26690673828125\n",
      "Epochs: 4970/5000----Loss: 318.266845703125\n",
      "Epochs: 4971/5000----Loss: 318.26690673828125\n",
      "Epochs: 4972/5000----Loss: 318.2667541503906\n",
      "Epochs: 4973/5000----Loss: 318.26678466796875\n",
      "Epochs: 4974/5000----Loss: 318.2668151855469\n",
      "Epochs: 4975/5000----Loss: 318.26678466796875\n",
      "Epochs: 4976/5000----Loss: 318.26666259765625\n",
      "Epochs: 4977/5000----Loss: 318.26666259765625\n",
      "Epochs: 4978/5000----Loss: 318.2666931152344\n",
      "Epochs: 4979/5000----Loss: 318.2666931152344\n",
      "Epochs: 4980/5000----Loss: 318.26666259765625\n",
      "Epochs: 4981/5000----Loss: 318.2666931152344\n",
      "Epochs: 4982/5000----Loss: 318.2665710449219\n",
      "Epochs: 4983/5000----Loss: 318.2666015625\n",
      "Epochs: 4984/5000----Loss: 318.26654052734375\n",
      "Epochs: 4985/5000----Loss: 318.2666320800781\n",
      "Epochs: 4986/5000----Loss: 318.2664489746094\n",
      "Epochs: 4987/5000----Loss: 318.26654052734375\n",
      "Epochs: 4988/5000----Loss: 318.2665100097656\n",
      "Epochs: 4989/5000----Loss: 318.26641845703125\n",
      "Epochs: 4990/5000----Loss: 318.26629638671875\n",
      "Epochs: 4991/5000----Loss: 318.266357421875\n",
      "Epochs: 4992/5000----Loss: 318.2662353515625\n",
      "Epochs: 4993/5000----Loss: 318.26629638671875\n",
      "Epochs: 4994/5000----Loss: 318.2662658691406\n",
      "Epochs: 4995/5000----Loss: 318.2662048339844\n",
      "Epochs: 4996/5000----Loss: 318.26611328125\n",
      "Epochs: 4997/5000----Loss: 318.2662048339844\n",
      "Epochs: 4998/5000----Loss: 318.2662048339844\n",
      "Epochs: 4999/5000----Loss: 318.2662353515625\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "n=5000\n",
    "for i in range(n):\n",
    "    pred=model(inputs)\n",
    "    loss=MSE(pred, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -=w.grad * 1e-5\n",
    "        b -=b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    print(f\"Epochs: {i}/{n}----Loss: {loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 45.9623,  80.9117],\n",
       "        [ 51.2441,  49.7796],\n",
       "        [ 85.7058, 125.4026],\n",
       "        [ 26.9460,  30.9596],\n",
       "        [107.6184, 128.4843]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 32.,  66.],\n",
       "        [ 52.,  76.],\n",
       "        [102., 155.],\n",
       "        [ 32.,  26.],\n",
       "        [ 99., 100.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torch built function\n",
    "\n",
    "We've implemented linear regression & gradient descent model using some basic tensor operations. However, since this is a common pattern in deep learning, PyTorch provides several built-in functions and classes to make it easy to create and train models with just a few lines of code.\n",
    "\n",
    "Let's begin by importing the torch.nn package from PyTorch, which contains utility classes for building neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.],\n",
       "        [ 74.,  66.,  43.],\n",
       "        [ 91.,  87.,  65.],\n",
       "        [ 88., 134.,  59.],\n",
       "        [101.,  44.,  37.],\n",
       "        [ 68.,  96.,  71.],\n",
       "        [ 73.,  66.,  44.],\n",
       "        [ 92.,  87.,  64.],\n",
       "        [ 87., 135.,  57.],\n",
       "        [103.,  43.,  36.],\n",
       "        [ 68.,  97.,  70.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using 15 training examples to illustrate how to work with large datasets in small batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "\n",
    "We'll create a TensorDataset, which allows access to rows from inputs and targets as tuples, and provides standard APIs for working with many different types of datasets in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorDataset allows us to access a small section of the training data using the array indexing notation ([0:3] in the above code). It returns a tuple with two elements. The first element contains the input variables for the selected rows, and the second contains the targets.\n",
    "\n",
    "We'll also create a DataLoader, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the data loader in a for loop. Let's look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 91.,  87.,  65.],\n",
      "        [ 92.,  87.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [ 68.,  96.,  71.],\n",
      "        [ 73.,  67.,  43.]])\n",
      "tensor([[ 80., 102.],\n",
      "        [ 82., 100.],\n",
      "        [119., 133.],\n",
      "        [104., 118.],\n",
      "        [ 56.,  70.]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each iteration, the data loader returns one batch of data with the given batch size. If shuffle is set to True, it shuffles the training data before creating batches. Shuffling helps randomize the input to the optimization algorithm, leading to a faster reduction in the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nstead of initializing the weights & biases manually, we can define the model using the nn.Linear class from PyTorch, which does it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1729, -0.4657, -0.2185],\n",
      "        [-0.0198,  0.4971, -0.3879]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2242, 0.0150], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models also have a helpful .parameters method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1729, -0.4657, -0.2185],\n",
       "         [-0.0198,  0.4971, -0.3879]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2242, 0.0150], requires_grad=True)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "list(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-27.7554,  15.1883],\n",
       "        [-39.0127,  17.1229],\n",
       "        [-59.8177,  42.3945],\n",
       "        [-10.2523,   5.0109],\n",
       "        [-47.8535,  19.2084],\n",
       "        [-27.1168,  14.6714],\n",
       "        [-38.7655,  16.2379],\n",
       "        [-59.8633,  41.9868],\n",
       "        [-10.8910,   5.5278],\n",
       "        [-48.2449,  18.8403],\n",
       "        [-27.5082,  14.3033],\n",
       "        [-38.3741,  16.6060],\n",
       "        [-60.0650,  43.2795],\n",
       "        [ -9.8609,   5.3790],\n",
       "        [-48.4921,  19.7253]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of defining a loss function manually, we can use the built-in loss function mse_loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nn.functional\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nn.functional package contains many useful loss functions and several other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the loss for the current predictions of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10630.1113, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually manipulating the model's weights & biases using gradients, we can use the optimizer optim.SGD. SGD is short for \"stochastic gradient descent\". The term stochastic indicates that samples are selected in random batches instead of as a single group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that model.parameters() is passed as an argument to optim.SGD so that the optimizer knows which matrices should be modified during the update step. Also, we can specify a learning rate that controls the amount by which the parameters are modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train the model. We'll follow the same process to implement gradient descent:\n",
    "\n",
    "1. Generate predictions\n",
    "\n",
    "2. Calculate the loss\n",
    "\n",
    "3. Compute gradients w.r.t the weights and biases\n",
    "\n",
    "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "\n",
    "5. Reset the gradients to zero\n",
    "\n",
    "The only change is that we'll work batches of data instead of processing the entire training data in every iteration. Let's define a utility function fit that trains the model for a given number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to note above:\n",
    "\n",
    "We use the data loader defined earlier to get batches of data for every iteration.\n",
    "\n",
    "Instead of updating parameters (weights and biases) manually, we use opt.step to perform the update and opt.zero_grad to reset the gradients to zero.\n",
    "\n",
    "We've also added a log statement that prints the loss from the last batch of data for every 10th epoch to track training progress. loss.item returns the actual value stored in the loss tensor.\n",
    "\n",
    "Let's train the model for 10000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 1.4477\n",
      "Epoch [20/10000], Loss: 1.2127\n",
      "Epoch [30/10000], Loss: 1.1874\n",
      "Epoch [40/10000], Loss: 0.8488\n",
      "Epoch [50/10000], Loss: 0.9678\n",
      "Epoch [60/10000], Loss: 1.2249\n",
      "Epoch [70/10000], Loss: 1.3920\n",
      "Epoch [80/10000], Loss: 0.7308\n",
      "Epoch [90/10000], Loss: 0.9319\n",
      "Epoch [100/10000], Loss: 0.8753\n",
      "Epoch [110/10000], Loss: 0.8323\n",
      "Epoch [120/10000], Loss: 0.9652\n",
      "Epoch [130/10000], Loss: 0.8570\n",
      "Epoch [140/10000], Loss: 0.8403\n",
      "Epoch [150/10000], Loss: 0.9978\n",
      "Epoch [160/10000], Loss: 0.7324\n",
      "Epoch [170/10000], Loss: 1.3965\n",
      "Epoch [180/10000], Loss: 0.6629\n",
      "Epoch [190/10000], Loss: 1.2298\n",
      "Epoch [200/10000], Loss: 1.3910\n",
      "Epoch [210/10000], Loss: 1.3921\n",
      "Epoch [220/10000], Loss: 0.9864\n",
      "Epoch [230/10000], Loss: 0.7814\n",
      "Epoch [240/10000], Loss: 1.1643\n",
      "Epoch [250/10000], Loss: 1.0725\n",
      "Epoch [260/10000], Loss: 1.1879\n",
      "Epoch [270/10000], Loss: 1.0514\n",
      "Epoch [280/10000], Loss: 1.1657\n",
      "Epoch [290/10000], Loss: 0.9420\n",
      "Epoch [300/10000], Loss: 1.3197\n",
      "Epoch [310/10000], Loss: 1.4642\n",
      "Epoch [320/10000], Loss: 1.4442\n",
      "Epoch [330/10000], Loss: 1.2365\n",
      "Epoch [340/10000], Loss: 1.3238\n",
      "Epoch [350/10000], Loss: 1.3549\n",
      "Epoch [360/10000], Loss: 1.0301\n",
      "Epoch [370/10000], Loss: 0.9120\n",
      "Epoch [380/10000], Loss: 1.2774\n",
      "Epoch [390/10000], Loss: 0.7962\n",
      "Epoch [400/10000], Loss: 1.0836\n",
      "Epoch [410/10000], Loss: 0.8897\n",
      "Epoch [420/10000], Loss: 0.8978\n",
      "Epoch [430/10000], Loss: 0.8807\n",
      "Epoch [440/10000], Loss: 0.9171\n",
      "Epoch [450/10000], Loss: 1.0600\n",
      "Epoch [460/10000], Loss: 1.3724\n",
      "Epoch [470/10000], Loss: 0.9576\n",
      "Epoch [480/10000], Loss: 1.3124\n",
      "Epoch [490/10000], Loss: 1.3983\n",
      "Epoch [500/10000], Loss: 1.2673\n",
      "Epoch [510/10000], Loss: 1.1248\n",
      "Epoch [520/10000], Loss: 0.8437\n",
      "Epoch [530/10000], Loss: 1.2670\n",
      "Epoch [540/10000], Loss: 1.2924\n",
      "Epoch [550/10000], Loss: 1.3634\n",
      "Epoch [560/10000], Loss: 0.8604\n",
      "Epoch [570/10000], Loss: 0.7955\n",
      "Epoch [580/10000], Loss: 0.9349\n",
      "Epoch [590/10000], Loss: 1.2832\n",
      "Epoch [600/10000], Loss: 1.0933\n",
      "Epoch [610/10000], Loss: 1.3045\n",
      "Epoch [620/10000], Loss: 1.3034\n",
      "Epoch [630/10000], Loss: 1.0920\n",
      "Epoch [640/10000], Loss: 1.0323\n",
      "Epoch [650/10000], Loss: 1.4466\n",
      "Epoch [660/10000], Loss: 1.3611\n",
      "Epoch [670/10000], Loss: 1.3160\n",
      "Epoch [680/10000], Loss: 1.3004\n",
      "Epoch [690/10000], Loss: 1.1449\n",
      "Epoch [700/10000], Loss: 1.1969\n",
      "Epoch [710/10000], Loss: 1.4643\n",
      "Epoch [720/10000], Loss: 0.9094\n",
      "Epoch [730/10000], Loss: 0.7858\n",
      "Epoch [740/10000], Loss: 1.1459\n",
      "Epoch [750/10000], Loss: 1.2085\n",
      "Epoch [760/10000], Loss: 1.1237\n",
      "Epoch [770/10000], Loss: 0.9686\n",
      "Epoch [780/10000], Loss: 1.3143\n",
      "Epoch [790/10000], Loss: 1.2599\n",
      "Epoch [800/10000], Loss: 1.5085\n",
      "Epoch [810/10000], Loss: 1.4131\n",
      "Epoch [820/10000], Loss: 0.7195\n",
      "Epoch [830/10000], Loss: 0.8744\n",
      "Epoch [840/10000], Loss: 0.8101\n",
      "Epoch [850/10000], Loss: 0.8629\n",
      "Epoch [860/10000], Loss: 1.4989\n",
      "Epoch [870/10000], Loss: 1.2071\n",
      "Epoch [880/10000], Loss: 0.9375\n",
      "Epoch [890/10000], Loss: 1.3241\n",
      "Epoch [900/10000], Loss: 0.8435\n",
      "Epoch [910/10000], Loss: 1.1658\n",
      "Epoch [920/10000], Loss: 0.9406\n",
      "Epoch [930/10000], Loss: 0.7327\n",
      "Epoch [940/10000], Loss: 0.9361\n",
      "Epoch [950/10000], Loss: 1.0115\n",
      "Epoch [960/10000], Loss: 1.0790\n",
      "Epoch [970/10000], Loss: 0.9111\n",
      "Epoch [980/10000], Loss: 1.4907\n",
      "Epoch [990/10000], Loss: 1.1979\n",
      "Epoch [1000/10000], Loss: 1.2966\n",
      "Epoch [1010/10000], Loss: 0.8666\n",
      "Epoch [1020/10000], Loss: 1.0520\n",
      "Epoch [1030/10000], Loss: 0.8547\n",
      "Epoch [1040/10000], Loss: 0.8726\n",
      "Epoch [1050/10000], Loss: 0.8176\n",
      "Epoch [1060/10000], Loss: 1.0144\n",
      "Epoch [1070/10000], Loss: 1.3115\n",
      "Epoch [1080/10000], Loss: 1.1818\n",
      "Epoch [1090/10000], Loss: 1.2138\n",
      "Epoch [1100/10000], Loss: 0.8018\n",
      "Epoch [1110/10000], Loss: 1.3080\n",
      "Epoch [1120/10000], Loss: 0.8954\n",
      "Epoch [1130/10000], Loss: 1.1899\n",
      "Epoch [1140/10000], Loss: 0.9452\n",
      "Epoch [1150/10000], Loss: 1.3237\n",
      "Epoch [1160/10000], Loss: 1.5335\n",
      "Epoch [1170/10000], Loss: 0.8540\n",
      "Epoch [1180/10000], Loss: 0.8853\n",
      "Epoch [1190/10000], Loss: 1.3217\n",
      "Epoch [1200/10000], Loss: 1.5562\n",
      "Epoch [1210/10000], Loss: 1.2060\n",
      "Epoch [1220/10000], Loss: 0.9883\n",
      "Epoch [1230/10000], Loss: 0.8250\n",
      "Epoch [1240/10000], Loss: 1.0966\n",
      "Epoch [1250/10000], Loss: 0.8426\n",
      "Epoch [1260/10000], Loss: 1.3294\n",
      "Epoch [1270/10000], Loss: 1.1153\n",
      "Epoch [1280/10000], Loss: 0.9017\n",
      "Epoch [1290/10000], Loss: 1.3308\n",
      "Epoch [1300/10000], Loss: 1.0315\n",
      "Epoch [1310/10000], Loss: 1.4992\n",
      "Epoch [1320/10000], Loss: 1.3579\n",
      "Epoch [1330/10000], Loss: 0.8972\n",
      "Epoch [1340/10000], Loss: 1.3257\n",
      "Epoch [1350/10000], Loss: 0.7970\n",
      "Epoch [1360/10000], Loss: 1.2500\n",
      "Epoch [1370/10000], Loss: 0.8472\n",
      "Epoch [1380/10000], Loss: 1.2684\n",
      "Epoch [1390/10000], Loss: 0.6742\n",
      "Epoch [1400/10000], Loss: 0.8980\n",
      "Epoch [1410/10000], Loss: 1.2796\n",
      "Epoch [1420/10000], Loss: 1.2713\n",
      "Epoch [1430/10000], Loss: 1.1867\n",
      "Epoch [1440/10000], Loss: 1.1258\n",
      "Epoch [1450/10000], Loss: 0.8191\n",
      "Epoch [1460/10000], Loss: 1.1027\n",
      "Epoch [1470/10000], Loss: 0.8854\n",
      "Epoch [1480/10000], Loss: 0.9817\n",
      "Epoch [1490/10000], Loss: 1.0191\n",
      "Epoch [1500/10000], Loss: 0.8704\n",
      "Epoch [1510/10000], Loss: 0.9570\n",
      "Epoch [1520/10000], Loss: 0.8973\n",
      "Epoch [1530/10000], Loss: 1.1791\n",
      "Epoch [1540/10000], Loss: 1.5304\n",
      "Epoch [1550/10000], Loss: 1.5695\n",
      "Epoch [1560/10000], Loss: 0.9346\n",
      "Epoch [1570/10000], Loss: 0.8240\n",
      "Epoch [1580/10000], Loss: 1.3537\n",
      "Epoch [1590/10000], Loss: 0.9088\n",
      "Epoch [1600/10000], Loss: 1.0015\n",
      "Epoch [1610/10000], Loss: 0.8949\n",
      "Epoch [1620/10000], Loss: 1.2994\n",
      "Epoch [1630/10000], Loss: 0.8918\n",
      "Epoch [1640/10000], Loss: 1.2577\n",
      "Epoch [1650/10000], Loss: 1.1911\n",
      "Epoch [1660/10000], Loss: 0.8477\n",
      "Epoch [1670/10000], Loss: 1.0996\n",
      "Epoch [1680/10000], Loss: 1.1168\n",
      "Epoch [1690/10000], Loss: 0.9352\n",
      "Epoch [1700/10000], Loss: 1.2930\n",
      "Epoch [1710/10000], Loss: 1.1073\n",
      "Epoch [1720/10000], Loss: 0.9942\n",
      "Epoch [1730/10000], Loss: 1.0691\n",
      "Epoch [1740/10000], Loss: 1.3194\n",
      "Epoch [1750/10000], Loss: 0.8187\n",
      "Epoch [1760/10000], Loss: 0.7836\n",
      "Epoch [1770/10000], Loss: 1.2126\n",
      "Epoch [1780/10000], Loss: 0.8929\n",
      "Epoch [1790/10000], Loss: 1.4274\n",
      "Epoch [1800/10000], Loss: 1.2468\n",
      "Epoch [1810/10000], Loss: 0.8146\n",
      "Epoch [1820/10000], Loss: 1.4109\n",
      "Epoch [1830/10000], Loss: 0.9777\n",
      "Epoch [1840/10000], Loss: 1.1029\n",
      "Epoch [1850/10000], Loss: 0.7465\n",
      "Epoch [1860/10000], Loss: 1.2721\n",
      "Epoch [1870/10000], Loss: 0.7869\n",
      "Epoch [1880/10000], Loss: 1.4164\n",
      "Epoch [1890/10000], Loss: 1.1828\n",
      "Epoch [1900/10000], Loss: 0.9663\n",
      "Epoch [1910/10000], Loss: 1.4385\n",
      "Epoch [1920/10000], Loss: 0.9035\n",
      "Epoch [1930/10000], Loss: 0.9976\n",
      "Epoch [1940/10000], Loss: 0.9028\n",
      "Epoch [1950/10000], Loss: 1.2080\n",
      "Epoch [1960/10000], Loss: 1.2728\n",
      "Epoch [1970/10000], Loss: 0.9601\n",
      "Epoch [1980/10000], Loss: 1.1770\n",
      "Epoch [1990/10000], Loss: 0.9243\n",
      "Epoch [2000/10000], Loss: 1.0152\n",
      "Epoch [2010/10000], Loss: 0.7887\n",
      "Epoch [2020/10000], Loss: 1.3233\n",
      "Epoch [2030/10000], Loss: 0.8503\n",
      "Epoch [2040/10000], Loss: 1.2046\n",
      "Epoch [2050/10000], Loss: 0.9794\n",
      "Epoch [2060/10000], Loss: 1.2075\n",
      "Epoch [2070/10000], Loss: 1.4593\n",
      "Epoch [2080/10000], Loss: 1.3712\n",
      "Epoch [2090/10000], Loss: 1.3882\n",
      "Epoch [2100/10000], Loss: 0.8045\n",
      "Epoch [2110/10000], Loss: 1.3509\n",
      "Epoch [2120/10000], Loss: 1.3516\n",
      "Epoch [2130/10000], Loss: 1.1638\n",
      "Epoch [2140/10000], Loss: 1.2622\n",
      "Epoch [2150/10000], Loss: 1.3154\n",
      "Epoch [2160/10000], Loss: 1.4940\n",
      "Epoch [2170/10000], Loss: 1.2969\n",
      "Epoch [2180/10000], Loss: 1.0661\n",
      "Epoch [2190/10000], Loss: 1.1112\n",
      "Epoch [2200/10000], Loss: 0.6845\n",
      "Epoch [2210/10000], Loss: 0.7872\n",
      "Epoch [2220/10000], Loss: 1.3266\n",
      "Epoch [2230/10000], Loss: 1.2006\n",
      "Epoch [2240/10000], Loss: 1.3369\n",
      "Epoch [2250/10000], Loss: 1.4464\n",
      "Epoch [2260/10000], Loss: 0.8445\n",
      "Epoch [2270/10000], Loss: 1.3752\n",
      "Epoch [2280/10000], Loss: 0.7946\n",
      "Epoch [2290/10000], Loss: 0.9398\n",
      "Epoch [2300/10000], Loss: 1.1285\n",
      "Epoch [2310/10000], Loss: 1.3813\n",
      "Epoch [2320/10000], Loss: 0.7832\n",
      "Epoch [2330/10000], Loss: 1.5360\n",
      "Epoch [2340/10000], Loss: 1.0041\n",
      "Epoch [2350/10000], Loss: 1.3318\n",
      "Epoch [2360/10000], Loss: 1.1975\n",
      "Epoch [2370/10000], Loss: 0.9708\n",
      "Epoch [2380/10000], Loss: 0.9217\n",
      "Epoch [2390/10000], Loss: 1.2763\n",
      "Epoch [2400/10000], Loss: 1.4878\n",
      "Epoch [2410/10000], Loss: 1.1689\n",
      "Epoch [2420/10000], Loss: 1.4618\n",
      "Epoch [2430/10000], Loss: 1.3381\n",
      "Epoch [2440/10000], Loss: 0.7430\n",
      "Epoch [2450/10000], Loss: 0.8967\n",
      "Epoch [2460/10000], Loss: 1.0291\n",
      "Epoch [2470/10000], Loss: 1.0237\n",
      "Epoch [2480/10000], Loss: 0.9501\n",
      "Epoch [2490/10000], Loss: 1.2535\n",
      "Epoch [2500/10000], Loss: 1.3790\n",
      "Epoch [2510/10000], Loss: 0.8905\n",
      "Epoch [2520/10000], Loss: 0.8857\n",
      "Epoch [2530/10000], Loss: 0.8908\n",
      "Epoch [2540/10000], Loss: 1.0184\n",
      "Epoch [2550/10000], Loss: 1.1325\n",
      "Epoch [2560/10000], Loss: 0.9152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2570/10000], Loss: 1.2194\n",
      "Epoch [2580/10000], Loss: 0.9680\n",
      "Epoch [2590/10000], Loss: 1.3457\n",
      "Epoch [2600/10000], Loss: 1.1015\n",
      "Epoch [2610/10000], Loss: 1.1123\n",
      "Epoch [2620/10000], Loss: 1.4732\n",
      "Epoch [2630/10000], Loss: 1.4264\n",
      "Epoch [2640/10000], Loss: 1.4479\n",
      "Epoch [2650/10000], Loss: 1.3902\n",
      "Epoch [2660/10000], Loss: 1.0213\n",
      "Epoch [2670/10000], Loss: 0.9498\n",
      "Epoch [2680/10000], Loss: 1.4695\n",
      "Epoch [2690/10000], Loss: 1.2870\n",
      "Epoch [2700/10000], Loss: 1.0970\n",
      "Epoch [2710/10000], Loss: 0.8671\n",
      "Epoch [2720/10000], Loss: 0.6777\n",
      "Epoch [2730/10000], Loss: 1.2343\n",
      "Epoch [2740/10000], Loss: 0.7191\n",
      "Epoch [2750/10000], Loss: 0.9344\n",
      "Epoch [2760/10000], Loss: 1.0562\n",
      "Epoch [2770/10000], Loss: 1.0088\n",
      "Epoch [2780/10000], Loss: 1.2846\n",
      "Epoch [2790/10000], Loss: 1.3971\n",
      "Epoch [2800/10000], Loss: 1.3548\n",
      "Epoch [2810/10000], Loss: 0.9284\n",
      "Epoch [2820/10000], Loss: 1.0431\n",
      "Epoch [2830/10000], Loss: 0.9265\n",
      "Epoch [2840/10000], Loss: 0.8164\n",
      "Epoch [2850/10000], Loss: 0.8040\n",
      "Epoch [2860/10000], Loss: 0.9927\n",
      "Epoch [2870/10000], Loss: 1.3708\n",
      "Epoch [2880/10000], Loss: 1.1025\n",
      "Epoch [2890/10000], Loss: 1.5611\n",
      "Epoch [2900/10000], Loss: 1.4154\n",
      "Epoch [2910/10000], Loss: 0.9045\n",
      "Epoch [2920/10000], Loss: 1.1478\n",
      "Epoch [2930/10000], Loss: 0.9124\n",
      "Epoch [2940/10000], Loss: 0.8618\n",
      "Epoch [2950/10000], Loss: 1.0092\n",
      "Epoch [2960/10000], Loss: 0.9832\n",
      "Epoch [2970/10000], Loss: 1.0838\n",
      "Epoch [2980/10000], Loss: 0.8490\n",
      "Epoch [2990/10000], Loss: 0.8043\n",
      "Epoch [3000/10000], Loss: 1.2877\n",
      "Epoch [3010/10000], Loss: 1.0026\n",
      "Epoch [3020/10000], Loss: 1.3723\n",
      "Epoch [3030/10000], Loss: 0.8052\n",
      "Epoch [3040/10000], Loss: 1.4672\n",
      "Epoch [3050/10000], Loss: 1.5719\n",
      "Epoch [3060/10000], Loss: 0.8779\n",
      "Epoch [3070/10000], Loss: 0.8387\n",
      "Epoch [3080/10000], Loss: 0.8238\n",
      "Epoch [3090/10000], Loss: 0.7151\n",
      "Epoch [3100/10000], Loss: 1.1950\n",
      "Epoch [3110/10000], Loss: 1.4767\n",
      "Epoch [3120/10000], Loss: 0.9840\n",
      "Epoch [3130/10000], Loss: 0.8809\n",
      "Epoch [3140/10000], Loss: 1.2240\n",
      "Epoch [3150/10000], Loss: 0.7840\n",
      "Epoch [3160/10000], Loss: 0.9544\n",
      "Epoch [3170/10000], Loss: 0.7805\n",
      "Epoch [3180/10000], Loss: 1.2478\n",
      "Epoch [3190/10000], Loss: 1.5120\n",
      "Epoch [3200/10000], Loss: 1.3504\n",
      "Epoch [3210/10000], Loss: 0.7396\n",
      "Epoch [3220/10000], Loss: 0.9330\n",
      "Epoch [3230/10000], Loss: 0.9165\n",
      "Epoch [3240/10000], Loss: 1.0039\n",
      "Epoch [3250/10000], Loss: 1.3130\n",
      "Epoch [3260/10000], Loss: 1.3285\n",
      "Epoch [3270/10000], Loss: 0.7352\n",
      "Epoch [3280/10000], Loss: 1.0361\n",
      "Epoch [3290/10000], Loss: 1.0048\n",
      "Epoch [3300/10000], Loss: 1.3617\n",
      "Epoch [3310/10000], Loss: 1.5371\n",
      "Epoch [3320/10000], Loss: 1.6962\n",
      "Epoch [3330/10000], Loss: 1.1309\n",
      "Epoch [3340/10000], Loss: 1.1176\n",
      "Epoch [3350/10000], Loss: 1.2750\n",
      "Epoch [3360/10000], Loss: 0.9764\n",
      "Epoch [3370/10000], Loss: 1.0352\n",
      "Epoch [3380/10000], Loss: 1.2855\n",
      "Epoch [3390/10000], Loss: 1.4036\n",
      "Epoch [3400/10000], Loss: 0.9670\n",
      "Epoch [3410/10000], Loss: 0.7500\n",
      "Epoch [3420/10000], Loss: 1.1779\n",
      "Epoch [3430/10000], Loss: 1.4273\n",
      "Epoch [3440/10000], Loss: 0.8040\n",
      "Epoch [3450/10000], Loss: 0.8717\n",
      "Epoch [3460/10000], Loss: 1.4522\n",
      "Epoch [3470/10000], Loss: 0.8561\n",
      "Epoch [3480/10000], Loss: 0.9365\n",
      "Epoch [3490/10000], Loss: 1.1971\n",
      "Epoch [3500/10000], Loss: 0.7272\n",
      "Epoch [3510/10000], Loss: 1.0046\n",
      "Epoch [3520/10000], Loss: 0.8239\n",
      "Epoch [3530/10000], Loss: 1.2365\n",
      "Epoch [3540/10000], Loss: 1.1783\n",
      "Epoch [3550/10000], Loss: 1.0693\n",
      "Epoch [3560/10000], Loss: 0.8185\n",
      "Epoch [3570/10000], Loss: 1.2648\n",
      "Epoch [3580/10000], Loss: 1.1110\n",
      "Epoch [3590/10000], Loss: 1.3219\n",
      "Epoch [3600/10000], Loss: 0.9349\n",
      "Epoch [3610/10000], Loss: 1.2573\n",
      "Epoch [3620/10000], Loss: 1.0441\n",
      "Epoch [3630/10000], Loss: 1.0296\n",
      "Epoch [3640/10000], Loss: 0.8940\n",
      "Epoch [3650/10000], Loss: 0.9032\n",
      "Epoch [3660/10000], Loss: 1.1647\n",
      "Epoch [3670/10000], Loss: 1.1253\n",
      "Epoch [3680/10000], Loss: 1.1957\n",
      "Epoch [3690/10000], Loss: 0.9137\n",
      "Epoch [3700/10000], Loss: 1.1814\n",
      "Epoch [3710/10000], Loss: 1.0224\n",
      "Epoch [3720/10000], Loss: 1.3839\n",
      "Epoch [3730/10000], Loss: 1.4201\n",
      "Epoch [3740/10000], Loss: 0.9705\n",
      "Epoch [3750/10000], Loss: 1.1126\n",
      "Epoch [3760/10000], Loss: 1.2720\n",
      "Epoch [3770/10000], Loss: 1.3937\n",
      "Epoch [3780/10000], Loss: 0.9812\n",
      "Epoch [3790/10000], Loss: 1.1766\n",
      "Epoch [3800/10000], Loss: 1.3137\n",
      "Epoch [3810/10000], Loss: 1.4384\n",
      "Epoch [3820/10000], Loss: 0.7315\n",
      "Epoch [3830/10000], Loss: 0.8203\n",
      "Epoch [3840/10000], Loss: 0.9157\n",
      "Epoch [3850/10000], Loss: 0.8476\n",
      "Epoch [3860/10000], Loss: 0.7900\n",
      "Epoch [3870/10000], Loss: 1.1228\n",
      "Epoch [3880/10000], Loss: 0.7408\n",
      "Epoch [3890/10000], Loss: 1.0994\n",
      "Epoch [3900/10000], Loss: 0.8893\n",
      "Epoch [3910/10000], Loss: 0.9397\n",
      "Epoch [3920/10000], Loss: 0.9338\n",
      "Epoch [3930/10000], Loss: 0.8889\n",
      "Epoch [3940/10000], Loss: 0.8946\n",
      "Epoch [3950/10000], Loss: 1.0682\n",
      "Epoch [3960/10000], Loss: 1.0993\n",
      "Epoch [3970/10000], Loss: 1.5754\n",
      "Epoch [3980/10000], Loss: 1.2208\n",
      "Epoch [3990/10000], Loss: 0.8733\n",
      "Epoch [4000/10000], Loss: 0.9955\n",
      "Epoch [4010/10000], Loss: 1.3258\n",
      "Epoch [4020/10000], Loss: 1.5359\n",
      "Epoch [4030/10000], Loss: 0.9634\n",
      "Epoch [4040/10000], Loss: 1.2459\n",
      "Epoch [4050/10000], Loss: 0.9619\n",
      "Epoch [4060/10000], Loss: 1.2858\n",
      "Epoch [4070/10000], Loss: 1.0744\n",
      "Epoch [4080/10000], Loss: 1.1552\n",
      "Epoch [4090/10000], Loss: 0.8897\n",
      "Epoch [4100/10000], Loss: 0.7114\n",
      "Epoch [4110/10000], Loss: 1.0198\n",
      "Epoch [4120/10000], Loss: 1.1644\n",
      "Epoch [4130/10000], Loss: 0.8845\n",
      "Epoch [4140/10000], Loss: 0.8731\n",
      "Epoch [4150/10000], Loss: 0.8300\n",
      "Epoch [4160/10000], Loss: 1.1106\n",
      "Epoch [4170/10000], Loss: 0.8446\n",
      "Epoch [4180/10000], Loss: 1.1584\n",
      "Epoch [4190/10000], Loss: 0.8733\n",
      "Epoch [4200/10000], Loss: 1.1462\n",
      "Epoch [4210/10000], Loss: 1.1616\n",
      "Epoch [4220/10000], Loss: 1.3211\n",
      "Epoch [4230/10000], Loss: 1.1228\n",
      "Epoch [4240/10000], Loss: 1.1041\n",
      "Epoch [4250/10000], Loss: 1.5245\n",
      "Epoch [4260/10000], Loss: 0.7893\n",
      "Epoch [4270/10000], Loss: 1.0645\n",
      "Epoch [4280/10000], Loss: 0.8809\n",
      "Epoch [4290/10000], Loss: 0.8070\n",
      "Epoch [4300/10000], Loss: 1.6132\n",
      "Epoch [4310/10000], Loss: 1.2629\n",
      "Epoch [4320/10000], Loss: 1.2337\n",
      "Epoch [4330/10000], Loss: 1.1708\n",
      "Epoch [4340/10000], Loss: 1.0951\n",
      "Epoch [4350/10000], Loss: 1.0326\n",
      "Epoch [4360/10000], Loss: 1.3271\n",
      "Epoch [4370/10000], Loss: 1.0533\n",
      "Epoch [4380/10000], Loss: 1.1786\n",
      "Epoch [4390/10000], Loss: 1.3908\n",
      "Epoch [4400/10000], Loss: 1.3520\n",
      "Epoch [4410/10000], Loss: 0.8592\n",
      "Epoch [4420/10000], Loss: 1.1638\n",
      "Epoch [4430/10000], Loss: 0.8358\n",
      "Epoch [4440/10000], Loss: 1.5038\n",
      "Epoch [4450/10000], Loss: 1.0393\n",
      "Epoch [4460/10000], Loss: 1.4237\n",
      "Epoch [4470/10000], Loss: 1.2310\n",
      "Epoch [4480/10000], Loss: 1.2231\n",
      "Epoch [4490/10000], Loss: 0.7923\n",
      "Epoch [4500/10000], Loss: 0.9597\n",
      "Epoch [4510/10000], Loss: 0.9086\n",
      "Epoch [4520/10000], Loss: 1.2147\n",
      "Epoch [4530/10000], Loss: 1.2787\n",
      "Epoch [4540/10000], Loss: 1.0246\n",
      "Epoch [4550/10000], Loss: 0.8735\n",
      "Epoch [4560/10000], Loss: 1.1447\n",
      "Epoch [4570/10000], Loss: 0.7941\n",
      "Epoch [4580/10000], Loss: 1.3038\n",
      "Epoch [4590/10000], Loss: 0.8983\n",
      "Epoch [4600/10000], Loss: 1.0409\n",
      "Epoch [4610/10000], Loss: 1.3534\n",
      "Epoch [4620/10000], Loss: 0.8891\n",
      "Epoch [4630/10000], Loss: 0.8492\n",
      "Epoch [4640/10000], Loss: 0.9888\n",
      "Epoch [4650/10000], Loss: 1.1596\n",
      "Epoch [4660/10000], Loss: 1.3924\n",
      "Epoch [4670/10000], Loss: 1.2818\n",
      "Epoch [4680/10000], Loss: 1.1565\n",
      "Epoch [4690/10000], Loss: 0.8394\n",
      "Epoch [4700/10000], Loss: 0.8400\n",
      "Epoch [4710/10000], Loss: 0.8565\n",
      "Epoch [4720/10000], Loss: 1.3195\n",
      "Epoch [4730/10000], Loss: 0.7148\n",
      "Epoch [4740/10000], Loss: 1.0975\n",
      "Epoch [4750/10000], Loss: 1.0474\n",
      "Epoch [4760/10000], Loss: 1.2304\n",
      "Epoch [4770/10000], Loss: 0.8789\n",
      "Epoch [4780/10000], Loss: 0.7752\n",
      "Epoch [4790/10000], Loss: 0.9159\n",
      "Epoch [4800/10000], Loss: 1.2462\n",
      "Epoch [4810/10000], Loss: 1.3739\n",
      "Epoch [4820/10000], Loss: 0.8380\n",
      "Epoch [4830/10000], Loss: 1.0436\n",
      "Epoch [4840/10000], Loss: 1.4140\n",
      "Epoch [4850/10000], Loss: 1.2693\n",
      "Epoch [4860/10000], Loss: 0.8995\n",
      "Epoch [4870/10000], Loss: 1.0612\n",
      "Epoch [4880/10000], Loss: 0.9702\n",
      "Epoch [4890/10000], Loss: 0.9776\n",
      "Epoch [4900/10000], Loss: 0.9283\n",
      "Epoch [4910/10000], Loss: 0.9039\n",
      "Epoch [4920/10000], Loss: 1.0920\n",
      "Epoch [4930/10000], Loss: 0.7955\n",
      "Epoch [4940/10000], Loss: 1.2922\n",
      "Epoch [4950/10000], Loss: 0.8418\n",
      "Epoch [4960/10000], Loss: 0.9817\n",
      "Epoch [4970/10000], Loss: 1.2116\n",
      "Epoch [4980/10000], Loss: 1.2698\n",
      "Epoch [4990/10000], Loss: 1.2068\n",
      "Epoch [5000/10000], Loss: 1.3811\n",
      "Epoch [5010/10000], Loss: 0.8340\n",
      "Epoch [5020/10000], Loss: 1.5174\n",
      "Epoch [5030/10000], Loss: 1.2413\n",
      "Epoch [5040/10000], Loss: 0.8330\n",
      "Epoch [5050/10000], Loss: 0.8625\n",
      "Epoch [5060/10000], Loss: 1.4991\n",
      "Epoch [5070/10000], Loss: 1.3395\n",
      "Epoch [5080/10000], Loss: 0.9451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5090/10000], Loss: 1.1556\n",
      "Epoch [5100/10000], Loss: 1.5433\n",
      "Epoch [5110/10000], Loss: 0.8595\n",
      "Epoch [5120/10000], Loss: 0.8722\n",
      "Epoch [5130/10000], Loss: 1.4662\n",
      "Epoch [5140/10000], Loss: 0.8304\n",
      "Epoch [5150/10000], Loss: 0.6668\n",
      "Epoch [5160/10000], Loss: 1.1592\n",
      "Epoch [5170/10000], Loss: 1.2560\n",
      "Epoch [5180/10000], Loss: 0.6873\n",
      "Epoch [5190/10000], Loss: 0.7153\n",
      "Epoch [5200/10000], Loss: 1.0455\n",
      "Epoch [5210/10000], Loss: 0.7635\n",
      "Epoch [5220/10000], Loss: 1.4709\n",
      "Epoch [5230/10000], Loss: 0.8506\n",
      "Epoch [5240/10000], Loss: 0.9669\n",
      "Epoch [5250/10000], Loss: 1.2511\n",
      "Epoch [5260/10000], Loss: 1.2511\n",
      "Epoch [5270/10000], Loss: 1.1636\n",
      "Epoch [5280/10000], Loss: 1.1975\n",
      "Epoch [5290/10000], Loss: 1.1769\n",
      "Epoch [5300/10000], Loss: 1.2792\n",
      "Epoch [5310/10000], Loss: 1.3535\n",
      "Epoch [5320/10000], Loss: 1.4859\n",
      "Epoch [5330/10000], Loss: 0.9809\n",
      "Epoch [5340/10000], Loss: 1.1206\n",
      "Epoch [5350/10000], Loss: 0.8543\n",
      "Epoch [5360/10000], Loss: 1.1693\n",
      "Epoch [5370/10000], Loss: 0.8896\n",
      "Epoch [5380/10000], Loss: 1.0408\n",
      "Epoch [5390/10000], Loss: 0.7933\n",
      "Epoch [5400/10000], Loss: 1.0986\n",
      "Epoch [5410/10000], Loss: 0.8998\n",
      "Epoch [5420/10000], Loss: 0.9756\n",
      "Epoch [5430/10000], Loss: 1.2470\n",
      "Epoch [5440/10000], Loss: 1.3608\n",
      "Epoch [5450/10000], Loss: 1.1502\n",
      "Epoch [5460/10000], Loss: 1.3195\n",
      "Epoch [5470/10000], Loss: 1.3288\n",
      "Epoch [5480/10000], Loss: 1.5806\n",
      "Epoch [5490/10000], Loss: 0.9367\n",
      "Epoch [5500/10000], Loss: 0.8702\n",
      "Epoch [5510/10000], Loss: 1.3187\n",
      "Epoch [5520/10000], Loss: 1.1427\n",
      "Epoch [5530/10000], Loss: 0.8651\n",
      "Epoch [5540/10000], Loss: 1.4544\n",
      "Epoch [5550/10000], Loss: 1.0216\n",
      "Epoch [5560/10000], Loss: 0.8629\n",
      "Epoch [5570/10000], Loss: 0.7593\n",
      "Epoch [5580/10000], Loss: 0.9336\n",
      "Epoch [5590/10000], Loss: 1.3383\n",
      "Epoch [5600/10000], Loss: 1.1556\n",
      "Epoch [5610/10000], Loss: 1.3998\n",
      "Epoch [5620/10000], Loss: 1.1459\n",
      "Epoch [5630/10000], Loss: 0.9189\n",
      "Epoch [5640/10000], Loss: 0.7674\n",
      "Epoch [5650/10000], Loss: 1.4493\n",
      "Epoch [5660/10000], Loss: 0.7278\n",
      "Epoch [5670/10000], Loss: 0.8169\n",
      "Epoch [5680/10000], Loss: 1.0985\n",
      "Epoch [5690/10000], Loss: 1.2944\n",
      "Epoch [5700/10000], Loss: 1.2216\n",
      "Epoch [5710/10000], Loss: 1.2162\n",
      "Epoch [5720/10000], Loss: 1.0374\n",
      "Epoch [5730/10000], Loss: 0.7949\n",
      "Epoch [5740/10000], Loss: 1.3678\n",
      "Epoch [5750/10000], Loss: 0.8608\n",
      "Epoch [5760/10000], Loss: 0.9163\n",
      "Epoch [5770/10000], Loss: 1.1082\n",
      "Epoch [5780/10000], Loss: 0.8566\n",
      "Epoch [5790/10000], Loss: 1.3047\n",
      "Epoch [5800/10000], Loss: 1.1030\n",
      "Epoch [5810/10000], Loss: 0.8181\n",
      "Epoch [5820/10000], Loss: 0.9247\n",
      "Epoch [5830/10000], Loss: 0.7573\n",
      "Epoch [5840/10000], Loss: 1.3990\n",
      "Epoch [5850/10000], Loss: 0.9093\n",
      "Epoch [5860/10000], Loss: 1.4619\n",
      "Epoch [5870/10000], Loss: 1.1169\n",
      "Epoch [5880/10000], Loss: 1.2020\n",
      "Epoch [5890/10000], Loss: 1.3825\n",
      "Epoch [5900/10000], Loss: 1.0074\n",
      "Epoch [5910/10000], Loss: 0.9799\n",
      "Epoch [5920/10000], Loss: 1.3737\n",
      "Epoch [5930/10000], Loss: 1.0939\n",
      "Epoch [5940/10000], Loss: 1.3101\n",
      "Epoch [5950/10000], Loss: 1.0741\n",
      "Epoch [5960/10000], Loss: 1.1058\n",
      "Epoch [5970/10000], Loss: 1.2089\n",
      "Epoch [5980/10000], Loss: 1.5055\n",
      "Epoch [5990/10000], Loss: 0.7145\n",
      "Epoch [6000/10000], Loss: 1.2362\n",
      "Epoch [6010/10000], Loss: 0.9569\n",
      "Epoch [6020/10000], Loss: 1.4569\n",
      "Epoch [6030/10000], Loss: 1.3490\n",
      "Epoch [6040/10000], Loss: 1.3188\n",
      "Epoch [6050/10000], Loss: 0.9152\n",
      "Epoch [6060/10000], Loss: 1.2864\n",
      "Epoch [6070/10000], Loss: 0.9074\n",
      "Epoch [6080/10000], Loss: 1.3242\n",
      "Epoch [6090/10000], Loss: 1.4216\n",
      "Epoch [6100/10000], Loss: 0.8923\n",
      "Epoch [6110/10000], Loss: 0.9123\n",
      "Epoch [6120/10000], Loss: 0.8211\n",
      "Epoch [6130/10000], Loss: 0.7925\n",
      "Epoch [6140/10000], Loss: 0.8987\n",
      "Epoch [6150/10000], Loss: 1.1019\n",
      "Epoch [6160/10000], Loss: 1.5402\n",
      "Epoch [6170/10000], Loss: 0.9142\n",
      "Epoch [6180/10000], Loss: 0.9017\n",
      "Epoch [6190/10000], Loss: 1.1792\n",
      "Epoch [6200/10000], Loss: 0.7484\n",
      "Epoch [6210/10000], Loss: 0.8172\n",
      "Epoch [6220/10000], Loss: 0.9159\n",
      "Epoch [6230/10000], Loss: 1.2983\n",
      "Epoch [6240/10000], Loss: 1.2382\n",
      "Epoch [6250/10000], Loss: 1.2952\n",
      "Epoch [6260/10000], Loss: 1.3790\n",
      "Epoch [6270/10000], Loss: 1.3285\n",
      "Epoch [6280/10000], Loss: 0.9084\n",
      "Epoch [6290/10000], Loss: 1.1783\n",
      "Epoch [6300/10000], Loss: 1.5269\n",
      "Epoch [6310/10000], Loss: 1.3377\n",
      "Epoch [6320/10000], Loss: 1.4519\n",
      "Epoch [6330/10000], Loss: 1.0061\n",
      "Epoch [6340/10000], Loss: 0.8592\n",
      "Epoch [6350/10000], Loss: 1.2273\n",
      "Epoch [6360/10000], Loss: 1.4460\n",
      "Epoch [6370/10000], Loss: 1.1035\n",
      "Epoch [6380/10000], Loss: 0.9981\n",
      "Epoch [6390/10000], Loss: 0.9019\n",
      "Epoch [6400/10000], Loss: 1.2339\n",
      "Epoch [6410/10000], Loss: 0.9017\n",
      "Epoch [6420/10000], Loss: 1.3006\n",
      "Epoch [6430/10000], Loss: 1.0671\n",
      "Epoch [6440/10000], Loss: 1.1567\n",
      "Epoch [6450/10000], Loss: 1.5764\n",
      "Epoch [6460/10000], Loss: 1.4456\n",
      "Epoch [6470/10000], Loss: 1.0871\n",
      "Epoch [6480/10000], Loss: 1.3943\n",
      "Epoch [6490/10000], Loss: 0.9714\n",
      "Epoch [6500/10000], Loss: 1.3612\n",
      "Epoch [6510/10000], Loss: 0.7540\n",
      "Epoch [6520/10000], Loss: 0.9909\n",
      "Epoch [6530/10000], Loss: 0.7731\n",
      "Epoch [6540/10000], Loss: 0.7469\n",
      "Epoch [6550/10000], Loss: 0.9484\n",
      "Epoch [6560/10000], Loss: 1.2617\n",
      "Epoch [6570/10000], Loss: 0.9556\n",
      "Epoch [6580/10000], Loss: 1.1742\n",
      "Epoch [6590/10000], Loss: 0.8970\n",
      "Epoch [6600/10000], Loss: 1.2919\n",
      "Epoch [6610/10000], Loss: 1.1668\n",
      "Epoch [6620/10000], Loss: 1.1604\n",
      "Epoch [6630/10000], Loss: 0.8974\n",
      "Epoch [6640/10000], Loss: 1.1070\n",
      "Epoch [6650/10000], Loss: 0.9181\n",
      "Epoch [6660/10000], Loss: 0.8632\n",
      "Epoch [6670/10000], Loss: 0.8634\n",
      "Epoch [6680/10000], Loss: 1.2396\n",
      "Epoch [6690/10000], Loss: 0.9932\n",
      "Epoch [6700/10000], Loss: 1.3335\n",
      "Epoch [6710/10000], Loss: 1.3155\n",
      "Epoch [6720/10000], Loss: 1.5531\n",
      "Epoch [6730/10000], Loss: 0.9833\n",
      "Epoch [6740/10000], Loss: 0.9494\n",
      "Epoch [6750/10000], Loss: 1.0744\n",
      "Epoch [6760/10000], Loss: 0.7785\n",
      "Epoch [6770/10000], Loss: 1.0408\n",
      "Epoch [6780/10000], Loss: 1.4216\n",
      "Epoch [6790/10000], Loss: 1.1223\n",
      "Epoch [6800/10000], Loss: 1.0973\n",
      "Epoch [6810/10000], Loss: 1.3421\n",
      "Epoch [6820/10000], Loss: 1.0413\n",
      "Epoch [6830/10000], Loss: 1.2115\n",
      "Epoch [6840/10000], Loss: 0.7844\n",
      "Epoch [6850/10000], Loss: 1.3481\n",
      "Epoch [6860/10000], Loss: 1.5090\n",
      "Epoch [6870/10000], Loss: 1.2872\n",
      "Epoch [6880/10000], Loss: 1.2384\n",
      "Epoch [6890/10000], Loss: 1.0950\n",
      "Epoch [6900/10000], Loss: 0.9133\n",
      "Epoch [6910/10000], Loss: 1.2724\n",
      "Epoch [6920/10000], Loss: 1.0434\n",
      "Epoch [6930/10000], Loss: 0.9267\n",
      "Epoch [6940/10000], Loss: 1.0150\n",
      "Epoch [6950/10000], Loss: 1.1607\n",
      "Epoch [6960/10000], Loss: 1.2211\n",
      "Epoch [6970/10000], Loss: 1.1392\n",
      "Epoch [6980/10000], Loss: 1.3140\n",
      "Epoch [6990/10000], Loss: 1.3564\n",
      "Epoch [7000/10000], Loss: 0.8916\n",
      "Epoch [7010/10000], Loss: 1.1526\n",
      "Epoch [7020/10000], Loss: 0.6499\n",
      "Epoch [7030/10000], Loss: 0.8208\n",
      "Epoch [7040/10000], Loss: 0.8353\n",
      "Epoch [7050/10000], Loss: 0.8054\n",
      "Epoch [7060/10000], Loss: 1.3007\n",
      "Epoch [7070/10000], Loss: 0.9022\n",
      "Epoch [7080/10000], Loss: 1.3569\n",
      "Epoch [7090/10000], Loss: 0.9886\n",
      "Epoch [7100/10000], Loss: 1.0891\n",
      "Epoch [7110/10000], Loss: 1.5124\n",
      "Epoch [7120/10000], Loss: 1.1996\n",
      "Epoch [7130/10000], Loss: 1.1153\n",
      "Epoch [7140/10000], Loss: 1.5650\n",
      "Epoch [7150/10000], Loss: 0.8682\n",
      "Epoch [7160/10000], Loss: 0.7791\n",
      "Epoch [7170/10000], Loss: 0.9700\n",
      "Epoch [7180/10000], Loss: 1.2489\n",
      "Epoch [7190/10000], Loss: 0.8757\n",
      "Epoch [7200/10000], Loss: 1.2093\n",
      "Epoch [7210/10000], Loss: 0.8771\n",
      "Epoch [7220/10000], Loss: 0.9112\n",
      "Epoch [7230/10000], Loss: 1.2539\n",
      "Epoch [7240/10000], Loss: 1.0822\n",
      "Epoch [7250/10000], Loss: 1.0305\n",
      "Epoch [7260/10000], Loss: 0.9257\n",
      "Epoch [7270/10000], Loss: 0.8636\n",
      "Epoch [7280/10000], Loss: 0.8445\n",
      "Epoch [7290/10000], Loss: 0.9298\n",
      "Epoch [7300/10000], Loss: 1.1703\n",
      "Epoch [7310/10000], Loss: 1.5283\n",
      "Epoch [7320/10000], Loss: 1.2377\n",
      "Epoch [7330/10000], Loss: 1.3922\n",
      "Epoch [7340/10000], Loss: 0.9123\n",
      "Epoch [7350/10000], Loss: 1.3245\n",
      "Epoch [7360/10000], Loss: 1.3474\n",
      "Epoch [7370/10000], Loss: 0.9006\n",
      "Epoch [7380/10000], Loss: 0.9653\n",
      "Epoch [7390/10000], Loss: 0.7427\n",
      "Epoch [7400/10000], Loss: 0.8915\n",
      "Epoch [7410/10000], Loss: 0.9253\n",
      "Epoch [7420/10000], Loss: 1.2534\n",
      "Epoch [7430/10000], Loss: 0.8380\n",
      "Epoch [7440/10000], Loss: 0.9322\n",
      "Epoch [7450/10000], Loss: 1.1868\n",
      "Epoch [7460/10000], Loss: 1.3696\n",
      "Epoch [7470/10000], Loss: 0.8307\n",
      "Epoch [7480/10000], Loss: 1.0484\n",
      "Epoch [7490/10000], Loss: 1.1343\n",
      "Epoch [7500/10000], Loss: 1.0977\n",
      "Epoch [7510/10000], Loss: 1.4893\n",
      "Epoch [7520/10000], Loss: 1.2085\n",
      "Epoch [7530/10000], Loss: 1.3185\n",
      "Epoch [7540/10000], Loss: 1.3129\n",
      "Epoch [7550/10000], Loss: 1.3908\n",
      "Epoch [7560/10000], Loss: 1.3326\n",
      "Epoch [7570/10000], Loss: 0.8244\n",
      "Epoch [7580/10000], Loss: 1.3249\n",
      "Epoch [7590/10000], Loss: 1.2554\n",
      "Epoch [7600/10000], Loss: 1.3079\n",
      "Epoch [7610/10000], Loss: 1.2234\n",
      "Epoch [7620/10000], Loss: 0.7921\n",
      "Epoch [7630/10000], Loss: 1.3530\n",
      "Epoch [7640/10000], Loss: 1.2501\n",
      "Epoch [7650/10000], Loss: 0.9459\n",
      "Epoch [7660/10000], Loss: 1.0412\n",
      "Epoch [7670/10000], Loss: 0.8987\n",
      "Epoch [7680/10000], Loss: 1.3217\n",
      "Epoch [7690/10000], Loss: 0.7784\n",
      "Epoch [7700/10000], Loss: 1.1379\n",
      "Epoch [7710/10000], Loss: 1.4849\n",
      "Epoch [7720/10000], Loss: 1.2756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7730/10000], Loss: 0.8157\n",
      "Epoch [7740/10000], Loss: 0.9237\n",
      "Epoch [7750/10000], Loss: 1.2386\n",
      "Epoch [7760/10000], Loss: 1.2018\n",
      "Epoch [7770/10000], Loss: 1.2797\n",
      "Epoch [7780/10000], Loss: 0.8312\n",
      "Epoch [7790/10000], Loss: 0.8726\n",
      "Epoch [7800/10000], Loss: 0.7857\n",
      "Epoch [7810/10000], Loss: 1.2891\n",
      "Epoch [7820/10000], Loss: 0.7989\n",
      "Epoch [7830/10000], Loss: 1.2304\n",
      "Epoch [7840/10000], Loss: 0.9172\n",
      "Epoch [7850/10000], Loss: 1.3950\n",
      "Epoch [7860/10000], Loss: 0.8716\n",
      "Epoch [7870/10000], Loss: 0.9098\n",
      "Epoch [7880/10000], Loss: 1.4335\n",
      "Epoch [7890/10000], Loss: 1.1453\n",
      "Epoch [7900/10000], Loss: 1.3239\n",
      "Epoch [7910/10000], Loss: 0.9807\n",
      "Epoch [7920/10000], Loss: 1.4704\n",
      "Epoch [7930/10000], Loss: 0.9001\n",
      "Epoch [7940/10000], Loss: 1.4132\n",
      "Epoch [7950/10000], Loss: 1.4726\n",
      "Epoch [7960/10000], Loss: 1.5136\n",
      "Epoch [7970/10000], Loss: 0.9777\n",
      "Epoch [7980/10000], Loss: 0.8661\n",
      "Epoch [7990/10000], Loss: 1.2105\n",
      "Epoch [8000/10000], Loss: 0.8327\n",
      "Epoch [8010/10000], Loss: 1.1448\n",
      "Epoch [8020/10000], Loss: 0.9293\n",
      "Epoch [8030/10000], Loss: 1.2026\n",
      "Epoch [8040/10000], Loss: 0.8807\n",
      "Epoch [8050/10000], Loss: 1.1651\n",
      "Epoch [8060/10000], Loss: 1.4659\n",
      "Epoch [8070/10000], Loss: 1.3284\n",
      "Epoch [8080/10000], Loss: 0.8983\n",
      "Epoch [8090/10000], Loss: 1.0744\n",
      "Epoch [8100/10000], Loss: 1.0619\n",
      "Epoch [8110/10000], Loss: 1.2175\n",
      "Epoch [8120/10000], Loss: 1.1668\n",
      "Epoch [8130/10000], Loss: 0.8186\n",
      "Epoch [8140/10000], Loss: 1.2083\n",
      "Epoch [8150/10000], Loss: 1.1907\n",
      "Epoch [8160/10000], Loss: 1.0764\n",
      "Epoch [8170/10000], Loss: 1.2334\n",
      "Epoch [8180/10000], Loss: 1.3593\n",
      "Epoch [8190/10000], Loss: 0.9363\n",
      "Epoch [8200/10000], Loss: 1.4160\n",
      "Epoch [8210/10000], Loss: 0.7315\n",
      "Epoch [8220/10000], Loss: 1.4055\n",
      "Epoch [8230/10000], Loss: 1.3218\n",
      "Epoch [8240/10000], Loss: 0.9481\n",
      "Epoch [8250/10000], Loss: 0.8413\n",
      "Epoch [8260/10000], Loss: 1.2833\n",
      "Epoch [8270/10000], Loss: 1.1381\n",
      "Epoch [8280/10000], Loss: 0.9048\n",
      "Epoch [8290/10000], Loss: 1.0923\n",
      "Epoch [8300/10000], Loss: 0.9721\n",
      "Epoch [8310/10000], Loss: 1.4224\n",
      "Epoch [8320/10000], Loss: 1.4344\n",
      "Epoch [8330/10000], Loss: 0.7569\n",
      "Epoch [8340/10000], Loss: 0.8447\n",
      "Epoch [8350/10000], Loss: 1.3009\n",
      "Epoch [8360/10000], Loss: 0.9773\n",
      "Epoch [8370/10000], Loss: 1.0384\n",
      "Epoch [8380/10000], Loss: 1.2767\n",
      "Epoch [8390/10000], Loss: 0.8321\n",
      "Epoch [8400/10000], Loss: 1.0304\n",
      "Epoch [8410/10000], Loss: 0.9781\n",
      "Epoch [8420/10000], Loss: 0.7929\n",
      "Epoch [8430/10000], Loss: 1.1121\n",
      "Epoch [8440/10000], Loss: 0.8426\n",
      "Epoch [8450/10000], Loss: 1.4155\n",
      "Epoch [8460/10000], Loss: 1.4401\n",
      "Epoch [8470/10000], Loss: 1.2382\n",
      "Epoch [8480/10000], Loss: 0.7385\n",
      "Epoch [8490/10000], Loss: 0.8987\n",
      "Epoch [8500/10000], Loss: 0.8950\n",
      "Epoch [8510/10000], Loss: 1.2093\n",
      "Epoch [8520/10000], Loss: 1.2323\n",
      "Epoch [8530/10000], Loss: 0.8669\n",
      "Epoch [8540/10000], Loss: 1.2115\n",
      "Epoch [8550/10000], Loss: 1.2634\n",
      "Epoch [8560/10000], Loss: 1.3859\n",
      "Epoch [8570/10000], Loss: 1.5446\n",
      "Epoch [8580/10000], Loss: 1.0892\n",
      "Epoch [8590/10000], Loss: 1.2830\n",
      "Epoch [8600/10000], Loss: 1.4156\n",
      "Epoch [8610/10000], Loss: 1.1115\n",
      "Epoch [8620/10000], Loss: 1.2872\n",
      "Epoch [8630/10000], Loss: 0.8309\n",
      "Epoch [8640/10000], Loss: 1.3582\n",
      "Epoch [8650/10000], Loss: 1.1930\n",
      "Epoch [8660/10000], Loss: 0.7827\n",
      "Epoch [8670/10000], Loss: 0.7496\n",
      "Epoch [8680/10000], Loss: 0.8490\n",
      "Epoch [8690/10000], Loss: 1.0072\n",
      "Epoch [8700/10000], Loss: 1.5225\n",
      "Epoch [8710/10000], Loss: 1.0675\n",
      "Epoch [8720/10000], Loss: 0.9427\n",
      "Epoch [8730/10000], Loss: 0.8609\n",
      "Epoch [8740/10000], Loss: 0.7961\n",
      "Epoch [8750/10000], Loss: 0.7910\n",
      "Epoch [8760/10000], Loss: 1.3054\n",
      "Epoch [8770/10000], Loss: 1.0474\n",
      "Epoch [8780/10000], Loss: 1.2288\n",
      "Epoch [8790/10000], Loss: 0.8302\n",
      "Epoch [8800/10000], Loss: 1.0554\n",
      "Epoch [8810/10000], Loss: 1.4793\n",
      "Epoch [8820/10000], Loss: 1.2886\n",
      "Epoch [8830/10000], Loss: 0.8879\n",
      "Epoch [8840/10000], Loss: 0.8617\n",
      "Epoch [8850/10000], Loss: 1.0093\n",
      "Epoch [8860/10000], Loss: 0.8544\n",
      "Epoch [8870/10000], Loss: 1.0636\n",
      "Epoch [8880/10000], Loss: 0.8801\n",
      "Epoch [8890/10000], Loss: 1.1638\n",
      "Epoch [8900/10000], Loss: 1.3555\n",
      "Epoch [8910/10000], Loss: 1.2364\n",
      "Epoch [8920/10000], Loss: 0.8061\n",
      "Epoch [8930/10000], Loss: 1.3842\n",
      "Epoch [8940/10000], Loss: 0.9721\n",
      "Epoch [8950/10000], Loss: 0.7880\n",
      "Epoch [8960/10000], Loss: 0.8046\n",
      "Epoch [8970/10000], Loss: 1.0185\n",
      "Epoch [8980/10000], Loss: 0.8167\n",
      "Epoch [8990/10000], Loss: 1.3374\n",
      "Epoch [9000/10000], Loss: 1.0693\n",
      "Epoch [9010/10000], Loss: 0.7841\n",
      "Epoch [9020/10000], Loss: 0.8439\n",
      "Epoch [9030/10000], Loss: 1.2438\n",
      "Epoch [9040/10000], Loss: 0.9410\n",
      "Epoch [9050/10000], Loss: 1.1933\n",
      "Epoch [9060/10000], Loss: 1.2243\n",
      "Epoch [9070/10000], Loss: 1.1018\n",
      "Epoch [9080/10000], Loss: 1.2969\n",
      "Epoch [9090/10000], Loss: 1.2474\n",
      "Epoch [9100/10000], Loss: 0.9278\n",
      "Epoch [9110/10000], Loss: 0.8759\n",
      "Epoch [9120/10000], Loss: 0.9204\n",
      "Epoch [9130/10000], Loss: 1.0277\n",
      "Epoch [9140/10000], Loss: 0.7856\n",
      "Epoch [9150/10000], Loss: 1.1510\n",
      "Epoch [9160/10000], Loss: 0.8681\n",
      "Epoch [9170/10000], Loss: 0.7783\n",
      "Epoch [9180/10000], Loss: 1.3277\n",
      "Epoch [9190/10000], Loss: 0.9324\n",
      "Epoch [9200/10000], Loss: 0.8942\n",
      "Epoch [9210/10000], Loss: 0.9242\n",
      "Epoch [9220/10000], Loss: 1.2231\n",
      "Epoch [9230/10000], Loss: 1.1631\n",
      "Epoch [9240/10000], Loss: 0.8534\n",
      "Epoch [9250/10000], Loss: 1.2620\n",
      "Epoch [9260/10000], Loss: 1.1656\n",
      "Epoch [9270/10000], Loss: 1.0626\n",
      "Epoch [9280/10000], Loss: 1.2409\n",
      "Epoch [9290/10000], Loss: 1.1794\n",
      "Epoch [9300/10000], Loss: 1.2076\n",
      "Epoch [9310/10000], Loss: 1.2186\n",
      "Epoch [9320/10000], Loss: 1.4880\n",
      "Epoch [9330/10000], Loss: 0.7620\n",
      "Epoch [9340/10000], Loss: 1.2248\n",
      "Epoch [9350/10000], Loss: 1.4729\n",
      "Epoch [9360/10000], Loss: 1.0434\n",
      "Epoch [9370/10000], Loss: 0.9110\n",
      "Epoch [9380/10000], Loss: 1.2665\n",
      "Epoch [9390/10000], Loss: 0.7067\n",
      "Epoch [9400/10000], Loss: 1.0025\n",
      "Epoch [9410/10000], Loss: 0.9758\n",
      "Epoch [9420/10000], Loss: 0.7505\n",
      "Epoch [9430/10000], Loss: 0.7918\n",
      "Epoch [9440/10000], Loss: 1.2783\n",
      "Epoch [9450/10000], Loss: 1.2391\n",
      "Epoch [9460/10000], Loss: 0.7242\n",
      "Epoch [9470/10000], Loss: 0.9294\n",
      "Epoch [9480/10000], Loss: 0.8682\n",
      "Epoch [9490/10000], Loss: 1.4917\n",
      "Epoch [9500/10000], Loss: 1.2410\n",
      "Epoch [9510/10000], Loss: 0.9756\n",
      "Epoch [9520/10000], Loss: 1.1765\n",
      "Epoch [9530/10000], Loss: 0.7999\n",
      "Epoch [9540/10000], Loss: 0.9345\n",
      "Epoch [9550/10000], Loss: 1.1867\n",
      "Epoch [9560/10000], Loss: 1.2260\n",
      "Epoch [9570/10000], Loss: 1.1931\n",
      "Epoch [9580/10000], Loss: 1.2310\n",
      "Epoch [9590/10000], Loss: 0.7964\n",
      "Epoch [9600/10000], Loss: 1.2693\n",
      "Epoch [9610/10000], Loss: 1.2218\n",
      "Epoch [9620/10000], Loss: 0.8531\n",
      "Epoch [9630/10000], Loss: 1.5558\n",
      "Epoch [9640/10000], Loss: 1.1418\n",
      "Epoch [9650/10000], Loss: 1.4285\n",
      "Epoch [9660/10000], Loss: 0.8615\n",
      "Epoch [9670/10000], Loss: 1.2640\n",
      "Epoch [9680/10000], Loss: 1.1146\n",
      "Epoch [9690/10000], Loss: 0.8543\n",
      "Epoch [9700/10000], Loss: 0.9673\n",
      "Epoch [9710/10000], Loss: 1.4183\n",
      "Epoch [9720/10000], Loss: 0.7556\n",
      "Epoch [9730/10000], Loss: 1.0383\n",
      "Epoch [9740/10000], Loss: 1.1379\n",
      "Epoch [9750/10000], Loss: 0.9985\n",
      "Epoch [9760/10000], Loss: 1.0186\n",
      "Epoch [9770/10000], Loss: 0.9455\n",
      "Epoch [9780/10000], Loss: 1.2929\n",
      "Epoch [9790/10000], Loss: 0.9043\n",
      "Epoch [9800/10000], Loss: 1.4165\n",
      "Epoch [9810/10000], Loss: 1.5483\n",
      "Epoch [9820/10000], Loss: 0.7926\n",
      "Epoch [9830/10000], Loss: 1.4220\n",
      "Epoch [9840/10000], Loss: 0.7994\n",
      "Epoch [9850/10000], Loss: 1.2249\n",
      "Epoch [9860/10000], Loss: 1.3710\n",
      "Epoch [9870/10000], Loss: 0.9489\n",
      "Epoch [9880/10000], Loss: 1.1976\n",
      "Epoch [9890/10000], Loss: 1.4322\n",
      "Epoch [9900/10000], Loss: 0.9035\n",
      "Epoch [9910/10000], Loss: 0.7797\n",
      "Epoch [9920/10000], Loss: 1.2238\n",
      "Epoch [9930/10000], Loss: 1.3078\n",
      "Epoch [9940/10000], Loss: 1.2061\n",
      "Epoch [9950/10000], Loss: 0.8035\n",
      "Epoch [9960/10000], Loss: 0.7512\n",
      "Epoch [9970/10000], Loss: 1.2332\n",
      "Epoch [9980/10000], Loss: 0.9037\n",
      "Epoch [9990/10000], Loss: 0.7041\n",
      "Epoch [10000/10000], Loss: 1.0120\n"
     ]
    }
   ],
   "source": [
    "fit(10000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.9930,  70.3957],\n",
       "        [ 82.0179, 100.5393],\n",
       "        [118.1808, 132.7892],\n",
       "        [ 20.8152,  37.7588],\n",
       "        [101.8813, 118.5002],\n",
       "        [ 55.7448,  69.3175],\n",
       "        [ 81.8741, 100.6357],\n",
       "        [118.4725, 133.3916],\n",
       "        [ 22.0634,  38.8369],\n",
       "        [102.9857, 119.6747],\n",
       "        [ 56.8492,  70.4921],\n",
       "        [ 80.7697,  99.4611],\n",
       "        [118.3246, 132.6927],\n",
       "        [ 19.7107,  36.5842],\n",
       "        [103.1295, 119.5783]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 57.,  69.],\n",
       "        [ 80., 102.],\n",
       "        [118., 132.],\n",
       "        [ 21.,  38.],\n",
       "        [104., 118.],\n",
       "        [ 57.,  69.],\n",
       "        [ 82., 100.],\n",
       "        [118., 134.],\n",
       "        [ 20.,  38.],\n",
       "        [102., 120.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the predictions are quite close to our targets. We have a trained a reasonably good model to predict crop yields for apples and oranges by looking at the average temperature, rainfall, and humidity in a region. We can use it to make predictions of crop yields for new regions by passing a batch containing a single row of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[53.5109, 67.5437]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[75, 63, 44.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted yield of apples is 53.3 tons per hectare, and that of oranges is 68.3 tons per hectare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
